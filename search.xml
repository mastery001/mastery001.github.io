<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深入理解NIO系列 - Selector详解]]></title>
    <url>%2F2019%2F02%2F28%2F2019-02-28-nio_selector%2F</url>
    <content type="text"><![CDATA[深入理解NIO系列 - Selector详解 Selector简述 A multiplexor of {@link SelectableChannel} objects. 参照Java doc中Selector描述的第一句话，Selector的作用是Java NIO中管理一组多路复用的SelectableChannel对象，并能够识别通道是否为诸如读写事件做好准备的组件 Selector的创建过程如下： 12345678910111213141516// 1.创建SelectorSelector selector = Selector.open();// 2.将Channel注册到选择器中// ....... new channel的过程 ....//Notes：channel要注册到Selector上就必须是非阻塞的，所以FileChannel是不可以使用Selector的，因为FileChannel是阻塞的channel.configureBlocking(false);// 第二个参数指定了我们对 Channel 的什么类型的事件感兴趣SelectionKey key = channel.register(selector , SelectionKey.OP_READ);// 也可以使用或运算|来组合多个事件，例如SelectionKey key = channel.register(selector , SelectionKey.OP_READ | SelectionKey.OP_WRITE);// 不过值得注意的是，一个 Channel 仅仅可以被注册到一个 Selector 一次, 如果将 Channel 注册到 Selector 多次, 那么其实就是相当于更新 SelectionKey 的 interest set. 一个Channel在Selector注册其代表的是一个SelectionKey事件，SelectionKey的类型包括： OP_READ：可读事件；值为：1&lt;&lt;0 OP_WRITE：可写事件；值为：1&lt;&lt;2 OP_CONNECT：客户端连接服务端的事件(tcp连接)，一般为创建SocketChannel客户端channel；值为：1&lt;&lt;3 OP_ACCEPT：服务端接收客户端连接的事件，一般为创建ServerSocketChannel服务端channel；值为：1&lt;&lt;4 一个Selector内部维护了三组keys： key set:当前channel注册在Selector上所有的key；可调用keys()获取 selected-key set:当前channel就绪的事件；可调用selectedKeys()获取 cancelled-key:主动触发SelectionKey#cancel()方法会放在该集合，前提条件是该channel没有被取消注册；不可通过外部方法调用 Selector类中总共包含以下10个方法： open():创建一个Selector对象 isOpen():是否是open状态，如果调用了close()方法则会返回false provider():获取当前Selector的Provider keys():如上文所述，获取当前channel注册在Selector上所有的key selectedKeys():获取当前channel就绪的事件列表 selectNow():获取当前是否有事件就绪，该方法立即返回结果，不会阻塞；如果返回值&gt;0，则代表存在一个或多个 select(long timeout):selectNow的阻塞超时方法，超时时间内，有事件就绪时才会返回；否则超过时间也会返回 select():selectNow的阻塞方法，直到有事件就绪时才会返回 wakeup():调用该方法会时，阻塞在select()处的线程会立马返回；(ps：下面一句划重点)即使当前不存在线程阻塞在select()处，那么下一个执行select()方法的线程也会立即返回结果，相当于执行了一次selectNow()方法 close(): 用完Selector后调用其close()方法会关闭该Selector，且使注册到该Selector上的所有SelectionKey实例无效。channel本身并不会关闭。 关于SelectionKey谈到Selector就不得不提SelectionKey，两者是紧密关联，配合使用的；如上文所示，往Channel注册Selector会返回一个SelectionKey对象，这个对象包含了如下内容： interest set,当前Channel感兴趣的事件集，即在调用register方法设置的interes set ready set channel selector attached object，可选的附加对象 interest set可以通过SelectionKey类中的方法来获取和设置interes set 1234567891011// 返回当前感兴趣的事件列表int interestSet = key.interestOps();// 也可通过interestSet判断其中包含的事件boolean isInterestedInAccept = interestSet &amp; SelectionKey.OP_ACCEPT;boolean isInterestedInConnect = interestSet &amp; SelectionKey.OP_CONNECT;boolean isInterestedInRead = interestSet &amp; SelectionKey.OP_READ;boolean isInterestedInWrite = interestSet &amp; SelectionKey.OP_WRITE; // 可以通过interestOps(int ops)方法修改事件列表key.interestOps(interestSet | SelectionKey.OP_WRITE); ready set当前Channel就绪的事件列表 1234567int readySet = key.readyOps();// 也可通过四个方法来分别判断不同事件是否就绪key.isReadable(); //读事件是否就绪key.isWritable(); //写事件是否就绪key.isConnectable(); //客户端连接事件是否就绪key.isAcceptable(); //服务端连接事件是否就绪 channel和selector我们可以通过SelectionKey来获取当前的channel和selector 12345// 返回当前事件关联的通道，可转换的选项包括:`ServerSocketChannel`和`SocketChannel`Channel channel = key.channel();//返回当前事件所关联的Selector对象Selector selector = key.selector(); attached object我们可以在selectionKey中附加一个对象: 12key.attach(theObject);Object attachedObj = key.attachment(); 或者在注册时直接附加: 1SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); 一个Selector完整的例子一个Selector的基本使用流程包括（读者不放试着按照这个流程自己实现一波）： 创建一个Selector 将Channel注册到Selector中，并设置监听的interest set loop 执行select()方法 调用selector.selectedKeys()获取当前就绪的key 迭代selectedKeys 从key中获取对应的Channel和附加信息(if exist) 判断是哪些 IO 事件已经就绪了, 然后处理它们. 如果是 OP_ACCEPT 事件, 则调用 “SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept()” 获取 SocketChannel, 并将它设置为 非阻塞的, 然后将这个 Channel 注册到 Selector 中. 根据需要更改 selected key 的监听事件. 将已经处理过的 key 从 selected keys 集合中删除. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122import java.io.IOException;import java.io.UnsupportedEncodingException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.nio.charset.Charset;import java.util.Iterator;import java.util.Set;/** * Created by locoder on 2019/2/28. */public class SelectorDemo &#123; public static void main(String[] args) throws IOException &#123; // create a Selector Selector selector = Selector.open(); // new Server Channel ServerSocketChannel ssc = ServerSocketChannel.open(); // config async ssc.configureBlocking(false); ssc.socket().bind(new InetSocketAddress(8080)); // register to selector // Notes：这里只能注册OP_ACCEPT事件，否则将会抛出IllegalArgumentException,详见AbstractSelectableChannel#register方法 ssc.register(selector, SelectionKey.OP_ACCEPT); // loop for (; ; ) &#123; int nKeys = selector.select(); if (nKeys &gt; 0) &#123; Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); for (Iterator&lt;SelectionKey&gt; it = keys.iterator(); it.hasNext(); ) &#123; SelectionKey key = it.next(); // 处理客户端连接事件 if (key.isAcceptable()) &#123; ServerSocketChannel serverSocketChannel = (ServerSocketChannel) key.channel(); SocketChannel clientChannel = serverSocketChannel.accept(); clientChannel.configureBlocking(false); clientChannel.register(selector, SelectionKey.OP_READ, ByteBuffer.allocate(1024 * 1024)); &#125; else if (key.isReadable()) &#123; SocketChannel socketChannel = (SocketChannel) key.channel(); ByteBuffer buf = (ByteBuffer) key.attachment(); int readBytes = 0; int ret = 0; try &#123; while ((ret = socketChannel.read(buf)) &gt; 0) &#123; readBytes += ret; &#125; if (readBytes &gt; 0) &#123; String message = decode(buf); System.out.println(message); // 这里注册写事件，因为写事件基本都处于就绪状态； // 从处理逻辑来看，一般接收到客户端读事件时也会伴随着写，类似HttpServletRequest和HttpServletResponse key.interestOps(key.interestOps() | SelectionKey.OP_WRITE); &#125; &#125; finally &#123; // 将缓冲区切换为待读取状态 buf.flip(); &#125; &#125; else if (key.isValid() &amp;&amp; key.isWritable()) &#123; SocketChannel socketChannel = (SocketChannel) key.channel(); ByteBuffer buf = (ByteBuffer) key.attachment(); if (buf.hasRemaining()) &#123; socketChannel.write(buf); &#125; else &#123; // 取消写事件，否则写事件内的代码会不断执行 // 因为写事件就绪的条件是判断缓冲区是否有空闲空间，绝大多时候缓存区都是有空闲空间的 key.interestOps(key.interestOps() &amp; (~SelectionKey.OP_WRITE)); &#125; // 丢弃本次内容 buf.compact(); &#125; // 注意, 在每次迭代时, 我们都调用 "it.remove()" 将这个 key 从迭代器中删除, // 因为 select() 方法仅仅是简单地将就绪的 IO 操作放到 selectedKeys 集合中, // 因此如果我们从 selectedKeys 获取到一个 key, 但是没有将它删除, 那么下一次 select 时, 这个 key 所对应的 IO 事件还在 selectedKeys 中. it.remove(); &#125; &#125; &#125; &#125; /** * 将ByteBuffer转换为String * * @param in * @return * @throws UnsupportedEncodingException */ private static String decode(ByteBuffer in) throws UnsupportedEncodingException &#123; String receiveText = new String(in.array(), 0, in.capacity(), Charset.defaultCharset()); int index = -1; if ((index = receiveText.lastIndexOf("\r\n")) != -1) &#123; receiveText = receiveText.substring(0, index); &#125; return receiveText; &#125;&#125; 深入Selector源码下面我们继续按照Selector的编码过程来学习Selector源码 （1）创建过程 从上图上可以比较清晰得看到，openjdk中Selector的实现是SelectorImpl,后SelectorImpl又将职责委托给了具体的平台，比如图中框出的linux2.6以后才有的EpollSelectorImpl, Windows平台则是WindowsSelectorImpl, MacOSX平台是KQueueSelectorImpl. 1234567891011public static Selector open() throws IOException &#123; return SelectorProvider.provider().openSelector();&#125;// 创建是依赖SelectorProvider.provider()系统级提供// 我们来看SelectorProvider.provider()方法// 从系统配置java.nio.channels.spi.SelectorProvider获取if (loadProviderFromProperty()) return provider;// 从ServiceLoader#loadif (loadProviderAsService()) return provider;// 如果还不存在则使用默认provider，即KQueueSelectorProviderprovider = sun.nio.ch.DefaultSelectorProvider.create(); （2）注册过程12345678910111213141516171819202122232425262728293031// AbstractSelectableChannel#register方法SelectionKey register(Selector sel, int ops,Object att)&#123; synchronized (regLock) &#123; // 判断当前Channel是否关闭 if (!isOpen()) throw new ClosedChannelException(); // 判断参数ops是否只包含OP_ACCEPT if ((ops &amp; ~validOps()) != 0) throw new IllegalArgumentException(); // 使用Selector则Channel必须是非阻塞的 if (blocking) throw new IllegalBlockingModeException(); // 根据Selector找到SelectionKey，它是可复用的，一个Selector只能有一个SelectionKey，如果存在则直接覆盖ops和attachedObject SelectionKey k = findKey(sel); if(key != null) &#123; .... &#125; // 如果不存在则直接实例化一个SelectionKeyImpl对象，并为ops和attachedObject赋值；实际调用AbstractSelector的register方法 // 将Selector和SelectionKey绑定 if (k == null) &#123; // New registration synchronized (keyLock) &#123; if (!isOpen()) throw new ClosedChannelException(); k = ((AbstractSelector)sel).register(this, ops, att); addKey(k); &#125; &#125; return k; &#125; &#125; （3）select过程select是Selector模型中最关键的一步，下面让我们来研究一下其过程 12345678910111213141516171819202122232425262728293031323334353637// 首先来看select的调用链// SelectorImpl#select -&gt; SelectorImpl#lockAndDoSelect -&gt; 具体provider提供的Selector中的doSelect方法// 值得注意的是：在lockAndDoSelect方法中执行了`synchronized(this)`操作，故select操作是阻塞的// open过程中我们知道，Selector有好几种实现，但基本都包含以下操作;感兴趣的同学可以具体看看这位大神写的博客：https://juejin.im/entry/5b51546df265da0f70070b93；这里就不深入写这部分了，篇幅有点长int doSelect(long timeout) &#123; // close判断，如果closed，则抛出ClosedSelectorException // 处理掉被cancel掉的SelectionKey，即`cancelled-key` this.processDeregisterQueue(); try &#123; // 设置中断器，实际调用的是AbstractSelector.this.wakeup();方法 // 调用的是方法AbstractInterruptibleChannel.blockedOn(Interruptible); this.begin(); // 从具体的模型中(kqueue、poll、epoll)选择 this.pollWrapper.poll(...); &#125;finally &#123; // 关闭中断器 this.end(); &#125; // 重新处理被cancel的key this.processDeregisterQueue(); // 更新各个事件的状态 int selectedKeys = this.updateSelectedKeys(); // 可能还有一些操作 ..... return selectedKeys; &#125; 疑惑点Q：各事件分别在什么条件下就绪？ OP_ACCEPT:客户端向服务端发起TCP连接建立【服务端的代码监听】 OP_CONNECT:客户端与服务端的连接建立成功或失败【客户端代码监听】 OP_READ:客户端向服务端发请求或服务端向客户端写入数据时 OP_WRITE:判断缓冲区是否有空闲空间 FYI java NIO selector全面深入理解 Java NIO中Write事件和Connect事件 Java NIO 的前生今世 之四 NIO Selector 详解 Java NIO分析(8): 高并发核心Selector详解]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java,nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征选择]]></title>
    <url>%2F2017%2F12%2F03%2F2017-12-03-feature_selection%2F</url>
    <content type="text"><![CDATA[特征选择方法 1. General 上图为数据挖掘或机器学习基本的场景描述，当特征提取后进行预处理后，此时需要选择对训练模型有意义的特征，这个过程我们称为特征选择。 特征选择：也称特征子集选择（Feature Subset Selection , FSS），或属性选择（Attribute Selection）；是指从特征集选取一个特征子集，使构造出来的模型更好。 特征选择技术的常常用于许多特征但样本（即数据点）相对较少的领域。特征选择应用的典型用例包括：解析书面文本和微阵列数据，这些场景下特征成千上万，但样本只有几十到几百个 1.1 Why在机器学习实际运用中，特征数量往往非常多，其中可能存在许多不相关的特征，特征之间也可能存在相互依赖，容易导致的后果： 训练时间变长 维度灾难，模型复杂 过拟合(决策树) 特征选择能剔除irrelevant或redundant的特征，从而达到减少特征个数、提升模型准确度、减少训练时间的目的 2. Procedure通常来说，会从两个方面来考虑选择特征： 特征是否发散：如果样本在某特征上基本无差异，则该特征则对样本的区分无意义 特征与目标的相关性：与目标相关性高的特征，应该作为最优特征 特征选择一般过程如下： 产生过程(Generation Procedure)：从特征全集中选取中特征子集 损失函数评价(Evaluation Function)：用损失函数对该特征进行评价 停止准则比较(Stopping Criterion)：若评价结果比停止准则(一般为阈值)差，则重复1,2步骤，否则走4 验证过程(Validation Procedure)：对特征子集验证其有效性 2.1 Generation Procedure产生过程是搜索特征子空间的过程。如果特征全集包含N个特征，则要生成的候选特征子集的总数是$2^N$，即使是中等规模的N，这也是个巨大的数字。解决这一问题的算法有完全搜索(Complete)，启发式搜索(Heuristic)，随机搜索(Random) 2.1.1 Complete完全搜索分为穷举搜索(Exhaustive)和非穷举搜索(Non-Exhaustive)两类 广度优先搜索(Breadth First Search) 广度优先遍历特征子空间；枚举了所有特征组合，时间复杂度为$O(2^n)$，实用性不高 分支限界搜索(Branch and Bound) 穷举搜索的基础上加上了分支限界；例如：剪掉某些不可能搜索出比当前最优解更优的分支。 定向搜索(Beam Search) 首先选择N个得分最高的特征作为特征子集，将其加入一个限制最大长度的优先队列，每次从队列中取得得分最高的子集，然后穷举向该子集加入1个特征后产生的所有特征集，将这些特征集加入队列 最优优先搜索(Best First Search) 在定向搜索的基础上不限制优先队列的长度 2.1.2 Heuristic 序列前向选择(SFS , Sequential Forward Selection) 从空集开始，每次选择一个特征加入子集X，使得特征函数$F(X)$最优； 缺点：无法剔除特征 序列后向选择(SBS , Sequential Backward Selection) 从特征全集开始，每次剔除一个特征，得到子集X，使得特征函数$F(X)$最优； 缺点：无法加入剔除的特征 双向搜索(BDS , Bidirectional Selection) 使用SFS和SBS同时构建子集X，直到两者的X相同时，停止搜索 增L去R选择算法(LRS , Plus-L Minus-R Selection) 从空集开始，每次加入L个，减去R个，选最优（L&gt;R)或者从全集开始，每次减去R个，增加L个，选最优(L&lt;R)。 序列浮动选择(Sequential Floating Selection) 在LRS的基础上取浮动的L和R参数 决策树(DTM , Decision Tree Method) 决策树的剪枝过程。评价函数为信息增益 2.1.3 Random 随机产生序列选择算法(RGSS , Random Generation plus Sequential Selection) 随机产生一个特征子集，然后使用SFS或SBS求解，最终求得各个子集的最优解 模拟退火算法(SA , Simulated Annealing) 选定一个点，求得改点的解，并迭代之后点的解进行比较求得最优解；但在一定概率范围内可认为比最优解大的解继续向后迭代，最终取得近似全局最优解 遗传算法(GA , Genetic Algorithms) 随机产生一组特征子集，并用评价函数进行评分，通过交叉、突变形式繁殖出下一代特征子集，经过N此淘汰后可得到评价函数最高的特征子集 随机算法缺点：依赖随机因素，有实验结果难以重现 2.2 Evaluation Function评价函数的作用是评价产生过程所提供的特征子集的好坏。 主要有三大类： Filter：过滤法 Wrapper：包装法 Embedded：集成法 2.2.1 Filter过滤法通过分析特征子集内部的特点来衡量好坏。一般用作预处理 按照发散性或相关性对各个特征进行评分，设定阈值或者待选择特征的个数，选择特征 常用方法： 方差阈：计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。默认情况下去除方差等于0的特征 相关系数法：计算各个特征对目标值的相关系数以及相关系数的P值；好的特征子集包含的特征应该与分类的相关度较高，而特征之间相关度较低 卡方检验：只能用于二分类，检验定性自变量对定性因变量的相关性 互信息法：也是检验定性自变量对定性因变量的相关性 一致性：若样本1与样本2属于不同的分类，但在特征A、B上的取值完全一样，那么特征子集{A,B}不应该选作最终的特征集 信息增益：ID3算法中的信息增益比较，取信息增益较高的特征子集 优缺点： 优点： 执行高效：通过涉及数据集上的非迭代运算 通用性：评估数据的固有属性，而不是评估特定分类器的相互作用 缺点： 大型子集的倾向性：由于目标函数通常是单调的，所以其更倾向与选择全特征集作为最优解，故不得不使得用户设定阈值中断 2.2.2 WrapperObjective Function是一个模式分类器，它通过统计重采样或交叉验证，预测准确度来评估特征子集 常用方法： 递归特征消除法：使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练 优缺点： 优点： 准确性：比Filter获得更好的识别率 泛化能力：避免过拟合的机制 缺点 执行速度慢：必须为每个特征子集训练分类器 缺乏通用性：与评价函数所使用的分类器的偏向有关 2.2.3 Embedded集成法，先使用某些机器学习的算法或模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣 常用方法： 基于惩罚项的特征选择法：筛选特征、降维 基于树模型的特征选择法：GBDT FYI 机器学习之特征工程-特征选择 Ricardo Gutierrez-Osuna, Introduction to Pattern Analysis ( LECTURE 11: Sequential Feature Selection ) Feature Selection for Classification 特征选择常用算法综述]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>feature</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2017%2F11%2F13%2F2017-11-13-decision_tree%2F</url>
    <content type="text"><![CDATA[决策树生成的原理及算法研究 概述决策树是一个预测模型；代表的是对象属性与对象值之间的一种映射关系。 决策树的思想：从数据集合中的知识信息提取出一系列规则，并根据规则逐步构建树 如上图，一般的决策树包含三种类型的节点（类似流程图）： Decision nodes：用矩形表示，如1,2； Chance nodes：用圆形表示 End nodes：用三角形表示 优缺点： 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据 缺点：可能会产生过度匹配问题；对于各类别样本数量不一致的数据，信息增益的结果会更偏向于更多数值的特征 适用数据类型：数值型和标称型 特征选择熵(entropy)被定义为信息的期望值，表示随机变量不确定性的度量。 如果待分类的事情可能被划分在多个分类中，则符号$x_i$的信息定义为： l(x_i) = - \log_{2}p(x_i)其中$p(x_i)$是选择该分类的概率。 熵的公式定义如下： H = - \sum_{i=1}^n p(x_i) \log_2 p(x_i)其中n是分类的数据。意思是一个变量可能的变化越多，它携带的信息量越大 设有随机变量(X,Y)，其联合概率分布为 P(X = x_i , Y = y_i) = p_{ij} \qquad i=1,2,\cdots,n;\quad j=1,2,\cdots,m条件熵H(Y|X)表示在已经随机变量X的条件下随机变量Y的不确定性，定义为 H(Y|X) = \sum_{i=1}^n p_i H(Y | X = x_i)12345678910111213141516# 如果数据集dataset = [[x1 , x2 , ... , xn , label] , ... ] 则香农熵计算公式如下def calcShannonEnt(dataset) : labelCount = len(dataset) labelCounts = &#123;&#125; # 得到各标签出现的次数 for row in dataset : label = row[-1] if label not in labelCounts.keys() : labelCounts[label] = 0 labelCounts[label] += 1 shannonEnt = 0.0 for label in labelCounts : # 标签出现的概率 p = float(labelCounts(label)) / labelCount shannonEnt -= prob * log(p , 2) return shannonEnt 信息增益信息增益表示得知特征X的信息而使得Label{Y}的信息不确定性减少的程度。特征A对训练数据集D的信息增益g(D,A)，定义为数据集D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即 g(D,A) = H(D) - H(D | A)一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息.决策树学习中的信息增益等价于训练数据集中类与特征的互信息 信息增益比以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题.使用信息增益比(information gain ratio)可以对这一问题进行校正. 定义：特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益g(D,A)与训练数据集D关于特征A的值的熵$H_A(D)$之比，即 g_R(D,A) = {g(D,A) \over H_A(D)}其中，$H_A(D) = - \sum_{i=1}^n {|D_i| \over |D|} \log_{2} {|D_i| \over |D|} $ ，n是特征A取值的个数 决策树生成决策树的构建步骤： 收集并准备数据：数值型数据必须离散化 特征选择：计算数据集中各特征的信息增益值，选取最优特征(信息增益值最大)划分数据集 类别划分：计算最优特征中对应的最优类别 创建子树：递归执行2，直到无特征划分数据集 ID3算法核心：在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树 输入：数据集D，特征集A，阈值$\varepsilon$ 输出：决策树T 若D中所有实例都属于同一label，则T为单结点树，并将该label作为其取值，返回T 若$A=\varnothing$，则T为单结点树，并将D中label出现次数最多的作为其取值，返回T 否则，计算A特征集中对D的信息增益，选取最大的值作为最优特征$A_g$ 若$A_g$的信息增益小于$\varepsilon$ ，则T为单结点树，并将D中label出现次数最多的作为其取值，返回T 否则对$A_g$中的每一个取值$a_i$，依$A_g = a_i$将D划分为若干个非空子集$D_i , i = 1,2,\cdots,k$，其中k为$A_3$的取值个数；将$D_i$中label出现次数最多的作为其取值，构建子结点，由结点及其子结点构建树T并返回 对第$i$个子结点，以$D_i$为训练集，以不包括$A_g$的新$A_k$作为特征集，递归地调用上述步骤，得到子树$T_i$并返回 C4.5的生成算法与ID3算法相似，只需将ID3算法中选择特征用的信息增益值替换为信息增益比 Example给定以下数据集，为其构建决策树 （1）计算熵和信息增益 label个数： 是（9） 否（6） H(D) = - {9 \over 15} \log_2 {9 \over 15} - {6 \over 15} \log_2 {6 \over 15} = 0.971分别以$A_1$，$A_2$，$A_3$ ，$A_4$表示年龄、有工作、有房子和信贷情况4个特征，则各自信息增益如下 以年龄为例，count(青年) = 5 , 其中类别是（2），否（3） count(中年) = 5 , 其中类别是（3），否（2） count(老年) = 5 , 其中类别是（4），否（1） \begin{equation} \begin{split} g(D,A_1) &= H(D) - \left[ {5 \over 15}H(D_1) + {5 \over 15}H(D_2) + {5 \over 15}H(D_2) \right] \\ &= 0.971 - \left[ {5 \over 15}(-{2 \over 5}\log_2{2 \over 5}-{3 \over 5}\log_2{3 \over 5}) + \\ {5 \over 15}(-{3 \over 5}\log_2{3 \over 5}-{2 \over 5}\log_2{2 \over 5}) + {5 \over 15}(-{4 \over 5}\log_2{4 \over 5}-{1 \over 5}\log_2{1 \over 5}) \right] \\ &= 0.971 - 0.888 \\ &= 0.083 \end{split} \end{equation}同理可计算出 \begin{aligned} & g(D , A_2) = 0.324 \\ & g(D , A_3) = 0.420 \\ & g(D , A_4) = 0.363 \\ \end{aligned}最后比较信息增益值，由于$A_3$的信息增益值最大，则选取$A_3$作为最优特征 （2）划分数据集 根据$A_3$的取值将数据集D划分为若干个非空$D_i , i = 1,2,\cdots,k$，其中k为$A_3$的取值个数；若$D_i$中所有实例label相同，则将此结点置为叶结点，这里的当有自己的房子=是，label全等于是，所以当特征$A_3$=是，label=是；否则对划分的数据集$D_i$递归执行(1),(2)步骤。 决策树的剪枝 使用决策树生成算法产生的决策树，对训练数据的分类很准确，但对于未知的特征取值的分类却没有那么准确，即出现过拟合现象，过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。为了解决这个问题，我们需要考虑降低决策树的复杂度，对已生成的决策树进行简化 — 称为剪枝 剪枝：从已生成的决策树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化树模型 决策树的剪枝往往通过极小化决策树整体的损失函数来实现。设树T的叶结点（可以理解为特征的每个取值）个数为|T|，t是树T的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$k=1,2,\cdots,K$，$H_t(T)$为叶结点t上的熵，$\alpha \ge 0$为参数，则决策树学习的损失函数可以定义为 C_\alpha(T) = \sum_{t=1}^{|T|}N_t H_t(T) + \alpha |T|其中叶结点t的熵为 H_t(T) = - \sum_{k=1}^K {N_{tk} \over N_t} \log_2{N_{tk} \over N_t}令 C(T) = \sum_{t=1}^{|T|}N_t H_t(T) = - \sum_{t=1}^{|T|} \sum_{k=1}^K N_{tk} \log_2{N_{tk} \over N_t}则 C_\alpha(T) = C(T) +\alpha |T|C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数$\alpha \ge 0$控制两者之间的影响.较大的$\alpha$促使其得到简单的树，较小的$\alpha$促使其得到较复杂的树，$\alpha = 0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度 树的剪枝算法 输入：生成算法产生的决策树T，参数为$\alpha$ 输出：修剪后的子树$T_\alpha$ （1）计算每个结点的熵 （2）递归地从树的叶结点向上回缩 设一组叶结点回缩到其父节点之前与之后的整体树分别为$T_B$与$T_A$，其对应的损失函数值分别是$C_\alpha (T_B)$与$C_\alpha (T_A)$，如果$C_\alpha (T_A) \le C_\alpha (T_B)$，则进行剪枝，将其父节点变为新的叶结点 FYIDecisionTree 信息增益 机器学习实战 统计学习方法]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>algorithm , tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正态分布]]></title>
    <url>%2F2017%2F11%2F04%2F2017-11-04-normal_distribution%2F</url>
    <content type="text"><![CDATA[正态分布的研究 定义正态分布(Normal Distribution)又名高斯分布(Gaussian Distribution)，是现代社会中被大量运用的一种概率分布。 若随机变量Ｘ服从一个位置参数为μ、尺度参数为σ的概率分布，记为： X \sim N( \mu , \sigma^2)其概率密度函数为： f(x) = \frac{1}{\sqrt{2\pi}\sigma} e^ - \frac{ (x - \mu)^2 }{2\sigma^2}其曲线如下： 若一个指标受到许多因素的影响，并且其中任何一个因素都不对其产生决定性的影响，那么该指标的值很可能近似于正态分布。 独立同分布的随机变量；随机变量多次平均稳定后的概率服从正态分布 N个正态分布的变量的线性组合也服从正态分布。 二项分布的极限分布是正态分布 当样本量N趋于无穷时，其极限分布都有正态的形式 类型： 严格正态分布：一般只会出现在理想的物理系统中，例如热力学系统中最大熵原理导致理想气体分子的速度服从正态分布 近似正态分布：如果一些变量服从相似的分布并且相关性较弱，那么它们的平均值就是近似正态分布的 假设正态分布：多数情况下，可以假设变量的分布服从正态分布；比如说误差分析 推导假设已知随机变量X服从参数为n和p的二项分布，记作X ~ B(n,p)，那么其概率密度函数为 \tag{1} b(n , p , i) = {n \choose k} p^k (1-p)^{n-k}求X落在二项分布中心点一定范围的概率$P_d = P(|X-np| \leq d)$ 证明： 考虑p=1/2的情形，代入二项分布概率密度函数可得 \tag{2} b(i) = b(n , {1 \over 2} , i) = {n \choose i}({1 \over 2})^n \tag{Striling公式} n! \sim \sqrt{2\pi n}({n \over e})^n \tag{3} b({n \over 2}) = {n! \over {n \over 2}! (1 - {n \over 2})! }({1 \over 2})^n = { \sqrt{2\pi n }( {n \over e} )^n \over ( \sqrt{ 2\pi {n \over 2} }({ {n \over 2} \over e})^{n \over 2})^2} = \sqrt{ {2 \over \pi n} } \tag{4} {b({n \over 2} + d) \over b( {n \over 2} ) } \sim e^{-{2d^2 \over n} } \tag{5} b({n \over 2} + d) \sim {2 \over \sqrt{2 \pi n} }e^{-{2d^2 \over n} }使用上式的结果，并在二项概率累加求和的过程中近似的使用定积分代替求和，很容易就能得到 \tag{6} P(|\frac{X}{n} - \frac{1}{2}| \le \frac{c} {\sqrt{n} } ) \sim \int_{-2c}^{2c} \frac{1}{\sqrt{2\pi}} e^{-{x^2 \over 2} } dx从而证得二项分布的极限分布是正态分布 中心极限定理设随机变量 $ X_n (n=1,2,\cdots) $服从参数为 p 的二项分布，则对任意的x, 恒有 \tag{中心极限定理} \lim_{n\rightarrow\infty} P\{ \frac {X_n - np} {\sqrt{np(1-p)} } \le x \} = \int{-\infty}^x {1 \over \sqrt{2\pi} } e^{ {-t^2 \over 2} }dt中心极限定理：中心极限定理说的是变量的观测值的平均数标准化后的极限分布是正态 误差分布曲线误差的一些定性描述： 误差是对称分布的；误差分布函数f(x)关于零点对称分布，概率密度随|x|增加而减小 大的误差出现频率低，小的误差出现频率高 设真值为$\theta$，$x_1,\cdots,x_n$为n次独立测量值，每次测量的误差为$e_i = x_i - \theta$，假设误差$e_i$的密度函数为$f(e)$，则测量值的联合概率为n个误差的联合概率，记为 \tag{1} L(\theta) = L(\theta;x_1,\cdots,x_n) = f(e_1) \cdots f(e_n) = f(x_i - \theta) \cdots f(x_n - \theta)假设误差分布导出的极大似然估计 = 算术平均值，则取$L(\theta)$达到最大值的$\hat{\theta} = \hat{\theta}(x_1,\cdots,x_n)$作为$\theta$的估计值，即 \tag{2} \hat{\theta} = \mathop{arg max}_{\theta} L(\theta)对(1)式两边同时取对数得 \ln{L(\theta)} = \sum_{i-1}^n \ln{f(x_i - \theta)}求导得 {d\ln{L(\theta}) \over d\theta} = \sum_{i=1}^n {f^\prime(x_i - \theta) \over f(x_i - \theta)}为求极大似然估计，令 \tag{3} {d\ln{L(\theta}) \over d\theta} = 0整理后得到 \tag{4} \sum_{i=1}^n {f^\prime(x_i - \theta) \over f(x_i - \theta)} = 0令$g(x) = {f^\prime (x) \over f(x)}$ \tag{5} \sum_{i=1}^n g(x_i - \theta) = 0由于上面我们假设极大似然估计的解$\theta$等于算术平均$\bar{x}$，带入(5)得到 \tag{6} \sum_{i=1}^n g(x_i - \bar{x}) = 0(6)式中取n=2，有 g(x_1 - \bar{x}) + g(x_2 - \bar{x}) = 0由于定性中指出f(x)是关于零点对称分布，所以f(x)应满足g(x) = g(-x)，所以此时有$x_1 - \bar{x} = -(x_2 - \bar{x})$，并且$x_1,x_2$是任意的，有此得到 \tag{奇函数} g(-x) = -g(x)(6)式中再取n=m+1，并且要求$x1=\cdots=x_m = -x , x_m+1 = mx$，则有$\bar{x} = 0$， \bar{x} = {-x-x,\cdots-x+mx \over m+1} = 0并且 \sum_{i=1}^n g(x_i - \bar{x}) = mg(-x) + g(mx)所以得到 g(mx) = mg(x)而满足上式的唯一的连续函数只有$g(x) = cx$，积分后从而进一步可以求解出 \begin{aligned} & \because {d\ln{f(x)} \over dx} = {f^\prime(x) \over f(x)} \\ & \therefore \ln{f(x)} = \int {f^\prime(x) \over f(x)} dx = \int cx dx = {1 \over 2}cx^2 + c \\ & \therefore f(x)=e^{ {1 \over 2} cx^2 + c} = Me^{ {1 \over 2} cx^2 } (M = e^c ) \\ \end{aligned}由于$f(x)$是概率分布函数，把$f(x)$正规化后可得到正态分布密度函数$N(0,{\varrho}^2)$ 由$\int_{-\infty}^{\infty} f(x)dx = 1 $(概率密度函数的面积=1)，所以定有c&lt;0，取$c=-{1 \over \sigma^2}$，则有$M={1 \over \sqrt{2\pi}\sigma}$ ，所以 \begin{split} & 令 y = {x \over \sqrt2 \sigma} ;则 dx = \sqrt2\sigma y \\ & \because \int_{-\infty}^{\infty}e^{-x^2} dx = \sqrt{\pi} \\ &\therefore \int_{-\infty}^{\infty}Me^{ {1 \over 2}-{1 \over \sigma^2}x^2} dx = M \int_{-\infty}^{\infty}e^{-y^2}(\sqrt2\sigma dy) = M\sqrt2\sigma \int_{-\infty}^{\infty}e^{-y^2}dy \\ &= M\sqrt{2\pi}\sigma = 1 \\ &\therefore M = {1 \over \sqrt{2\pi}\sigma} \\ &f(x) = {1 \over \sqrt{2\pi}\sigma} e^{-{x^2 \over 2 \sigma^2} } \end{split}则$(e_1,\cdots,e_n)$的联合概率分布为 (e_1,\cdots,e_n) \sim f(e_1)\cdots f(e_n) \sim {1 \over (\sqrt{2\pi}\sigma)^n} e^{-{1 \over 2\sigma^2} \sum_{i=1}^n(e_i)^2}要使得这个概率最大，必须使得$\sum_{i=1}^n (e_i)^2$取最小值，这正好就是最小二乘法的要求 FYIintro-normal-distribution 正态分布 还原正态分布之高斯推导过程 对高斯分布函数形式的推导]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法汇总]]></title>
    <url>%2F2017%2F11%2F04%2F2017-11-04-machine_learning_algorithm%2F</url>
    <content type="text"><![CDATA[机器学习算法的分类以及详细介绍 算法Mathematical formulationL-P范数范数是一种强化了的距离概念。包括 向量范数：向量空间中向量的大小 矩阵范数：矩阵引起的变化的大小 公式定义：$Lp=\sqrt[p]{\sum\limits_{1}^n x_i^p}，x=(x_1,x_2,\cdots,x_n)$ 根据P 的变化，范数也有着不同的变化，一个经典的有关P范数的变化图如下： 当p=0时，即L0范数，主要被用来度量向量中非零元素的个数 当p=1时，即L1范数(也被称作稀疏规则算子—Lasso regularization)，表示向量x中非零元素的绝对值之和；主要用来度量两个向量间的差异；(曼哈顿距离、最小绝对误差) 如绝对误差和(Sum of Absolute Difference) SAD(x_1,x_2) = \sum_i \left|x_{1i} - x_{2i}\right| 当p=2时，即L2范数，表示向量元素的平方和再开方。常见的L2范数有欧式距离公式，或如平方差和(Sum of Squared Difference) SAD(x_1 , x_2) = \sum_i (x_{1i} - x_{2i})^2 当p=∞时，也就是L-∞范数，主要被用来度量向量元素的最大值 ​ 监督学习监督学习(supervised learning)：利用样本输入和期望输出来学习如何预测的技术 K-近邻 决策树(decision trees) 梯度下降/上升(gradient-descent / gradient-boosted ) 逻辑回归(logistis regression) 贝叶斯过滤(naive Bayes) 交替最小二乘法(Alternative least squares) 支持向量机(SVMs) 随机森林(random forests) 神经网络 无监督学习无监督学习(unsupervised learning)：从一组数据中找寻某种结构 聚类 负矩阵因式分解(non-negative matrix factorization) 自组织映射(self-organizing maps) ML步骤 收集数据，筛选特征 模型算法选取 生成cost function，得出假设函数 新值代入假设函数 K-近邻步骤如下： 计算已知类别数据集中的点与当前点的距离(欧式距离公式) 按照距离递增排序 选取与当前点距离最小的前k个点 确定前k个点所在类别出现的概率 返回前k个点出现概率最高的类别(即当前点预测分类) 决策树假设有n个特征，依次根据数据增益情况求出最优特征，然后根据特征求出对应的分支 根据数据集选取出最佳特征 计算该特征对应的值分别对应的结果标签—- 可能是结果标签，也可能是其他特征的决策树]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集体智慧编程笔记]]></title>
    <url>%2F2017%2F10%2F29%2F2017-10-29-collective_intelligence_notes%2F</url>
    <content type="text"><![CDATA[阅读集体智慧编程所做的一些笔记 集体智慧导言集体智慧：为了创造新的想法，而将一群人的行为、偏好和思想组合在一起 机器学习：是人工智能（AI,artificial intelligence）领域中和算法相关的一个子域，它允许机器不断地学习。大多数情况下，将一组数据传递给算法，由算法推断出与这些数据的属性相关的信息。数据-&gt;训练-&gt;模型-&gt;预测-&gt;结果。 局限性：只能凭借已有数据进行归纳训练，而且归纳方式也有局限性 提供推荐协作型过滤Collaborative filtering：对一大群人进行搜索，并从中找出品味相近的一小群人 条件：人、特征、特征值 相似度计算函数(值越大，相似度越高)： 欧几里得距离：当距离越大，则偏好越近；当=1时则代表偏好相同 \frac{1}{1+\sqrt{\left({x_2-x_1}\right)^2 + \left({y_2-y_1}\right)^2}}​ 皮尔逊相关度：判断两组数据与某一直线拟合程度的一种度量。 找出两者都存在的特征 计算两者特征值总和(sum1,sum2)、平方总和(sumSq1,sumSq2)以及乘积之总和(pSum) 之后按如下公式进行计算 123456# 计算皮尔逊评价值num = pSum - (sum1 * sum2 / n)den = sqrt((sumSq1 - pow(sum1 , 2) / n) * (sumSq2 - pow(sum2 , 2) / n))if den == 0 : return 0r = num / den num = pSum - \frac{sum1-sum2}{n} den = \sqrt{\left(sumSq1 - \frac{sum1^2}{n}\right) * \left(sumSq2 - \frac{sum2^2}{n}\right) } 曼哈顿距离算法 Jaccard/Tanimoto系数(Tanimoto coefficient)：数据中交集和并集的比例 123456789101112'''若数据集中取值只有0或1，则可定义度量如下下列代码使用1-系数的作用是使得值越小代表相似度越高'''def tanimoto(v1 , v2) : c1 , c2 , shr = 0 , 0 , 0 for i in range(len(v1)) : if v1[i] != 0 : c1 += 1 # 出现在v1中 if v2[i] != 0 : c2 += 1 # 出现在v2中 if v1[i] != 0 &amp;&amp; v2[i] != 0 : shr += 1 # 同时出现在v1和v2中 return 1.0 - (float(shr) / (c1 + c2 - shr)) ​ 互信息系数：度量非线性相关性 更多相似度计算函数 推荐用户 通过相似度计算函数计算除当前用户外的所有用户的相似度 将相似度值从大到小依次排序 取得前n个结果 推荐物品 构建相似度表 用户 相似度 物品 S.x物品 xxx 取值为0~1 用户评价值 用户评价值 * 相似度 总计 $\sum_{i=1}^n s_i$ Sim.Sum 相似度之和(该用户所有评论的物品) 总计/Sim.Sum . 将相似度表从大到小排序 发现群组分级聚类分级聚类：通过连续不断地将最为相似的群组两两合并，来构造出一个群组的层级结构（每个群组都是由单一元素开始的） 群组数据结构 1234567891011'''聚类群组(层级树)：以树状结构表示可通过PIL(Python Imaging Library)进行树状图绘制'''class bicluster: def __init__(self , vec , left = None , right = None , distance = 0.0 , id = None): self.vec = vec # 数据集，两个群组合并后的数据集为其均值 self.left = left # 左节点 self.right = right # 右节点 self.id = id # 聚类唯一id，合并聚类的id为负数 self.distance = distance # 群组的距离 步骤 初始化聚类群组列表(数据集的每一行为一个群组) loop start：计算每个群组间的距离（可参考上述的相似度计算函数），找到距离最小的两个群组x和y 计算距离最小的两个群组的均值—&gt;作为新群组的数据集 构造新群组 从群组列表中删除x和y，并将新群组加入到群组列表中 loop end 返回群组列表第一个元素 —&gt;其为聚类最终结果 缺点 在没有额外投入的情况下，树状图不会真正将数据拆分到不同组 该算法的计算量很大，时间复杂度为$n^3$ K-均值聚类K-均值聚类：在分级聚类的基础上添加k个中心点的概念，即最终确定的聚类是k个，而非1个 步骤 随机确定k个中心位置， loop start：将各个群组分配到最临近的中心点； 分配完成后，聚类中心点会移动到该聚类的的所有节点的平均位置处 当分配过程不再发生变化，结束循环loop end 搜索与排名构建搜索引擎的步骤一般如下： 数据搜集：获取网页url、内嵌网页等 建立索引：创建网页中单词与网页的索引，附属信息有：位置及归属网页等 搜索排名：根据单词搜索结果，使用不同的度量方法进行排名；方法有以下几种 基于内容排名 单词频度 文档位置 单词距离 利用外部回指链接 简单计数法 - PageRank算法 - 基于链接文本的pr值 基于点击行为的神经网络 归一化函数对于评价值来说，其特点有值区间不确定性、可比较性差；为了让其具有相同的值域和变化方向，需要对结果归一化处理。 而评价值的大小比较，取决于评价方法而定，有的评价方法评分值越大越好，有的则越小越好。 下列是该书中的归一化处理函数： 1234567891011def normalizescores(self , scores , smallIsBetter = 0): vsmall = 0.00001 # 避免被零整除 # 当值越小时，评价值越高 if smallIsBetter : minscore = min(scores.values()) return dict([(u , float(minscore) / max(vsmall , l)) for u , l in scores.items()]) else : maxscore = max(scores.values()) if maxscore == 0 : maxscore = vsmall return dict([(u, float(c) / maxscore) for u, c in scores.items()]) 归一化资料可参考： 离差标准化和Z-Score标准化) 数据的标准化和归一化 基于内容排名顾名思义，将网页搜索结果根据网页内容进行评价，根据评价结果进行排名 单词频度单词频度：根据搜索词在网页中出现的次数对网页进行评价 缺陷：若搜索词在某个网页中出现次数很多，可是该网页却不是最优结果 文档位置文档位置：根据搜索词在文档中的位置之和进行评价，位置越靠前评价值越高 单词距离单词距离：根据查询的多个搜索词在网页中的距离之和进行评价，距离越小评价值越高 利用外部回指链接简单计数简单计数：根据网页被其他网页引用次数进行评价，引用次数越多评价值越高 PageRank算法PageRank算法由Google创始人(以Larry Page命名)发明，该算法为每个网页都赋予了一个指示网页重要程度的评价值 计算公式： 假设有A,B,C,D四个网页，那么PR(A)的计算公式如下： PR(A) = 0.15 + 0.85 * (PR(B) / linkcount(B) + PR(C) / linkcount(C) + PR(C) / linkcount(C)) 阻尼因子=0.85，指示用户持续点击每个网页中链接的概率为85% 利用链接文本利用链接文本：根据网页的链接文本来决定网页的相关程度 描述：根据搜索词得到网页A和B，A-&gt;B，若B在搜索结果列表中，则把B的评价值加上A的pr值 1234567搜索结果列表for 搜索词 in 搜索词列表 : 获取搜索词对应的网页链接关系表[(from , to) , ...] for (from , to) in [(from , to) , ...]: if to in 搜索结果列表 : pr = pr(from) 搜索结果列表[to] += pr 优化solve：协作类问题 擅长处理：受多种变量影响，存在许多可能解的问题，以及结果因这些变量的组合而产生很大变化的问题 优化算法是通过尝试许多不同题解并给这些题解打分以确定其质量的方式来找到一个问题的最优解。 步骤： 描述题解 确定成本函数(The Cost Function) 选取优化算法 随机搜索算法思想：构造可能出现的解题输入参数组合，并代入到成本函数，将每组解进行比较，得到最优解（如下图，从所有题解中找到最优解—这里只标记了几个局部最优解，进行比较） 缺点：该算法是到处跳跃的(jumps around)，非常低效；其不会自动寻找已经发现的最优解相近的题解 爬山法算法思想：从一个随机搜索题解开始，然后在其临近的题解中寻找最优解；下图中假设找到红点A，将临近往上或往下的点进行解计算后，找到更优解，重复该步骤，直到邻近的解都比该解大，则认为是该点是最优解 对比：相较于随机搜索，其具有更低的成本 缺点：算法结果得到的其实不是全局最优解，而是局部最优解(类似于Gradient descent) 为了得到全局最优解，其算法有个更好的替代方案—随机重复爬山法(random-restart hill climbing),即让爬山法以多个随机生成的初始解为起点运行若干次，借此希望其中有一个解能逼近全局最优解 模拟退火算法退火：将合金加热后再慢慢冷却的过程 算法思想：以一个问题随机解开始，通过退火期间的迭代，随机选取题解中的某个数字，然后朝某个方向变化；算法执行过程中实际计算当前题解和新题解(即随机选取的数字计算的解)，当新题解优于当前题解时，则新题解替换当前题解；若新题解劣于当前题解，则需要判断新题解是否在可接受范围，若可接受，则替换当前题解 退火过程如下： 条件 —&gt; 温度(temperature) , 冷却因子(coolfactor) 公式 —&gt; temperature = temperature * coolfactor 接受范围值计算公式： p = e^{(-(highcost - lowcost) / temperature)} 由于初始温度很高，所以p初始时将无限趋近于1，随着题解的差异越来越大，概率越来越低，因此该算法更倾向于稍差的解，而不会是最差的解 核心代码： 12345678910111213141516171819'''p : 可能的题解范围costfunction : 成本计算函数t : 初始温度cf : 冷却因子'''def annealingoptimize(p , costfunction , t , cf) : while t &gt; 0.1 : i # 当前题解参数 j # 新题解参数 # 计算当前题解 currentcost = costfunction(i) # 移动方向，计算新题解 newcost = costfunction(j) # 这里为什么使用random.random()? # 因为如果使用上一次的p，则大概率的可能只能求取到局部最优解 if newcost &lt; currentcost or random.random() &lt; pow(math.e , - (newcost - currentcost) / t ) : currentcost = newcost t = t * cf 遗传算法算法思想：通过成本函数建立种群，在种群中通过精英选拔法(elitism)得到最优解，种群剩余的解再通过修改题解的参数得到全新的题解，两者组合得到新种群；通过重复以上过程n次后，最终得到的题解为最优解 名词概念： 种群(population)：随机生成一组解 精英选拔法：种群中位于最顶层的题解 修改题解 变异(mutation)：对现有解进行微小的、简单的、随机的改变 交叉(crossover)或配对(breeding)：随机选取两个解，并将两者按某种方式进行结合 伪代码： 12345678910111213141516171819202122232425'''popsize : 群组解个数costfunction : 成本函数n : 迭代次数'''def geneticoptimize(popsize , costfunction , n ) : pop = [] for i in range(popsize) : pop.append(costfunction(i)) for i in range(n) : # 当前种群的最优题解 sorted(pop) # 创建新群组 newpop = [] # 添加当前群组最优解 newpop.append(pop[:1]) # 添加当前群组剩余群组修改题解后的群组 remainpop = pop[1:] newpop.append(mutation(remainpop) or crossover(remainpop)) pop = newpop # 输出最优解 return pop[0] 文档过滤solve：通过算法学习并鉴别文档所属的分类 典型应用场景： 自动划分邮件类别 垃圾邮件过滤 早期的邮件过滤做法都是基于规则的分类器（rule-based classifiers）；事先定义好规则，例如单词黑名单、英文大写字母的过多使用等等 存在的问题： 过滤不准确，正常邮件被认为是垃圾邮件 文档分类的一般做法如下： 收集现有的已分类好的文档数据作为训练数据 计算文档中单词在该分类的概率P(word | category) 对2计算的概率作加权平均(当训练集过小时，会导致分类不准确) 通过学习算法计算文档属于分类的概率P(category | document) 朴素贝叶斯(NaiveBayes) 费舍尔方法(SpamBayes) 朴素贝叶斯贝叶斯定理公式： P(A | B) = \frac{ P(B | A) * P(A) } {P(B)}使用贝叶斯的条件：A和B发生的概率是相互独立，不相关的 代入到上述文档分类步骤： 计算P(document | category)，即将该document所有的word出现在该category概率相乘 P(document | category) = \prod_i^n P(word_i | category) 通过1计算的概率，代入到贝叶斯公式计算P(category | document) P(category | document) = \frac{P(document | category) * P(category)}{P(document)} 限于算法计算的概率是依赖训练集的丰富程度的，所以对文档的分类会存在不准确性的；为了规避这种不准确性，可以设置一个分类划分的阈值n— 即算法得到的分类的概率值必须大于在其他分类中的概率的n倍 费尔舍方法费尔舍方法为文档中的每个特征都求得了分类的概率，然后又将这些概率组合起来，并判断其是否有可能构成一个随机集合。 步骤： 通过已经计算好P(word | category)，来求出P(category | word)；常见方法是(具有指定特征的属于某分类的文档数) / (具有指定特征的文档总数) 属于某分类的概率clf = P(word | category) 属于所有分类的概率freqsum = P(word | category)之和 最终概率p = clf / freqsum 算出文档中所有word的概率之积 去取自然对数再乘以-2；score = -2 * log(p) 最后利用倒置对数卡方函数求得概率，得出该组概率的最大值—如果概率彼此独立且随机分布，则此计算结果满足对数卡方分布(chi-squared distribution)]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐]]></title>
    <url>%2F2017%2F10%2F29%2F2017-10-29-recommendation%2F</url>
    <content type="text"><![CDATA[推荐的一些基本介绍 什么是推荐定义：根据群体偏好来向用户推荐相关的物品 例子： Amazon、淘宝通过对用户浏览、购买等行为记录后，并推荐可能喜欢的商品 网易云音乐的私人FM 美团APP上的猜你喜欢 一般步骤： 搜集用户行为数据 建立用户-物品评价值表 计算用户-用户之间或物品-物品之间的相似度值 提供相似度最高的推荐 协作型过滤Collaborative filtering：对一大群人进行搜索，并从中找出品味相近的一小群人。算法会对这些人所偏爱的其他内容进行考察，并将它们组合起来构造出一个经过排名的推荐列表 基于用户推荐用户-物品评价值表： 物品1 物品2 物品n 用户1 3.5 4.5 4 用户2 4 3 用户3 3 4.5 用户4 4 用户5 3.5 4 4.5 相似度计算基于用户-物品评价值表，我们可以计算出用户-用户之间的相似度评价值，有若干种方法可以计算。 欧几里得距离：当距离越大，则偏好越近；当=1时则代表偏好相同 皮尔逊相关度：判断两组数据与某一直线拟合程度的一种度量。 曼哈顿距离算法 Jaccard/Tanimoto系数(Tanimoto coefficient) 互信息系数：度量非线性相关性 更多相似度计算函数 欧几里得距离在数学中，欧几里得距离是欧几里得空间中两点间’’普通“(即直线)距离。 d = \sqrt{(x_2 - x_1)^2}通过欧几里得公式我们可以计算基于用户-物品评价值表的偏好空间中用户与用户的距离，距离越近，则越相似 1234567sum = 0# useritems为用户-物品评价值表for item in useritems[u1] : if item in useritems[u2] : sum += pow(useritems[u1][item] - useritems[u2][item] , 2) similarity = sqrt(sum) 皮尔逊相关度评价皮尔逊相关系数是判断两组数据与某一直线拟合程度的一种度量（即尽可能使图中红线靠近所有坐标点） 皮尔逊相关度计算函数会返回介于-1与1之间的数据。当值为1时表明两个人对每一个物品是完全一致的评价，值为-1时则表明两者完全不相关 推荐物品为所有用户和待推荐用户计算相似度，根据相似度进行排名；这里我们得到了品味相近的人，现在想获得物品的推荐，当然我们可以从品味相近的用户中挑选尚未评价过的物品，但这样太随意了，可能存在的问题： 品味相近的用户尚未对某个物品进行评价，可能这个物品是想要的 品味相近的用户不看好某个物品，但可能这个物品是想要的 为了解决上述问题，我们需要通过一个加权的用户评价值表来进行打分 用户1的加权用户评价值表： 相似度 物品1 S(物品1) 物品2 S(物品2) 物品n S(物品n ) 用户2 0.38 3 物品1 * 相似度 3 用户3 0.89 4 4.5 用户4 0.92 4 4 用户5 0.66 4 4 4.5 总计 Sim.Sum 相似度之和(该用户所有评论的物品) 总计/Sim.Sum 总计：得到用户对物品的评价，品味相近的人，为评价值提供更多的贡献 总计/Sim.Sum：为了防止受更多人评论的物品影响最终评价结果，需要用总计值/相似度之和进行标准化 最终我们会得到用户对所有物品的评价值列表，经过按评价值排序后，最后输出推荐列表 存在的问题对于用户和物品数量级很大的情况，将一个用户和所有其他用户进行比较，然后再对每位用户评过分的商品进行比较，其速度是难以忍受的 并且也许用户在偏好方面在彼此间很少会有重叠，这可能会令计算用户的相似度判断变得十分困难 基于物品推荐基于物品的协作型过滤允许我们将大量的计算任务预先执行，从而使需要给予推荐的用户能够更快地得到他们想要的结果 总体思路： 为每件物品预先计算好最为相近的物品 推荐时，取到该用户评价过的所有物品，找到与这些物品相似的物品 根据计算相似度加权后输出评价值 与基于用户推荐的区别是：物品间的比较不会像用户间比较的那么频繁变化，这就意味着这个计算过程可以离线进行]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git系列之基础篇--push、pull、fetch]]></title>
    <url>%2F2017%2F05%2F09%2F2017-05-09-git_05%2F</url>
    <content type="text"><![CDATA[《Git系列》文章是博主阅读Git官方book提取整理出的内容，意在深入理解Git的工作机制、原理 简介通过add和commit命令使我们能将更改应用到Git本地仓库，但Git作为分布式版本控制系统，还存在一个分布式远程仓库，本地仓库只是远程仓库的一个副本。本篇将通过push、pull、fetch等命令让大家了解如何和Git远程仓库交互。 远程仓库 远程仓库是指托管在因特网或其他网络中的你的项目的版本库，本地仓库中可同时存在一个或多个远程仓库 在Git系列—Git仓库文章中也有提到远程仓库的概念，并且提到了remote命令—用于管理远程仓库 123➜ mastery001.github.io git:(develop) ✗ git remote -vorigin https://github.com/mastery001/mastery001.github.io.git (fetch)origin https://github.com/mastery001/mastery001.github.io.git (push) 通过git remote -v命令可列出当前仓库的远程仓库地址和alias，这里我们的本地仓库中有一个远程仓库了，下面我们就详细介绍如何将本地更新提交至远程或从远程同步等操作 push语法为git push remote_alias branch_name fetch语法为git fetch remote_alias branch_name pull语法为git pull remote_alias branch_name]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git系列之基础篇--commit]]></title>
    <url>%2F2017%2F05%2F08%2F2017-05-08-git_04%2F</url>
    <content type="text"><![CDATA[《Git系列》文章是博主阅读Git官方book提取整理出的内容，意在深入理解Git的工作机制、原理 简介前一节介绍了add命令—将当前工作目录的修改添加至暂存区，那么存储至暂存区后应当及时交由Git数据库进行版本管理，这时就是此篇文章需要介绍的commit命令大展身手的时候了！ commit提交 commit顾名思义为提交，其作用是将暂存区的内容提交至Git本地数据库，注意，每次提交都仅作用于当前分支 1.语法 git commit [options] ...（更多参数详见：git-commit） 2.提交内容 git进行提交操作时会保存一个提交对象，包含如下内容： 指向暂存内容快照的指针(暂存操作会为每一个文件使用SHA-1 哈希算法计算校验和，生成一个blob对象) 作者的姓名和邮箱 提交时输入的信息以及指向它的父对象的指针（首次提交产生的提交对象没有父对象，普通提交操作产生的提交对象有一个父对象，而由多个分支合并产生的提交对象有多个父对象） 初级使用使用add命令我们已经将当前工作目录所有的修改都已经添加进暂存区了(可用git status确认是否已经添加进暂存区)，之后运行提交命令： 12345678910➜ test git:(master) ✗ git commit -m &quot;first commit&quot;[master (root-commit) 6fffa3a] first commit 1 file changed, 1 insertion(+) create mode 100644 test.txt➜ test git:(master) git logcommit 6fffa3a925f1454195a297c7b5373c273eeaa5aaAuthor: zouziwen &lt;zouziwen@meituan.com&gt;Date: Mon May 8 17:35:28 2017 +0800 first commit 从commit执行后返回的信息可得出本次提交时在master分支，commit-id为6fffa3a，commit-message为first commit，一个文件被插入；并且如上例，可通过git log来查询当前工作目录的提交历史 高级使用与add命令相同，Git也提供了一组高级命令来完成完成commit操作，在前一节add高级命令的基础上向下执行 1234567891011➜ test git:(master) git write-tree // 在当前目录生成树9d223a574a4b40a55bbf02a047c042c6c0b889a9➜ test git:(master) echo &quot;first commit&quot; | git commit-tree 9d223a574a4b40a55bbf02a047c042c6c0b889a9 // 提交tree85f186d00fe88ab272053dfe0d80b1f26dec6464➜ test git:(master) git update-ref refs/heads/master 85f186d00fe88ab272053dfe0d80b1f26dec6464 //更新当前分支的commit➜ test git:(master) git log commit 85f186d00fe88ab272053dfe0d80b1f26dec6464Author: zouziwen &lt;zouziwen@meituan.com&gt;Date: Mon May 8 18:33:45 2017 +0800 first commit Git对象123456➜ test git:(master) git cat-file -t cb19826acbc17d65e6b492abf2b5b10930c184f3blob➜ test git:(master) git cat-file -t 9d223a574a4b40a55bbf02a047c042c6c0b889a9tree➜ test git:(master) git cat-file -t 85f186d00fe88ab272053dfe0d80b1f26dec6464commit 如上命令分别得出Git对象存储的内容，如下图： Git会为每个文件都生成一个数据对象(blob-object)，而为了解决文件名保存的问题，还提供了一个树对象(tree-object)。Git以一种类似于 UNIX 文件系统的方式存储内容，但作了些许简化。 所有内容均以树对象和数据对象的形式存储，其中树对象对应了 UNIX 中的目录项，数据对象则大致上对应了 inodes 或文件内容。 一个树对象包含了一条或多条树对象记录（tree entry），每条记录含有一个指向数据对象或者子树对象的 SHA-1 指针，以及相应的模式、类型、文件名信息。 Git会为每次commit生成一个commit对象，每个commit对象对应一个tree，而此tree对象相当于当前目录根目录； git loggit log命令输出了当前工作分支的提交历史，默认不用任何参数的话，git log会按提交时间列出所有的更新，最近的更新排在最上面。这个命令会列出每个提交的 SHA-1 校验和、作者的名字和电子邮件地址、提交时间以及提交说明。 至于使用在官网文档中介绍的很清楚，这里就不多余举出。传送门：git-log 上一篇：Git系列之基础篇—add 下一篇：Git系列之基础篇—push、pull、fetch]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git系列之基础篇--add]]></title>
    <url>%2F2017%2F05%2F06%2F2017-05-06-git_03%2F</url>
    <content type="text"><![CDATA[《Git系列》文章是博主阅读Git官方book提取整理出的内容，意在深入理解Git的工作机制、原理 简介前两篇分别介绍Git版本控制系统和Git仓库的一些知识，从本章起将正式深入Git的使用； 在第一篇文章《Git系列—初识》中提到了文件状态的几种类型，除了已提交状态之外，Git工作目录下不外乎就两种文件状态：已跟踪或未跟踪。 已跟踪：是指那些被纳入了版本控制的文件，在上一次快照中有它们的记录，在工作一段时间后，它们的状态可能处于未修改，已修改或已放入暂存区。初次克隆某个仓库的时候，工作目录中的所有文件都属于已跟踪文件，并处于未修改状态。 未跟踪：工作目录中除已跟踪文件以外的所有其它文件都属于未跟踪文件，它们既不存在于上次快照的记录中，也没有放入暂存区。 从上图中可以看出无论是Untracked或Modified状态的文件最终都需要转换成Staged状态的文件才能被Git管理，一般称Staged状态的文件为存入暂存区的文件。 status在正式使用add命令之前，在这里先引入status命令，其作用为检查当前文件状态；一般的，Git工作目录下的文件除上述介绍的几种文件状态之外，还有一种特殊的状态，即未更改状态—代表所有已跟踪文件在上次提交后都未被更改过。 语法为：git status addadd命令在Git版本控制中其含义是将所有Untracked和Unstaged的文件放入暂存区，暂存区的文件即可被Git跟踪管理的文件。 初级使用一般的，使用git add命令开始追踪一个文件，其语法为：git add [&lt;options&gt;] [--] &lt;pathspec&gt;...(options可通过git add -h查看，亦可查阅add) 所以如下实例： 12345678910111213141516171819202122➜ test git:(master) ✗ echo &quot;test&quot; &gt; test.txt➜ test git:(master) ✗ git statusOn branch masterInitial commitUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) test.txtnothing added to commit but untracked files present (use &quot;git add&quot; to track)➜ test git:(master) ✗ git add test.txt (亦可为 git add . 为添加当前目录所有未跟踪或未暂存文件)➜ test git:(master) ✗ git statusOn branch masterInitial commitChanges to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: test.txt 上述实例中当创建一个新文件test.txt时，使用git status命令查看，Git会提示Untracked files:(use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)；而当执行add操作后，会提示Changes to be committed:(use &quot;git rm --cached &lt;file&gt;...&quot; to unstage)，可使用git rm --cached命令将该文件移除暂存区；所以读者可勤用git status命令，Git会友好的提示一些命令。 再次修改文件test.txt使用git status查看发现修改已暂存的文件出现modified状态的文件，根据git status提示，若需要应用此次修改则使用git add，若不应用则使用git checkout -- 1234567891011121314151617181920212223242526➜ test git:(master) ✗ echo &quot;test11&quot; &gt; test.txt➜ test git:(master) ✗ git statusOn branch masterInitial commitChanges to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: test.txtChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: test.txt➜ test git:(master) ✗ git add .➜ test git:(master) ✗ git statusOn branch masterInitial commitChanges to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: test.txt 高级使用前文中使用git add命令可将文件放入暂存区进行追踪，那么Git还提供了一套高级命令来完成git add操作； Git是一个内容寻址文件系统，其核心部分是一个简单的键值对数据库（key-value data store）。它会为每一个暂存区的文件使用SHA-1 哈希算法计算校验和，生成一个blob对象。你可以向该数据库插入任意类型的内容，它会返回一个键值，通过该键值可以在任意时刻再次检索（retrieve）该内容。 可以通过底层命令 hash-object 来演示上述效果——该命令可将任意数据保存于 .git 目录，并返回相应的键值。 123456789101112131415161718➜ test git:(master) ✗ lstest.txt➜ test git:(master) ✗ git hash-object -w test.txtcb19826acbc17d65e6b492abf2b5b10930c184f3➜ test git:(master) ✗ git cat-file -p cb19826acbc17d65e6b492abf2b5b10930c184f3test11➜ test git:(master) ✗ git cat-file -t cb19826acbc17d65e6b492abf2b5b10930c184f3blob➜ test git:(master) ✗ git update-index --add --cacheinfo 100644 cb19826acbc17d65e6b492abf2b5b10930c184f3 test.txt➜ test git:(master) ✗ git stOn branch masterInitial commitChanges to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: test.txt git hash-object -w 数据来源(stdin或文件) :输出长度为 40 个字符的校验和(一个将待存储的数据外加一个头部信息（header）一起做 SHA-1 校验运算而得的校验和) git cat-file -p SHA-1 :输出校验值对应的内容，-p是判断其值并显示其内容 , -t为输出该hash的类型，取值有blob,tree,commit git update-index —add —cacheinfo 文件模式 SHA-1 文件名 :必须为上述命令指定 —add 选项，因为此前该文件并不在暂存区中;同样必需的还有 —cacheinfo 选项，因为将要添加的文件位于 Git 数据库中，而不是位于当前目录下。文件模式： 100644：普通文件 100755：可执行文件 120000：符号链接 三种模式即是 Git 文件（即数据对象）的所有合法模式（当然，还有其他一些模式，但用于目录项和子模块）。 此时可查看.git目录下的objects内容（根据SHA-1的hash值存储的文件）： 从上述的实例中我们再次能感受到Git其内部原理是键值对数据库（key-value data store），通过文件的hash值来定位文件的修改。 总结 git add命令是将未暂存的文件存入暂存区，其内部原理是计算出该文件对应的hash值，并存储至objects目录下，以通过其内容来管理变更。 SHA-1 哈希算法Git 中所有数据在存储前都计算校验和，然后以校验和来引用。Git 用以计算校验和的机制叫做 SHA-1 散列（hash，哈希）。 这是一个由 40 个十六进制字符（0-9 和 a-f）组成字符串，基于 Git 中文件的内容或目录结构计算出来。 上一篇：Git系列—Git仓库 下一篇：Git系列之基础篇—commit]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git系列--Git仓库]]></title>
    <url>%2F2017%2F05%2F05%2F2017-05-05-git_02%2F</url>
    <content type="text"><![CDATA[《Git系列》文章是博主阅读Git官方book提取整理出的内容，意在深入理解Git的工作机制、原理 简介 前一篇文章中简单引入了Git仓库的概念—用来保存项目的元数据和对象数据库的地方，是整个Git数据库的集合； Git仓库是整个Git版本控制系统的基础，所有的Git操作都必须基于Git仓库，下面就让我们来认识一下它吧！ 获取Git仓库Git项目仓库的获取方式有以下两种： 本地创建Git仓库 从Git远程服务器克隆仓库 本地仓库本地创建Git仓库的方式可以用一条命令来完成： cd到需要项目目录中 执行git init操作初始化仓库 远程仓库如果你正在进行团队协作项目开发或更换电脑需要重新拉取项目，那么git clone能很好的帮你从远程拉取到远程Git仓库中最新版本的项目代码； 克隆仓库的命令格式是git clone [url]，当然你也可以拉取Git仓库中特定分支的内容，格式是git clone -b [branch] [url] git clone命令默认拉取的是远程Git仓库master分支中的数据，默认配置中会拉取每一个文件的每一个版本 Git仓库目录结构一个新的Git仓库的目录结构如下图所示： HEAD:指示目前被检出的分支 config:包含项目特有的配置选项 description:仅供 GitWeb 程序使用，我们无需关心 hooks:包含客户端或服务端的钩子脚本（hook scripts） info:包含一个全局性排除（global exclude）文件，用以放置那些不希望被记录在 .gitignore 文件中的忽略模式（ignored patterns） objects:存储所有数据内容 refs:存储指向数据（分支）的提交对象的指针 配置与使用 这部分的内容主要介绍了一些Git正式使用前的一些准备工作和简单使用，config和remote两个指令。 config在使用Git之前需要配置姓名和邮箱等信息(不配置该信息可能无法提交至远程)，那么配置的方式有以下两种： git config指令 文件配置 (1)指令方式 12git config --global user.name &quot;John Doe&quot;git config --global user.email johndoe@example.com --global选项的含义代表全局配置，即只要配置一次即可在当前机器上生效，若需要对某个项目单独配置，需在该项目目录下执行config命令，但不必带上--global参数。 config全部可选参数可见官网文档：git-config (2)文件方式 /etc/gitconfig 文件: 包含系统上每一个用户及他们仓库的通用配置。 如果使用带有 —system 选项的 git config 时，它会从此文件读写配置变量。 ~/.gitconfig 或 ~/.config/git/config 文件：只针对当前用户。 可以传递 —global 选项让 Git 读写此文件。 当前使用仓库的 Git 目录中的 config 文件（就是 .git/config）：针对该仓库。 上述三个文件对应的作用域级别不同，顺序为 仓库级别 &gt; 用户级别 &gt; 系统级别 补充：可通过config命令来查看已生效的配置：git config --list remoteGit作为分布式版本控制系统的代表，其主要作用是为了能更好的进行团队协作，故进行远程Git仓库的配置是必不可少的； git remote -v可查看当前Git仓库关联的远程库列表(通过git clone的仓库自动带有远程仓库的信息)，附上博主的一个Git仓库 左侧的origin为远程仓库的别名，右侧为远程Git仓库的地址 remote的一些简单操作： 若是本地创建的Git仓库，则需要手动添加远程Git仓库：git remote add [alias] [url] 重命名：git remote rename [old_alias] [new_alias] 删除远程仓库：git remote rm [alias] 上一篇：Git系列—初识 下一篇：Git系列之基础篇—add]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git系列--初识]]></title>
    <url>%2F2017%2F05%2F04%2F2017-05-04-git_01%2F</url>
    <content type="text"><![CDATA[《Git系列》文章是博主阅读Git官方book提取整理出的内容，意在深入理解Git的工作机制、原理 闲聊博主在读大学初期未曾接触过版本控制系统，在这里分享一下博主最初的手工版本控制 写新功能的代码先保存一份当前工程代码的副本project_01 在当前工程代码的基础上进行开发，开发完毕后突然发现尚可有更好的实现方式，但又想保存此次实现（以便参考），于是保存项目副本project_02 完成第二步后，将project_01重新导入编辑器，重新开发 见下图： 12345 (开发)project_01 ----&gt; project_02 \ \ (重新导入project_01开发) ----&gt; project03 上述手工版本控制的缺点： 不便于比较，需要手动定位至文件目录下 开发项目麻烦，导入和删除步骤繁琐 版本控制 版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。 本地版本控制系统最初版本控制系统也是和博主的方式一样进行手工操作，后来为了解决手工的问题，人们用数据库来记录文件历次更新的差异。如下图（代表的有RCS） 集中化的版本控制系统本地版本控制系统对于个人用户来说是绰绰有余的，但对于企业用户来说却并不适应，于是乎就出现了CVS、Subversion以及Perforce等集中化的版本控制系统（Centralized Version Control Systems，简称 CVCS）, 分布式版本控制系统集中化的版本控制系统存在一个致命的缺陷—单点故障，分布式版本控制系统（Distributed Version Control System，简称 DVCS）的代表有：Git、Mercurial、Bazaar 以及 Darcs 等 差异性 版本控制系统 优点 缺点 本地版本控制系统 持久化，防丢失 不利于团队协作 集中化的版本控制系统 团队协作，权限分明 中央服务器的单点问题 分布式版本控制系统 分布式，持久化，团队协作，clone即备份 项目大的情况下提取慢 GitGit是分布式版本控制系统的一种，其特点在于： 大部分版本控制系统以保存文件变更列表来存储信息，而git则是采用将变更的整个文件进行保存快照方式存储 Git是本地可以完成所有操作，直至版本上线时才需要将提交push至远程 为了保证完整性，git会在文件存储前计算校验和，之后通过校验和引用 文件状态一个Git的版本控制中文件存在四种状态： Untracked(未追踪文件)：尚未添加至版本控制中的文件 staged(已暂存)：添加至暂存区的文件(git add ) modified(已修改)：在现有版本上进行修改的文件 committed(已提交)：提交至本地数据库中的文件（git commit） 工作区域Git存在三个工作区域的概念： Git仓库：用来保存项目的元数据和对象数据库的地方，是整个git数据库的集合 工作目录：对项目的某个版本提取出来的内容，即工作分支 暂存区：也称为索引，保存了下次将要提交的信息 工作流程一个完整的git工作流程如下： 建立或克隆一个Git仓库，并将其或者在其基础上创建工作目录 在工作目录中添加或修改文件 将文件的修改添加至暂存区中 提交更新，将Git暂存区中的文件commit至Git仓库中 push至远程 下一篇：Git系列—Git仓库]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Codis线上加密码]]></title>
    <url>%2F2017%2F03%2F01%2F2017-03-01-codis_hot_set_password%2F</url>
    <content type="text"><![CDATA[本篇博客为codis的集群使用者提供线上加密码的方法，并且详解了其每一步操作，绝对干货！ 线上告警前面一篇博客已经简单介绍了Codis是什么，具体可跳转：Codis初体验心得 前几日，博主部署的Codis集群的一个group中的master的Mem Used显示为/NaN GB，导致Codis集群无法正常提供服务，日志如下： 查看该端口的日志发现， 连接zk时出现request time out提示错误， 并且在top中发现有一个进程占了778.7%CPU 后通过运维了解到，由于redis未设置密码，并且redis存在公网ip，导致黑客通过redis的该漏洞植入了木马程序，导致该端口不能正常提供服务；同样的在网上找到了redis该漏洞的描述：Redis 未授权访问缺陷可轻易导致系统被黑 解决方案： 屏蔽公网ip 给codis集群设置密码 重新搭建集群，并将数据重新导入(适用于集群中未做过migrate操作) 当然这里博主采用的是第二种方案。 Codis加密Notes：Codis加密的过程中由于要重启代理，所以会导致服务无法正常提供服务； 加密步骤： 在config.ini添加password选项 修改每个codis-server的密码 重启proxy和dashboard 线上服务添加密码并重启 例如本次需要为codis添加密码为123456,下面就详细介绍每一步. 修改config.ini在config.ini文件中添加password选项，如下图： 修改codis-server每一个codis-server相当于redis服务，所以改其密码可使用与redis方式一致，可参考：redis配置认证密码 连接每一个codis-server后执行：config set requirepass 123456 当然，最好要将对应的redis.conf文件中的requirepass密码也加上哦，这样能保证配置文件与运行的服务一致； 重启proxy和dashboard在描述重启的操作之前先在这引入下kill这个指令的作用，这里引用下其描述信息，具体可参考:kill命令 kill命令用来删除执行中的程序或工作。kill可将指定的信息送至程序。预设的信息为SIGTERM(15),可将指定程序终止。若仍无法终止该程序，可使用SIGKILL(9)信息尝试强制删除程序。程序或工作的编号可利用ps指令或job指令查看。 通过上面一句话我们可以了解到，kill默认可发送一个信号至程序，预示该程序须终止；那么就意味着当我们调用kill命令关闭codis的程序时，意味着通知zk要移除该codis程序； 所以重启proxy和dashboard如下： 获取到proxy和dashboard的进程id 调用kill 进程id 通知zk移除proxy和dashboard 启动proxy和dashboard； proxy：nohup ../bin/codis-proxy --log-level info -c config.ini -L ./log/proxy.log --cpu=8 --addr=0.0.0.0:19000 --http-addr=0.0.0.0:11000 &amp; dashboard：nohup ../bin/codis-config -c config.ini -L ./log/dashboard.log dashboard --addr=:18087 --http-log=./log/requests.log &amp;&gt;/dev/null &amp; 线上服务添加密码具体添加密码的方式与redis添加密码方式无异，不过博主这里使用的是经过博主完善的codis-spring-java,其基于codis作者提供的Reborn-java添加了如下功能： 适配Spring-data-redis 当zk连接状态变化时，判断是否CONNECTED或RECONNECTED，若是则重新加载pools 添加password选项 执行完上述步骤后即可上线重启服务. 题外话在本次事故中发现Codis集群在master无法正常提供服务时，slave不会自动切换为master，这是一大缺陷. 后面在网上查阅资料发现有第三方服务codis-ha，不过貌似其不支持auth，即当codis集群存在密码时会导致主从连接丢失：dashboard引入codis-ha的功能 参考资料 Redis 未授权访问缺陷可轻易导致系统被黑 redis配置认证密码]]></content>
      <categories>
        <category>Knowledge</category>
      </categories>
      <tags>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO类概述]]></title>
    <url>%2F2017%2F02%2F22%2F2017-02-22-java_nio%2F</url>
    <content type="text"><![CDATA[本篇博客的初衷是记录了博主本次阅读系列博客(Java NIO入门教程详解—链接见文末)全文过程中的一些笔记，描述了Java NIO中核心类以及其方法的作用； Buffer 1.属性 capacity：容量 limit：上界，缓冲区的临界区，即最多可读到哪个位置 position：下标，当前读取到的位置(例如当前读出第5个元素，则读完后，position为6) mark：标记，备忘位置 1大小关系：0 &lt;= mark &lt;= position &lt;= limit &lt;= capacity 2.方法 mark()：记录当前position的位置，使mark=position reset()：恢复到上次备忘的位置，即position=mark clear()：将缓存区置为待填充状态，即position=0;limit=capacity;mark=-1 flip()：将缓冲区的内容切换为待读取状态，即limit=position;position=0;mark=-1 rewind()：恢复缓冲区为待读取状态，即position=0;mark=-1 remaining()：缓冲区剩余元素，即limit-position compact()：丢弃已经读取的数据，保留未读取的数据，并使缓存中处于待填充状态 isDirect()：是否是直接操作内存的Buffer；若是，则此Buffer直接操作JVM堆外内存 ，使用Unsafe实现；否则操作JVM堆内存 slice()：从当前buffer中生成一个该buffer尚未使用部分的新的缓冲区，例如当前buffer的position为3，limit为5，则新的缓冲区limit和capacity都为2，offset的3，数据区域两者共享； Channel 简单的说，Channel即通道，JDK1.4新引入的NIO概念，一种全新的、极好的Java I/O示例，提供与I/O服务的直接连接。Channel用于在字节缓存和位于Channel另一侧的实体(通常是File或Socket)之间有效的传输数据。通道(Channel)是一种途径，借助该途径，可以用最小的总开销来访问操作系统本身的I/O服务。缓冲区(Buffer)则是通道内部用来发送和接受消息的端点。 1.方法 close()：调用close方法会导致工作在该通道上的线程暂时阻塞；close关闭期间，任何其他调用close方法的线程将会阻塞；如果一个线程工作在该通道上被阻塞并且同时被中断，那么该通道将会关闭 isOpen()：通道的开关状态 2.Scatter/Gather Scatter/Gather允许您委托操作系统来完成辛苦活：将读取到的数据分开存放到多个存储桶(bucket)或者将不同的数据区块合并成一个整体。这是一个巨大的成就，因为操作系统已经被高度优化来完成此类工作了。它节省了您来回移动数据的工作，也就避免了缓冲区拷贝和减少了您需要编写、调试的代码数量。 FileChannel FileChannel是线程安全的，只能通过FileInputStream,FileOutputStream,RandomAccessFile的getChannel方法获取FileChannel通道，原理是获取到底层操作系统生成的fd(file descriptor) 1.方法 FileChannel的position属于共享的，属于底层fd的position，当调用RandomAccessFile的seek()方法调整position，则生成的FileChannel对象的position为seek后的值 truncate()：用于设置文件的长度size，若设置的size&lt;当前size，则多出的部分会被删除 force()：强制将全部待定的修改都写入磁盘中的文件上(所有的现代文件系统都会缓存数据和延迟磁盘文件更新以提高性能)【若是force作用于远程文件系统，则不能保证该操作一定能成功，需要验证当前使用的操作系统或文件系统在同步修改方面是否可以依赖】 transferTo()和transferFrom()：允许将一个通道交叉连接到另一个通道传递数据； 文件锁（FileLock） 文件锁的对象是文件而不是通道或线程； 获得独占锁的前提是对文件有写权限，获得共享锁只要对文件有读权限即可 如果一个线程在一个文件上获得了独占锁，若运行在同一JVM上，则另一个线程请求文件的独占锁是允许的；若运行在不同的JVM上，则其线程请求文件的独占锁会被阻塞；因为锁最终是由操作系统或文件系统来判优并且几乎总是在进程级别而不是在线程级别上判优 独占锁和共享锁是由底层操作系统或文件系统决定，若底层不支持共享锁，则即使获取锁时的shared参数为true，调用FileLock的isShared()方法也将返回false FileLock从获取到之后有效，失效条件： 调用FileLock中的release()方法 它所关联的通道被关闭 JVM关闭 123456789101112// 这里是FileChannel关于FileLock的API（抛出IOException，这里未列出）// 获取文件的独占锁（获取操作是阻塞的），等价于调用lock(0,Long.MAX_VALUE , false)public final FileLock lock()//指定文件内部锁定区域的开始position以及锁定区域的size，shared标识待获取是共享锁(true)还是独占锁(false)// 文件锁的范围可以大于文件的大小public abstract FileLock lock (long position, long size, boolean shared)// lock方法的非阻塞方式，若是待获取的区域是已经被锁定的，则此时会直接返回nullpublic final FileLock tryLock()public abstract FileLock tryLock (long position, long size, boolean shared) 内存文件映射 内存文件映射，简单地说就是将文件映射到内存的某个地址上。&gt;1. 普通方式读取文件流程首先内存空间分为内核空间和用户空间，在应用程序读取文件时，底层会发起系统调用，由系统调用将数据先读入到内核空间，然后再将数据拷贝到应用程序的用户空间供应用程序使用。这个过程多了一个从内核空间到用户空间拷贝的过程。&gt;2. 内存文件映射流程文件会被映射到物理内存的某个地址上（不是数据加载到内存），此时应用程序读取文件的地址就是一个内存地址，而这个内存地址会被映射到了前面说到的物理内存的地址上。应用程序发起读之后，如果数据没有加载，系统调用就会负责把数据从文件加载到这块物理地址。应用程序便可以读取到文件的数据。省去了数据从内核空间到用户空间的拷贝过程。所以速度上也会有所提高。 在Java中，具体内存文件映射的方法是通过FileChannel的map()方法来创建一个由磁盘文件支持的虚拟文件映射(virtual memory mapping)并在那块虚拟内存空间外部封装一个MappingByteBuffer对象 通过内存映射机制来访问一个文件效率比其他方式高，因为不需要做明确的系统调用；其直接操作的内存使位于JVM堆外的内存，且虚拟内存可以自动缓存内存页(memory page) 当为一个文件建立虚拟内 存映射之后，文件数据通常不会因此被从磁盘读取到内存(这取决于操作系统)。该过程类似打开一个文件：文件先被定位，然后一个文件句柄会被创建，当您准备好之后就可以通过这个句柄来访问文件数据 123456789// 第二、三个参数分别表示映射区域的开始(position)和映射的总大小(size)// 若map的size&gt;文件的大小，则文件会自动扩容至sizepublic abstract MappedByteBuffer map(MapMode mode,long position, long size)MapMode的三种模式，受FileChannel的访问权限控制1. READ_ONLY：只读权限2. READ_WRITE：写权限3. PRIVATE：写时拷贝(copy-on-write)映射；意味着通过`put()`方法所做的任何修改都会导致产生一个对原数据的私有的数据拷贝，之后将修改应用到拷贝后的数据，此份数据只能被当前MappedByteBuffer对象所见;（**Notes：PRIVATE的拷贝是按内存页拷贝的，若一次put操作只修改了前一个内存页的内容，则后一个内存页的被READ_WRITE的修改也会应用到PRIVATE的映射中**） MappedByteBuffer的几个方法如下： load()：加载整个文件至内存；该操作将会产生大量的页调入(page-in)，具体数量取决于文件中被映射区域的大小；该方法的主要作用是提前加载文件至内存以方便后续的访问速度尽可能的快 isLoaded()：判断一个映射文件是否已经完全加载至内存 force()： 同FileChannel的force方法，强制将缓冲区的修改应用到永久磁盘驱动器 Socket’s Channel新的Socket通道类可以运行非阻塞模式并且是可选择的。新旧两者对应关系如下： ServerSocketChannel和ServerSocket SocketChannel和Socket DatagramChannel和DatagramSocket 每个Socket’s Channel在实例化时都会创建一个对等的Socket对象，调用Channel中的socket()方法可获取到；而同时Socket对象也存在getChannel()方法，当且仅当通过Channel产生的对等Socket对象，调用getChannel()方法才会返回相应的Channel，若是使用传统的方式创建Socket，则调用该方法始终得到的是null 连接实例 123456789101112131415161718192021// 连接地址SocketAddress address = new InetSocketAddress(10011);/******************** 服务端ServerSocketChannel通道创建 *************************/// jdk1.7之前的bind方式ServerSocketChannel serverChannel = ServerSocketChannel.open();serverChannel.socket().bind(address);// jdk1.7之后的bind方式//ServerSocketChannel serverChannel = ServerSocketChannel.bind(address);/******************** 客户端SocketChannel通道创建 *************************/SocketChannel clientChannel = SocketChannel.open();// 通道连接clientChannel.connect(address);// Socket对象连接//clientChannel.socket().connect(address);// 每个Socket's Channel都继承自AbstractSelectableChannel，可控制通道是否阻塞channel.configureBlocking(boolean);// 并且能够获取到修改阻塞方式的锁channel.blockingLock(); 管道(Pipe)java.nio.channels中含有一个名为Pipe(管道)的类，作用是使Java进程内部的两个通道(Channel)之间的数据传输; 核心知识： 调用Pipe.open()方法创建 Pipe.SourceChannel：负责读，调用Pipe.source()获取 Pipe.SlinkChannel：负责写，调用Pipe.slink()获取 Selector(选择器)选择器提供选择执行已经就绪任务的能力，其实现依赖底层操作系统的select()和pool()这两个系统调用，使得Java也能像C或C++提供同时管理多个I/O通道； 核心功能类： Selector：多个Channel的监听者，通过select()方法实时响应就绪好的通道，keys()方法其返回的是SelectionKey对象 SelectorChannel：支持就绪检查的通道类的抽象；其可以被注册到Selector上，一个SelectorChannel可以被注册到多个Selector上，但对每个Selector而言每个SelectorChannel只能注册一次 SelectionKey：代指了通道与选择器之间的注册关系； 具体细节可参考NIO教程中的选择器部分，这里就不做深入描述，之后研究select/pool/epoll的原理时再详细介绍其原理。 参考资料 理解java中的mmap Java NIO入门教程详解]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis内存管理和持久化机制]]></title>
    <url>%2F2017%2F02%2F17%2F2017-02-17-redis_principle%2F</url>
    <content type="text"><![CDATA[本篇博客参考了多位前辈对Redis的深入分析的Blog和Redis实现原理等书籍，简单描述了Redis的内存管理和持久化机制，列出了其内存结构和编码类型，以及Redis持久化的RDB和AOF两种方式的基本原理 内存管理Redis是一个基于内存的key-value的数据库，其内存管理是非常重要的；其针对不同操作系统的差异，同时方便自己实现相关的统计函数，封装了不同平台的实现，具体可参阅深入redis内部—内存管理； Redis支持多种不同的数据类型，如下： String List Set Hash Sorted Set 下面就详细来介绍下其数据结构和编码。 数据结构所有的Redis对象都被封装在RedisObject这个结构体当中，如下： 1234567typedef struct redisObject &#123; unsigned type, // 4字节，数据类型(String,List,Set,Hash,Sorted Set) unsigned encoding, // 4字节，编码方式 unsigned lru, // 24字节 int refcount, // 对象引用计数 void *ptr // 数据具体存储的指向&#125; robj; 数据类型的常用编码方式如下： 数据类型 常用 少量数据 特殊情况 读 写 String RAW EMBSTR INT O(1) O(1) List LinkedList ZipList pop:O(1)lset:O(N) push:O(1)lindex:O(N) Set Hash Table INTSET(少量整数) O(1) O(1) Hash Hash Table ZipList O(1) O(1) Sorted Set SkipList ZipList zscore:O(1)zrank:O(logN) O(logN) 编码方式介绍： RAW：RedisObject的ptr指向名为sds的空间，包含Len和Free头部和buf的实际数据，Free采用了某种预分配（若Len=1M，则Free分配1M空间；SDS的长度为Len+Free+buf+1(额外的1字节用于保存空字符)） EMBSTR：与RedisObject在连续的一块内存空间，省去了多次内存分配；条件是字符串长度&lt;=39 INT：字符串的特殊编码方式，若存储的字符串是整数时，则ptr本身会等于该整数，省去了sds的空间开销；实际上Redis在启动时会默认创建10000个RedisObject，代表0-10000的整数 ZipList(压缩列表)：除了一些标志性字段外用一块类似数组的连续空间来进行存储，缺点是读写时整个压缩列表都需要更改，一般能达到10倍的压缩比。Hash默认值为512，List默认是64 Hash Table：默认初始大小为4，使用链地址法解决hash冲突；rehash策略：将原来表中的数据rehash并放入新表，之后替换；大量rehash可能会造成服务不可用，因此Redis使用渐进式rehash策略，分批进行 过期机制Redis为了不影响正常的读写操作，一般只会在必要或CPU空闲的时候做过期清理的动作； 必要：一次事件循环结束，进入事件侦听前 CPU空闲：系统空闲时做后台定时清理任务（时间限制为25%的CPU时间）；Redis后台清理任务默认100ms执行1次，25%限制是表示25ms用来执行key清理 过期key清理算法： 依次遍历所有db； 从db中随机取得20个key，判断是否过期，若过期，则剔除； 若有5个以上的key的过期，则重复步骤2，否则遍历下一个db 清理过程中若达到了时间限制，则退出清理过程 持久化Redis支持四种持久化方式；如下： 定时快照方式(snapshot)[RDB方式] 基于语句追加文件的方式(aof) 虚拟内存(vm) 已被放弃 Diskstore方式实验阶段 前两种方式为小数据量追加落地方式；后两种为尝试存储数据超过物理内存时，一次性落地方式； 定时快照方式(snapshot)该方式实际是在Redis内部执行一个定时任务，根据redis.conf中配置的save的时间间隔去检查当前数据改变次数和时间是否满足配置，如果满足则从父进程fork(copy-on-write机制)出一个子进程，通过该子进程遍历内存来转换成rdb文件； 12345678910111213141516171819# Save the DB on disk:# 设置sedis进行数据库镜像的频率。# 900秒（15分钟）内至少1个key值改变（则进行数据库保存--持久化）。# 300秒（5分钟）内至少10个key值改变（则进行数据库保存--持久化）。# 60秒（1分钟）内至少10000个key值改变（则进行数据库保存--持久化）。save 900 1save 300 10save 60 10000 stop-writes-on-bgsave-error yes# 在进行镜像备份时,是否进行压缩。yes：压缩，但是需要一些cpu的消耗。no：不压缩，需要更多的磁盘空间。rdbcompression yes# 一个CRC64的校验就被放在了文件末尾，当存储或者加载rbd文件的时候会有一个10%左右的性能下降，为了达到性能的最大化，你可以关掉这个配置项。rdbchecksum yes# 快照的文件名dbfilename dump.rdb# 存放快照的目录dir /var/lib/redis 缺陷：快照只是一段时间的数据的体现，若发生宕机，数据会丢失 基于语句追加文件的方式(aof)类似mysql的binlog方式，每个数据发生改变都会追加至一个log文件中； 12345678# 是否开启AOF，默认关闭（no）appendonly yes# 指定 AOF 文件名appendfilename appendonly.aof# Redis支持三种不同的刷写模式：# appendfsync always #每次收到写命令就立即强制写入磁盘，是最有保证的完全的持久化，但速度也是最慢的，一般不推荐使用。appendfsync everysec #每秒钟强制写入磁盘一次，在性能和持久化方面做了很好的折中，是受推荐的方式。# appendfsync no #完全依赖OS的写入，一般为30秒左右一次，性能最好但是持久化最没有保证，不被推荐。 缺陷：追加log文件可能过大，恢复慢；实时写log会影响redis本身性能 参考资料 Redis内存使用优化与存储 Redis持久化 Snapshot和AOF说明 细解Redis内存管理和优化 Redis 设计与实现]]></content>
      <categories>
        <category>Knowledge</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JMM概述]]></title>
    <url>%2F2017%2F02%2F13%2F2017-02-13-jmm%2F</url>
    <content type="text"><![CDATA[本篇博客旨在突出体现JMM的基本作用，以及其特性；当然面试中也有很多朋友会遇到这个题目，简单了解下还是有助于找工作的。 引言本篇博客旨在突出体现JMM的基本作用，以及其特性；当然面试中也有很多朋友会遇到这个题目，简单了解下还是有助于找工作的。 Java内存模型的抽象 Java线程之间的通信由Java内存模型（本文简称为JMM）控制，JMM决定一个线程对共享变量的写入何时对另一个线程可见。 从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化 重排序为了程序能够更高效的运行，编译器和处理器都会对指令进行重排序；重排序分为以下三种类型： 编译器优化的重排序 指令级并行的重排序 内存系统的重排序 只要是重排序都有可能会导致多线程内出现内存可见性的问题 内存屏障指令为了保证内存可见性，java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM把内存屏障指令分为下列四类： 屏障类型 指令示例 说明 LoadLoad Barriers Load1; LoadLoad; Load2 确保Load1数据的装载，之前于Load2及所有后续装载指令的装载。 StoreStore Barriers Store1; StoreStore; Store2 确保Store1数据对其他处理器可见（刷新到内存），之前于Store2及所有后续存储指令的存储。 LoadStore Barriers Load1; LoadStore; Store2 确保Load1数据装载，之前于Store2及所有后续的存储指令刷新到内存。 StoreLoad Barriers Store1; StoreLoad; Load2 确保Store1数据对其他处理器变得可见（指刷新到内存），之前于Load2及所有后续装载指令的装载。StoreLoad Barriers会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。 happens-before 如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在happens-before关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 happens-before规则如下： 程序顺序规则：一个线程中的每个操作，happens- before 于该线程中的任意后续操作。 监视器锁规则：对一个监视器锁的解锁，happens- before 于随后对这个监视器锁的加锁。 volatile变量规则：对一个volatile域的写，happens- before 于任意后续对这个volatile域的读。 传递性：如果A happens- before B，且B happens- before C，那么A happens- before C。 注意，两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前 顺序一致性模型 JMM对正确同步的多线程程序，其执行将具有顺序一致性(sequentially consistent)—-即程序的执行结果和在顺序一致性模型中得到的结果相同；（这里同步是指广义上的同步，包括同步原语(lock,volatile,final)的正确使用） 特性： 一个线程中所有操作都必须按照程序的顺序来执行 不管程序是否同步，所有线程都只能看到一个单一的操作执行顺序。在此模型下，每个操作都必须原子执行且立即对所有线程可见 内存语义JMM还定义了volatile、锁、final的内存语义，具体细节已经有同仁总结的很好了，所以这里就不多阐述了。 附上链接： volatile 锁 final]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>jmm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pushy初识]]></title>
    <url>%2F2017%2F01%2F17%2F2017-01-17-pushy%2F</url>
    <content type="text"><![CDATA[Pushy是基于HTTP/2的Java类库，是向Apple的APNs发送推送通知的第三方类库。 IntroductionPushy是一个给Apns的发推送通知的Java类库；是Turo创建和维护的项目。 与notnoop使用基于二进制协议不同的是，Pushy使用的是基于HTTP/2-based APNs 协议 具体两种协议的比较可参考：基于HTTP2的全新APNs协议 maven依赖12345678910111213141516171819&lt;!-- pushy --&gt;&lt;dependency&gt; &lt;groupId&gt;com.relayrides&lt;/groupId&gt; &lt;artifactId&gt;pushy&lt;/artifactId&gt; &lt;version&gt;0.8.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-tcnative-boringssl-static&lt;/artifactId&gt; &lt;version&gt;1.1.33.Fork24&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.eclipse.jetty.alpn/alpn-api --&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty.alpn&lt;/groupId&gt; &lt;artifactId&gt;alpn-api&lt;/artifactId&gt; &lt;version&gt;1.1.2.v20150522&lt;/version&gt;&lt;/dependency&gt; Example作为一个程序员，果断上代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import java.io.File;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import com.relayrides.pushy.apns.ApnsClient;import com.relayrides.pushy.apns.ApnsClientBuilder;import com.relayrides.pushy.apns.ClientNotConnectedException;import com.relayrides.pushy.apns.PushNotificationResponse;import com.relayrides.pushy.apns.util.ApnsPayloadBuilder;import com.relayrides.pushy.apns.util.SimpleApnsPushNotification;import io.netty.util.concurrent.Future;import io.netty.util.concurrent.GenericFutureListener;public class PushyExample &#123; public static void main(String[] args) throws Exception &#123; final ApnsClient apnsClient = new ApnsClientBuilder() .setClientCredentials(new File("p12Path"), "111111").build(); final Future&lt;Void&gt; connectFutrue = apnsClient.connect(ApnsClient.DEVELOPMENT_APNS_HOST); // 等待连接apns成功, 良好的编程习惯，需要有最长等待时间 try &#123; connectFutrue.await(10 , TimeUnit.MINUTES); &#125; catch (Exception e) &#123; if(e instanceof InterruptedException) &#123; System.out.println("Failed to connect APNs , timeout"); &#125; e.printStackTrace(); &#125; final ApnsPayloadBuilder payBuilder = new ApnsPayloadBuilder(); payBuilder.setAlertBody("pushy Example"); String payload = payBuilder.buildWithDefaultMaximumLength(); final String token = "******"; SimpleApnsPushNotification notification = new SimpleApnsPushNotification(token, null, payload); Future&lt;PushNotificationResponse&lt;SimpleApnsPushNotification&gt;&gt; responseFuture = apnsClient .sendNotification(notification); responseFuture .addListener(new GenericFutureListener&lt;Future&lt;PushNotificationResponse&lt;SimpleApnsPushNotification&gt;&gt;&gt;() &#123; @Override public void operationComplete(Future&lt;PushNotificationResponse&lt;SimpleApnsPushNotification&gt;&gt; arg0) throws Exception &#123; try &#123; final PushNotificationResponse&lt;SimpleApnsPushNotification&gt; pushNotificationResponse = arg0 .get(); if (pushNotificationResponse.isAccepted()) &#123; System.out.println("Push notification accepted by APNs gateway."); &#125; else &#123; System.out.println("Notification rejected by the APNs gateway: " + pushNotificationResponse.getRejectionReason()); if (pushNotificationResponse.getTokenInvalidationTimestamp() != null) &#123; System.out.println("\t…and the token is invalid as of " + pushNotificationResponse.getTokenInvalidationTimestamp()); &#125; &#125; &#125; catch (final ExecutionException e) &#123; System.err.println("Failed to send push notification."); e.printStackTrace(); if (e.getCause() instanceof ClientNotConnectedException) &#123; System.out.println("Waiting for client to reconnect…"); apnsClient.getReconnectionFuture().await(); System.out.println("Reconnected."); &#125; &#125; &#125; &#125;); // 结束后关闭连接, 该操作会直到所有notification都发送完毕并回复状态后关闭连接 Future&lt;Void&gt; disconnectFuture = apnsClient.disconnect(); try &#123; disconnectFuture.await(1 , TimeUnit.HOURS); &#125; catch (Exception e) &#123; if(e instanceof InterruptedException) &#123; System.out.println("Failed to disconnect APNs , timeout"); &#125; e.printStackTrace(); &#125; &#125;&#125; Error Lists java.lang.NoClassDefFoundError: org/eclipse/jetty/alpn/ALPN$ProviderMaven中添加如下配置： 1234567891011&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-tcnative-boringssl-static&lt;/artifactId&gt; &lt;version&gt;1.1.33.Fork24&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.eclipse.jetty.alpn/alpn-api --&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty.alpn&lt;/groupId&gt; &lt;artifactId&gt;alpn-api&lt;/artifactId&gt; &lt;version&gt;1.1.2.v20150522&lt;/version&gt;&lt;/dependency&gt; error:10000438:SSL routines:OPENSSL_internal:TLSV1_ALERT_INTERNAL_ERROR证书错误，更换证书即可]]></content>
      <categories>
        <category>Knowledge</category>
      </categories>
      <tags>
        <tag>apns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法概括]]></title>
    <url>%2F2017%2F01%2F10%2F2017-01-10-sort_algorithm%2F</url>
    <content type="text"><![CDATA[排序算法作为数据结构与算法的基础知识，在实际开发中有许多的应用；当然还有更重要的一点，排序算法几乎属于面试必问的知识。博主个人对排序算法做了一些概括，如下文。 引言 排序总共分为五类： 插入排序：直接插入排序，希尔排序 选择排序：直接选择排序，堆排序 交换排序：冒泡排序，快速排序 归并排序 基数排序 下文将分别用一段话来概述其算法的实现； 1. 插入排序 插入排序是通过构建有序序列的基础上，对于未排序数据从后向前扫描，并找出相应位置插入的过程 1.1 直接插入排序基本思想：顺序地将待排序的数据元素按其关键字值的大小插入到已排序数据元素子集合的适当位置。子集合的数据元素个数从只有一个数据元素开始逐次增大，当子集合大小最终与集合大小相同时排序完毕。 通俗地来讲，有一数组a，长度为n,初始第一个元素为集合R{a1}，依次为R{a1,a2}、R{a1,a2,a3}、…..、R{a1,a2,a3,….,an}排序;最终R{a1,a2,a3,….,an}排序的结果即排序完毕；(Notes:集合顺序不一定是R{a1,a2,a3}) Java代码详见：InsertionSort.java 12345时间复杂度： 1. 最佳：O(n) 2. 平均：O(n^2) 3. 最坏：O(n^2)空间复杂度：O(1) 1.2 希尔插入排序基本思想：把待排序的数据元素分成若干个小组，对同一小组内的数据元素用直接插入法排序；小组的个数逐次缩小，当完成了所有数据元素都在一个组内的排序后排序过程结束。希尔排序又称作为缩小增量排序。 通俗地来讲，有一数组a，长度为n,并设定一组增量,并将数组a按增量进行分组，例如有一增量为3，则分组为R{a1,a4,a7…..},R{a2,a5,a8…}等;再将分组好的数组进行插入排序。 Java代码详见：ShellSort.java 12345时间复杂度： 1. 最佳：O(n) 2. 平均：O((nlog(n))^2) 3. 最坏：O((nlog(n))^2) 空间复杂度：O(1) 增量的选择： 最后一个增量必须为1 应避免增量序列中的取值为倍数（尤其是相邻的值），否则会发生重复比较 2. 选择排序 选择排序是每一次从待排序的数据元素中选出最小（或最大）的一个元素，存放在序列的起始位置，直到全部待排序的数据元素排完 2.1 直接选择排序基本思想：有一数组R{a0,..,an-1}，按顺序从R{a0,..,an-1},R{a1,…,an-1},…….,R{an-2,an-1}中选取出最小值，该值与每次数组中的第一个元素进行交换,总共需要n-1次，得到一个排好序的有序序列。 Java代码详见：SelectSort.java 12345时间复杂度： 1. 最佳：O(n^2) 2. 平均：O(n^2) 3. 最坏：O(n^2) 空间复杂度：O(1) 2.2 堆排序基本思想：即把最大堆堆顶的最大数取出，之后将剩余的堆重新调整为最大堆，并再次将堆顶的最大数取出，直到堆中最后一个数被取出时结束。一般堆中有如下几种操作： 最大堆调整（Max-Heapify）：将堆的末端子节点作调整使得子节点永远小于父节点 创建最大堆（Build-Max-Heap）：将堆中所有数据重排序，使其成为最大堆 堆排序（Heap-Sort）：移除位在最大堆中的堆顶元素，并递归调用最大堆调整运算 堆排序中几个重要的节点计算公式： 父节点：(i-1)/2，i的父节点下标 子节点(左)：2i + 1，i的左子节点下标 子节点(右)：2(i+1)，i的右子节点下标 最大堆与最小堆定义： 最大堆(大根堆)：堆顶元素(根节点)的值是堆中所有值中最大值 最小堆(小根堆)：堆顶元素(根节点)的值是堆中所有值中最小值 Java代码详见：HeapSort.java 12345时间复杂度： 1. 最佳：O(nlog(n)) 2. 平均：O(nlog(n)) 3. 最坏：O(nlog(n))空间复杂度：O(1) 3. 交换排序 交换排序是将序列中的值进行比较并按大小进行交换位置；特点为：将值大的数据向序列尾部一定，将值小的数据向序列头部移动 3.1 冒泡排序基本思想：对每一对相邻元素作比较大小，并按照大小交换位置。 从a0-an数组中，进行相邻元素比较，较大者则与较小者交换位置，这样一次完整比较后则最后一个元素即为最大值 剩余a0-an-1为未排序的序列；重复1）步骤，不过是从a0-an-1数组中进行元素比较 持续每次对越来越少的数组重复上述的步骤，直到没有任何一对元素需要比较 Java代码详见：BubbleSort.java 12345时间复杂度： 1. 最佳：O(n) 2. 平均：O(n^2) 3. 最坏：O(n^2) 空间复杂度：O(1) 3.2 快速排序基本思想：快速排序是基于分治法处理的，步骤如下： 分解：将数组a[p,r]划分成两个子数组a[p,q-1]和a[q+1,r],使得A[p,q-1] &lt;= A[q] &lt;= A[q+1,r] 解决：递归调用[分解]步骤，分别对两个子数组进行分解 合并：将最终结果进行合并 简单地说，就是将数组a[p,r]按数组中的某个值(基准)划分，小于该值的归为一个数组，大于该值的归为一个数组；之后重复对分解后的数组进行递归操作。 Java代码详见：QuickSort.java 12345时间复杂度： 1. 最佳：O(nlog(n)) 2. 平均：O(nlog(n)) 3. 最坏：O(n^2) 空间复杂度：O(n log(n)) 4. 归并排序基本思想：归并排序也是基于分治法处理的；归并排序分为多路归并和两路归并，可用与外排序和内排序；这里主要讲下内排序的两路归并方式。 分解：将数组a[p,r]分为(r-p)/2个长度为2的数组，并使每个数组都有序 合并：将子数组两两合并，最终合成的数据则已经排好序 Java代码详见：MergeSort.java 12345时间复杂度： 1. 最佳：O(nlog(n)) 2. 平均：O(nlog(n)) 3. 最坏：O(n^2) 空间复杂度：O(n) 5. 基数排序基本思想：将数组序列中的数(必须为正整数)都统一成相同数位，超出位数前面补零；然后从最低位开始，依次按个位、十位、百位….(具体按数的位数决定)的方式进行多次排序，最终则为有序序列； Java代码详见：RadixSort.java 123456时间复杂度： 1. 最佳：O(nk) 2. 平均：O(nk) 3. 最坏：O(nk)空间复杂度：O(n+k)k为无序序列中最大值的数位10^k，例如98-&gt;10^2;故k=2 总结本文主要阐述了这些排序算法的基本思想和时间复杂度等情况，并附上了本人写的代码；写此文的初衷亦是为了巩固自己的排序算法基础，如果表述不当的地方请阅读了本文的朋友多指教。谢谢！ 附上一些排序算法的不错的blog： http://bubkoo.com/tags/algorithm/ http://wiki.jikexueyuan.com/project/data-structure-sorting/]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
      <tags>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[接口响应时间长的调优经验]]></title>
    <url>%2F2017%2F01%2F06%2F2017-01-06-service_architecture%2F</url>
    <content type="text"><![CDATA[随着移动互联网的兴起，海量的终端带来的是数据量的飞速增长，对于存储的需求也随之增长；随之引发的接口响应率等问题也不断暴露….. 业务背景业务之初采用的技术架构： 使用单库单表进行存储app信息 接口层面与库直接交互操作 服务状态： 数据库采用NoSql中的MongoDB； 接口每日请求10亿，超时率在15%左右 数据量激增带来的问题数据量的激增给我们的系统带来了哪些挑战？ 使用单表存储导致单表数据量过大，索引数据过大；存储+索引竟达到200G 接口直接与库操作导致接口响应(请求时间&gt;100ms)缓慢；MongoDB是基于内存的数据库，单库单表特别影响性能 推送时直接走库导致推送过慢 解决方案为了提高接口的性能，并通过一步步的引入缓存、队列等中间件减少数据库的读请求和分离数据库的写请求，且对线程池的调优。 Version 1此版本接入了缓存Redis(Codis集群)，通过Redis的高性能以及多样的数据结构来减少直接查询数据库的操作。 友情链接：Redis文档-中文版 注意事项： 需要考虑缓存穿透和缓存雪崩的情况； 缓存与数据库的数据一致性问题 缓存与数据库一致性保证 如何校验数据库和缓存之间数据的一致性 接入缓存后，数据库的请求减少，接口超时率平均在10%左右，在晚高峰时段还是能达10%左右 尚未解决问题： 接口内直接对库进行写操作 Mongo数据库单表数据量达到亿级，并且存在无效索引 Version 2此版本接入了队列，通过队列将数据库插入和修改操作在队列消费端操作，在接口中隔离了对数据库的操作。 引入队列后，博主以为接口应该不会超时了，虽然数据库的数据量比较大，CRUD比较慢，可接口中已经将写操作清零，读操作减少一大半。查询nginx日志发现超时尽管有所减少，但超时每秒还是有几百，总体每日平均超时为5%左右。 这个问题让我有点想不通，于是博主便想着去看下Java线程是否可以调优，目前系统使用的是Netty4作为容器，监听端口来响应；作了以下事情： 查看GC日志，配合jstat命令查询GC频率，是否出现频繁GC，或者Full GC次数过多等情况 查看线程日志，配合jstack命令分析是否出现线程死锁，Object.wait()情况 通过jdk自带的jmc工具观察堆内存使用情况；也可观察线程占用CPU百分比 通过上面的三步操作，得到结果： GC频率不高，Full GC几乎没有 内存使用正常，尚未超过临界值 通过jstack发现大量的线程处于WAITING状态，可是在jstack中尚未发现是哪个方法是引起WAITING的元凶 于是去网上找资料，如何能定位某个方法的执行时间，找到了jdk自带的jvisualvm工具[Notes：jdk1.7u45后才自带有该工具，否则需要自行安装] 打开jvisualvm-&gt;远程-&gt;添加JMX连接-&gt;抽样器-&gt;CPU-&gt;在CPU样例中点击快照 Notes:快照需要等一两分钟跑了数据后再进行生成，一般生成两三次快照进行观察对比。 博主通过jvisualvm的快照中发现大量的线程在自己实现的业务handler中处理时间过长，在这里友情链接下Netty的实现原理 由于业务handler线程只开启了CPU*2个，导致io线程阻塞，无法接收新的请求，超时率高。于是博主将线程数调高，超时率已经只有1%左右。 1234567下面附上个人对Netty线程的理解：netty是基于boss，worker，handler三者相同配合的nio框架1. boss：负责接收io请求，实际情况下，如果只监听了一个端口，只需要开启一个boss2. worker： 负责处理io请求，一般个数不要超过CPU核数，默认为CPU\*2，超过反而影响性能3. handler：负责处理业务的线程，如果是在高并发环境，可以将线程数调大，但一台机器的线程数最好`不要超过1000`，否则影响性能。将线程数调大可以防止worker堵塞。 Version 3 在版本1和2中分别在业务架构上引入了缓存和队列，并进行了线程调优，缓解了大部分的接口压力，提升了接口响应时间； 此版本将就Mongo的基础进行优化。 关于Mongo的实现可参阅官方文档：MongoDB Manual 上文中提到过数据存储用Mongo，并采用单库单表的形式在存储app数据，并且由于历史原因存在无效索引；所以可采取以下方式： 删除无效索引，减少索引占用的内存 由于Mongo内存管理部分完全交由操作系统内核处理，在执行update或delete操作时容易产生内存碎片，导致运行时间过长容易造成大量内存无法被利用。所以需要定期回收Mongo空间，释放内存(具体方式请自行Google) 将数据库单库单表形式变成多库多表，进行水平拆分， 将设备与app的关系存储在按Hash取模的方式的表中， 将大表拆分，按app分表存储 下面附上MongoDB在使用过程中的一些优化建议：Mongodb 实战优化 总结本篇博客意在总结本次调优过程中的大体思路，具体一些工具的使用细节请大家自行查阅资料。谢谢！]]></content>
      <categories>
        <category>Experience</category>
      </categories>
      <tags>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis scan命令的一次坑]]></title>
    <url>%2F2016%2F12%2F05%2F2016-12-05-redis_scan_notice%2F</url>
    <content type="text"><![CDATA[Redis作为当前服务架构不可或缺的Cache，其支持丰富多样的数据结构，Redis在使用中其实也有很多坑，本次博主遇到的坑或许说是Java程序员会遇到的多一点，下面就听博主详细道来。 线上服务堵塞1234567891011String key = keyOf(appid);int retryCount = 3;int socketRetryCount = 3;Exception ex = null;while(retryCount &gt; 0 &amp;&amp; socketRetryCount &gt; 0) &#123; try &#123; return redisDao.getMap(key); &#125;catch (Exception e) &#123; &#125;&#125; 12月2日被告知服务出现异常，查看日志发现其运行到上述代码getMap方法处后日志就没有内容了。 问题分析12345678910111213141516171819202122232425&quot;pool-13-thread-6&quot; prio=10 tid=0x00007f754800e800 nid=0x71b5 waiting on condition [0x00007f758f0ee000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x0000000779b75f40&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043) at org.apache.commons.pool2.impl.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:583) at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:442) at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363) at redis.clients.util.Pool.getResource(Pool.java:49) at redis.clients.jedis.JedisPool.getResource(JedisPool.java:99) at org.reborndb.reborn.RoundRobinJedisPool.getResource(RoundRobinJedisPool.java:300) at com.le.smartconnect.adapter.spring.RebornConnectionFactory.getConnection(RebornConnectionFactory.java:43) at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:128) at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:91) at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:78) at xxx.run(xxx.java:80) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)at java.lang.Thread.run(Thread.java:745) Locked ownable synchronizers:- &lt;0x000000074f529b08&gt; (a java.util.concurrent.ThreadPoolExecutor$Worker) 从线程日志可以看出服务堵塞在获取redis连接处. 分析： 代码配置中redis最大连接为3000 redis配置中session_max_timeout为0，即永不断开连接 一次修改分析从以上两点分析得出，redis连接被耗尽，于是查找代码得知由于重写spring-data-redis中的hscan方面导致，代码如下： 12345678910111213141516171819202122232425262728293031323334RedisConnection rc = redisTemplate.getConnectionFactory().getConnection();if (rc instanceof JedisConnection) &#123; JedisConnection JedisConnection = (JedisConnection) rc; return new ConvertingCursor&lt;Map.Entry&lt;byte[], byte[]&gt;, Map.Entry&lt;String, String&gt;&gt;( JedisConnection.hScan(rawValue(key), cursor, scanOptions), new Converter&lt;Map.Entry&lt;byte[], byte[]&gt;, Map.Entry&lt;String, String&gt;&gt;() &#123; @Override public Entry&lt;String, String&gt; convert(final Entry&lt;byte[], byte[]&gt; source) &#123; return new Map.Entry&lt;String, String&gt;() &#123; @Override public String getKey() &#123; return hashKeySerializer.deserialize(source.getKey()); &#125; @Override public String getValue() &#123; return hashValueSerializer.deserialize(source.getValue()); &#125; @Override public String setValue(String value) &#123; throw new UnsupportedOperationException( "Values cannot be set when scanning through entries."); &#125; &#125;; &#125; &#125;);&#125; else &#123; return hashOps.scan(key, scanOptions);&#125; 上述代码返回ConvertingCursor后未释放连接，导出连接被占满。 二次修改分析于是修改代码为正常释放连接 12345try &#123; ...&#125;finally &#123; RedisConnectionUtils.releaseConnection(rc, factory);&#125; 代码经过上线，再次跑程序查看线上日志发现报了大量的Connection time out. 于是博主就思考是不是由于重写代码不对，尝试使用spring-data-redis的原生代码，即直接调用hashOps.scan(key, scanOptions)方法，再次上线。 上线后观察日志：发现这次不是报Connection time out,日志中大量报Unknown reply:错误。 分析如下： 由于代码是在多线程环境下运行，有几百个线程去调用hscan操作，spring-data-redis原生的代码执行完一次hscan操作后就会关闭连接并返回一个迭代器Cursor，但是遍历Cursor时在本次count后会再次根据游标重新使用该连接进行查询，可是连接却已经被关闭，这时会使用新的连接是可以正常迭代的，但是一旦复用到其他线程使用的连接则会导致报错Unknown reply. 三次修改分析经过思考后得出结论，redis在执行scan操作时一旦连接被释放，那么scan操作将不会进行下去，则报Connection time out. 查阅官方文档得出结论，redis的scan操作需要full iteration，即最优方式是一个连接将以此scan任务执行完全后释放该连接。 修改代码如下： 123456789101112131415161718192021222324252627282930RedisConnectionFactory factory = redisTemplate.getConnectionFactory();RedisConnection rc = factory.getConnection();if (rc instanceof JedisConnection) &#123; JedisConnection JedisConnection = (JedisConnection) rc; Cursor&lt;Map.Entry&lt;String, String&gt;&gt; cursorResult = new ConvertingCursor&lt;Map.Entry&lt;byte[], byte[]&gt;, Map.Entry&lt;String, String&gt;&gt;( JedisConnection.hScan(rawValue(key), cursor, scanOptions), new Converter&lt;Map.Entry&lt;byte[], byte[]&gt;, Map.Entry&lt;String, String&gt;&gt;() &#123; ... &#125;);return new ScanResult&lt;Map.Entry&lt;String, String&gt;&gt;(cursorResult, factory, rc);&#125;public void releaseConnection() throws IOException&#123; IOException ex = null; if(cursor != null) &#123; try &#123; cursor.close(); &#125; catch (IOException e) &#123; ex = e; &#125; &#125; try &#123; RedisConnectionUtils.releaseConnection(rc, factory); &#125; catch (Exception e) &#123; &#125; if(ex != null) &#123; throw ex; &#125;&#125; 将连接返回给业务代码，并在业务代码执行完毕后将连接释放，问题解决。 总结 连接一旦开启就必须释放，否则造成内存泄漏或服务堵塞不可用 重写代码时需要谨记仔细查阅官方文档给出的方案并实施 多线程下使用redis的scan操作需要使用一个连接遍历完Cursor，而不能复用连接，否则导致报错Unknown reply.]]></content>
      <categories>
        <category>Experience</category>
      </categories>
      <tags>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Codis初体验心得]]></title>
    <url>%2F2016%2F09%2F22%2F2016-09-22-codis_introduction%2F</url>
    <content type="text"><![CDATA[Codis是一个分布式的Redis解决方案，对于上层的应用来说，连接Codis Proxy和连接原生的Redis Server没有明显的区别，上层应用可以像使用单机的Redis一样使用，Codis底层会处理请求的转发，不停机的数据迁移等工作，所有后边的一切事情，对于前面客户端来说是透明的，可以简单的认为后边连接是一个内存无限大的Redis服务。 前言Redis和Memcache是当下最流行的Cache技术，都是可基于内存的Cache，读写效率高。 Redis已经是一个必不可少的部件，丰富的数据结构和超高的性能以及简单的协议，让Redis能够很好的作为数据库的上游缓存层。当然我们的项目中也使用了Redis进行Cache。Redis的优点有许多，这里我就不多做说明，主要列出下列几点Redis不足的地方。 Redis缺点： 耗内存。尽管Redis对一些数据结构采用了压缩算法存储，但占用内存量还是过高。 Redis的单点问题。单点Redis容量大小总受限于内存，在业务对性能要求比较高的情况下，单个Redis显然无法满足我们的需求。 对于单点Redis问题我们自然想到进行分布式扩容，目前市面上有类似： Redis自带的Cluster（官方不推荐使用） Twitter的Twemproxy 豌豆荚工程师开发的Codis（和Codis升级后的RebornDB） 至于选择是Twemproxy还是Codis看各个业务自己的需求，目前博主项目使用的就是Codis。 为什么选择Codis： 业务需要，数据可能需要迁移，机器横向扩容 经过线上测试，Codis的升级版Reborn在pipline操作的性能比Codis慢了几十倍 附上几个链接： Redis常见集群方案、Codis实践及与Twemproxy比较 codis,redis,twemproxy三者对比 安装Codis的安装请参考官方步骤：Build Codis 简要步骤： 安装Go 使用Go获取Codis代码 Go编译Codis代码 部署Codis的部署可参考官网的步骤：Codis部署 博主这里有一个一键执行的脚本示例大家可以参考：Codis sample Notes:脚本中的配置需要改为自己的zookeeper地址和一些Redis服务IP config.ini 作用：基础配置，zookeeper地址以及proxy_id等 coordinator：可选择etcd或者zookeeper，示例中的是zookeeper，这里只介绍zookeeper的配置，至于etcd的配置请参考官方文档 zk地址：host:ip 示例：zk=192.168.0.123:2181 如果有多个zk，则以逗号,分隔 product：产品名称, 这个Codis集群的名字, 可以认为是命名空间, 不同命名空间的Codis没有交集 dashboard_addr：dashboard 服务的地址, CLI 的所有命令都依赖于 dashboard 的 RESTful API, 所以必须启动，一般ip配置为当前主机，port默认配置为18087 proxy_id：代理的id，不同机器的代理id不能相同 start_dashboard.sh 作用：启动dashboard 一般不用修改，但是当config.ini中的dashboard_addr配置的port不是18087时，需要将该shell中的port改为相同的port start_redis.sh 作用：启动Redis Server 示例中启动了四个codis-server(即Redis Server)，开启个数各自行调整，另外Redis的conf使用的是当前目录下redis_conf中的配置，可自行修改 add_group.sh 作用：为Redis Server分组 替换配置中的host和port，为codis-server的host和ip。根据业务需要进行合理的分组，可配置多个组 initslot.sh 作用：初始化slot 这一步非常重要，为了让缓存均匀的分布到每个codis-server中，在初始化的时候就需要将slot进行均匀分配。 最佳分配公式： 1024 / group个数 如有6个group，则可以按如下分配 123456../bin/codis-config -c config.ini slot range-set 0 170 1 online../bin/codis-config -c config.ini slot range-set 171 341 2 online../bin/codis-config -c config.ini slot range-set 342 512 3 online../bin/codis-config -c config.ini slot range-set 513 693 4 online../bin/codis-config -c config.ini slot range-set 694 864 5 online../bin/codis-config -c config.ini slot range-set 865 1023 6 online start_proxy.sh 作用：启用代理 —addr：配置访问codis的ip和port —http-addr：codis的监控ip和port 完成上面的步骤后即可在浏览器下访问：http://localhost:18087/admin 进行查看codis的状态 Dashboard Server_group Slots 常见问题 初始化slot时未全部分配至所有group 这个问题博主在第一次使用的时候遇到过，分配了6个group，但是在初始化slot的时候配置是： 12../bin/codis-config -c config.ini slot range-set 0 511 1 online../bin/codis-config -c config.ini slot range-set 512 1023 2 online 只分配了两个group，这样导致key只能存储在前两个group中，另外4个group都不会存储了。 解决方案： (1)执行Auto Rebalance 在Dashboard界面中找到Migrate Status，点击Auto Rebalance指令，当key的数量特别多且占用内存很大时迁移时间需要很久。 (2)重新分配slot 具体执行方式请google解决，我尚未尝试过 For Java Users Codis的作者开发了新一代的升级版Codis—Reborn，并为Java开发者提供了相应的jodis和Reborn-java 这里博主向大家推荐一个在Reborn-java的基础上对Spring-data-redis的支持的codis-spring-java 通过简单的配置即可与Spring-data-redis进行兼容 配置zk地址即可自行管理Redis连接池]]></content>
      <categories>
        <category>Knowledge</category>
      </categories>
      <tags>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字节数组的妙用]]></title>
    <url>%2F2016%2F07%2F26%2F2016-07-26-byte_cache%2F</url>
    <content type="text"><![CDATA[在计算机高级语言中，字节属于最小单位，例如在Java中，int占用4个字节，long占用8个字节等。基本上所有基本类型(包括String)都可以转换成字节，那么这到底有何作用，本篇博客主要是记录了我使用字节数组的经验，希望可以给大家提供一些思路。 缓存对象缓存类型大小分析在实际开发中，经常会用到本地缓存，或使用Redis或者Memcached来作分布式缓存,Java一般存入缓存中的对象无非是以下几种: 序列化的Java对象：一个Java对象序列化后所占用的字节是按对象中属性个数，方法个数，以及属性的值决定，最小也需要几百个字节来存储，大的话可能需要几万个字节 String(可能是json串)：占用字节由字符串的长度决定 规则的byte[]数组：占用字节由数组长度决定，相比较于String来说，基本类型转换成固定字节的数组，而不是转换成内容长度的String，故字节数组所占用的字节比String更少 上述的占用字节数是不考虑Redis或Memcached内部会做出的压缩操作。 从上面三种对象分别占用的字节数来分析得出结论： 在大量的缓存数据(亿级以上)的情况下，为了提高空间利用率，切勿将Java对象当做缓存的内容 字节数组所需空间最少 字节数组的使用实例 下述场景属于个人虚构的 场景：在电影票系统数据库中，电影院id为int类型，电影id为int类型，上映时间为long类型，电影名称为String类型，现有10亿数据，需要将这些数据存入Redis中。 分析：10亿数组采用Java对象作为存储数据显然是不可取，而上映时间为long类型，可能出现数字非常大的id，所以准备采用字节数组形式存储。 实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 缓存结构为：采用hash结构，电影院id为key，电影id为hashKey，value为字节数组，内容为：上映时间+电影名称，存储为大小为8+String.getBytes().length/*** 代码只贴出了生成字节数组的部分，至于字节数组转换成值的部分大家可以自行实现*/// 以下代码主要获取value，即字节数组byte[] getBytes(long startTime , String name) &#123; byte[] strBytes = name.getBytes(); int strLength = strBytes.length; byte[] cache = new byte[8 + strLength]; longToByteArray(startTime , cache , 0); int index = 8; for(int i = 0 ; i &lt; strLength ; i++) &#123; cache[index + i] = strBytes[i]; &#125;&#125;// 将long类型的数转换成字节数组void longToByteArray(long value, byte[] byteArray, int start) &#123; byteArray[start] = (byte) (value &gt;&gt;&gt; 56);// 取最高8位放到0下标 byteArray[start + 1] = (byte) (value &gt;&gt;&gt; 48);// 取最高8位放到0下标 byteArray[start + 2] = (byte) (value &gt;&gt;&gt; 40);// 取最高8位放到0下标 byteArray[start + 3] = (byte) (value &gt;&gt;&gt; 32);// 取最高8位放到0下标 byteArray[start + 4] = (byte) (value &gt;&gt;&gt; 24);// 取最高8位放到0下标 byteArray[start + 5] = (byte) (value &gt;&gt;&gt; 16);// 取次高8为放到1下标 byteArray[start + 6] = (byte) (value &gt;&gt;&gt; 8); // 取次低8位放到2下标 byteArray[start + 7] = (byte) (value); // 取最低8位放到3下标&#125;// 将字节数组转换成long类型long byteArrayToLong(byte[] byteArray, int start) &#123; byte[] a = new byte[8]; int i = a.length - 1, j = start + 7; for (; i &gt;= 0; i--, j--) &#123;// 从b的尾部(即int值的低位)开始copy数据 if (j &gt;= start) a[i] = byteArray[j]; else a[i] = 0;// 如果b.length不足4,则将高位补0 &#125; // 注意此处和byte数组转换成int的区别在于，下面的转换中要将先将数组中的元素转换成long型再做移位操作， // 若直接做位移操作将得不到正确结果，因为Java默认操作数字时，若不加声明会将数字作为int型来对待，此处必须注意。 long v0 = (long) (a[0] &amp; 0xff) &lt;&lt; 56;// &amp;0xff将byte值无差异转成int,避免Java自动类型提升后,会保留高位的符号位 long v1 = (long) (a[1] &amp; 0xff) &lt;&lt; 48; long v2 = (long) (a[2] &amp; 0xff) &lt;&lt; 40; long v3 = (long) (a[3] &amp; 0xff) &lt;&lt; 32; long v4 = (long) (a[4] &amp; 0xff) &lt;&lt; 24; long v5 = (long) (a[5] &amp; 0xff) &lt;&lt; 16; long v6 = (long) (a[6] &amp; 0xff) &lt;&lt; 8; long v7 = (long) (a[7] &amp; 0xff); return v0 + v1 + v2 + v3 + v4 + v5 + v6 + v7;&#125; 当使用本地缓存需要提高内存的利用率，也是可以使用字节数组的。 推荐一篇博客：java Byte和各数据类型(short,int,long,float,double)之间的转换 字节数组的作用 提高空间利用率 压缩内容，在网络传输时，能有效压缩传输数据的大小，从而提高效率 以上内容纯属个人见解，有不妥的地方请各位大神指正，谢谢！]]></content>
      <categories>
        <category>Knowledge</category>
      </categories>
      <tags>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git协同开发的那点事]]></title>
    <url>%2F2016%2F07%2F19%2F2016-07-19-develop_by_git%2F</url>
    <content type="text"><![CDATA[随着分布式和开源这些概念的不断普及，现在有大部分的开发者都在用Git管理开源项目、个人项目和公司的项目开发。本篇博客主要是研究实际在使用Git进行多人协作开发过程中的一些场景和解决方案。 引言 Git是一个开源的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。 网上有许多介绍Git的博客等，下面就附上链接： Git(百度百科) 为什么用Git Git和SVN的区别 协同开发场景分析场景一：开发分支错误项目来了个需求1，从develop分支中拉分支A进行开发；开发的中途，又来了一个紧急需求2，于是我就直接在A分支中开发需求2；直到发现不能在A分支中开发需求2的时候需求2的代码已经commit了几次了。 如下图提交： 1m1-m2-m3-m4-m5 m1、m2、m5是需求1的代码，m3和m4是需求2的代码 解决方案： 假设m3和m4是commit id为0bda20e和1a04d5f 利用cherry-pick和revert: 从develop分支创建新分支B：git checkout B 将A分支中的需求2的commit复制到分支B：git cherry-pick 0bda20e 1a04d5f 撤销在A分支中开发的需求2：git checkout A 和 git revert 0bda20e 1a04d5f 附上cherry-pick和revert的一些博客： git cherry-pick git引发的血案（cherry-pick找回丢失的commit) git revert 用法 场景二：分支进度不一致123m1-m2-m3-m4-m5-m6(master) \ f1-f2-f3-f4-f5(feature) 多人协作开发时，你需要开发一个新功能，于是你从master上的commit3开始拉了一个分支feature，可是这个新功能开发周期很长，等到你完成的时候，master已经提交到m6了。如上图所示。 解决方案： 使用rebase： 切换到分支feature：git checkout feature rebase到master：git rebase master 有冲突解决后执行：git rebase --continue 分支树则变成(如下图所示) 123m1-m2-m3-m4-m5-m6(master) \ f1-f2-f3-f4-f5(feature) 附上rebase的一些博客： git rebase简介(基本篇) git rebase让时光倒流 cherry-pick, merge, rebase git-rebase(认真看，分析很到位) 暂时先发这么多，到时再补上。有错误的地方麻烦各位指正。谢谢！]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人git项目推荐]]></title>
    <url>%2F2016%2F07%2F18%2F2016-07-18-myself_project%2F</url>
    <content type="text"><![CDATA[学习了Java也有好几年了，在这过程中也开发了几个类似于框架或工具包的项目，在这里就王婆卖瓜一下，觉得还行的话不妨可以Download下来看看。 Autumn FrameworkAutumn是基于Servlet的Action Mapping映射、依赖注入和数据库的ORM关系映射实现的JavaWeb框架。 本次设计主要采用了单例模式，工厂模式，适配器模式、模板方法模式、代理模式的组合设计来完成的。 采用了Struts的action的基本原理，采用通过配置action（action.xml）的方式来进行前台与控制层的交互。对HttpServletRequest对象进行重写，用于代理request对象的方法进行表单收集操作。并且集成了FileUpload组件的文件上传，Action可通过配置注解自动对表单进行收集成对象！ 采用了Spring的IOC的思想，通过配置beans.xml文件对dao层和service层对象进行注入，并且内部细节隐藏，可直接通过工厂获取对应的对象！ github地址：https://github.com/mastery001/Autumn reactor-http该项目基于Apache http组件之上进行进一步包装使用，将http请求事件进行垂直切割，以完成http调用监控、统计、降级等作用。 0.x版本：适配的是apache http3.1的版本 1.x版本：适配的是apache http4.x的版本 github地址：https://github.com/mastery001/reactor-http codis-spring-java该项目是基于codis作者提供的Reborn-java添加了如下功能： 适配Spring-data-redis 当zk连接状态变化时，判断是否CONNECTED或RECONNECTED，若是则重新加载pools 添加password选项 github地址：https://github.com/mastery001/codis-spring-java Dolphin-Voice-Language该项目基于openjdk中编译代码(参考了大量的编译代码)逐步实现的解释型语言dv，有助于理解编译原理(词法分析，语法分析) github地址：https://github.com/mastery001/Dolphin-Voice-Language]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>framework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven多module下的Spring启动]]></title>
    <url>%2F2016%2F07%2F17%2F2016-07-17-multi_spring_module%2F</url>
    <content type="text"><![CDATA[本篇blog属于个人工作时的经验之谈，在使用Maven进行项目构建Spring项目中遇到的启动问题，纯属个人见解。 项目结构在实际开发中，往往是用Maven来进行构建项目，一个项目一般都由多个Maven module组成，例如： xxx-base(项目启动module，以下为当前module依赖的module) xxx-common xxx-service(spring配置文件，属性properties文件) … xxx-service的spring配置文件： 123456789&lt;context:component-scan base-package="xxx.xxx.xxx" /&gt;&lt;bean class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt; &lt;property name="locations"&gt; &lt;list&gt; &lt;value&gt;classpath*:properties/xxx.properties&lt;/value&gt; &lt;/list&gt;list&gt; &lt;/property&gt;&lt;/bean&gt; xxx-service的properties文件： 1234567redis.host=$&#123;redis.host&#125;redis.port=$&#123;redis.port&#125;redis.password=$&#123;redis.password&#125;redis.timeout=100000redis.pool.maxIdle=600redis.pool.maxTotal=1500 项目启动xxx-base的spring配置文件： 报错方式配置 1234567891011&lt;context:component-scan base-package="xxx.xxx" /&gt;&lt;bean class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt; &lt;property name="locations"&gt; &lt;list&gt; &lt;value&gt;classpath:properties/xxx.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt;&lt;import resource="classpath:xxx-applicationContext.xml"/&gt; 异常如下图： 解决方案 在存在PropertyPPlaceholderConfigure Bean的每个配置文件中都加上name12345678910```xml&lt;bean class=&quot;org.springframework.beans.factory.config.PropertyPlaceholderConfigurer&quot;&gt; &lt;property name=&quot;ignoreUnresolvablePlaceholders&quot; value=&quot;true&quot; /&gt; &lt;property name=&quot;locations&quot;&gt; &lt;list&gt; &lt;value&gt;classpath:properties/xxx.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 自动扫描spring配置文件 当一个工程引用的module过多时，那么需要import的spring配置文件就特别多，这时可以将每个module的配置文件都使用相同的命名，例如applicationContext.xml， 在项目启动时通过如下代码启动： 1ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext("classpath*:applicationContext.xml"); 这样就可以去除掉一大串的import标签了。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>maven spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉查找树的Java实现]]></title>
    <url>%2F2016%2F05%2F27%2F2016-05-27-binary_search_tree%2F</url>
    <content type="text"><![CDATA[最近在闲看博客时看到一篇专门写红黑树的实现原理，以Java的TreeMap为例讲解，写的很不错，仔细看下来发现很多地方不是很理解，毕竟没有对树的理解并没有很深，所以决定一步一步的将与树相关的扩展实现都了解一遍，沿着下面的学习路线开始，大家也可以参考以下。 树的基本知识 二叉树的知识 二叉查找树 平衡二叉树 红黑树 B树，B-树，B+树 附上上面的将红黑树的blog：史上最清晰的红黑树讲解 树的基本概念 树Tree是n（n≥0）个结点的有限集。在任意一棵非空树中：（1）有且仅有一个特定的被称为根root的结点；（2）当n&gt;1时，其余结点可分为m（m&gt;0）个互不相交的有限集T1，T2，…，Tm，其中每一个集合本身又是一棵树，并且称为根的子树SubTree。 结点拥有的子树数称为结点的度（Degree）。度为0的结点称为叶子（Leaf）或终端结点。度不为0的结点称为非终端结点或分支结点。 树的度是树内各结点的度的最大值。 结点的子树的根称为该结点的孩子（Child），相应地，该结点称为孩子的双亲（Parent）。 结点的层次（Level）是从根结点开始计算起，根为第一层，根的孩子为第二层，依次类推。树中结点的最大层次称为树的深度（Depth）或高度。 如果将树中结点的各子树看成从左至右是有次序的（即不能互换），则称该树为有序树，否则称为无序树。 二叉树的基本概念 二叉树（Binary Tree）的特点是每个结点至多具有两棵子树（即在二叉树中不存在度大于2的结点），并且子树之间有左右之分。 二叉树的性质： 在二叉树的第i层上至多有2i-1个结点（i≥1）。 深度为k的二叉树至多有2k-1个结点（k≥1）。 对任何一棵二叉树，如果其终端结点数为n0，度为2的结点数为n2，则n0=n2+1。 具有n个结点的完全二叉树的深度为不大于log2n的最大整数加1。 如果对一棵有n个结点的完全二叉树的结点按层序编号（从第1层到最后一层，每层从左到右），则对任一结点i（1≤i≤n）,有 a、如果i=1,则结点i是二叉树的根，无双亲；如果i&gt;1，则其双亲是结点x（其中x是不大于i/2的最大整数）。 b、如果2i&gt;n，则结点i无左孩子（结点i为叶子结点）；否则其左孩子是结点2i。 c、如果2i+1&gt;n，则结点i无右孩子；否则其右孩子是结点2i+1。 二叉查找树 二叉查找树（BinarySearch Tree，也叫二叉搜索树，或称二叉排序树Binary Sort Tree）或者是一棵空树，或者是具有下列性质的二叉树： 若它的左子树不为空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不为空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉查找树。 结点定义：12345678910111213141516171819202122232425262728public class TreeNode&lt;T&gt; &#123; /** * 结点的值 */ private T value; /** * 左结点 */ private TreeNode&lt;T&gt; left; /** * 右结点 */ private TreeNode&lt;T&gt; right; /** * 父亲结点 */ private TreeNode&lt;T&gt; parent; /** * 频率 */ private int freq;&#125; 插入根据二叉查找树的性质，插入一个节点的时候，如果根节点为空，就此节点作为根节点，如果根节点不为空，就要先和根节点比较，如果比根节点的值小，就插入到根节点的左子树中，如果比根节点的值大就插入到根节点的右子树中，如此递归下去，找到插入的位置。重复节点的插入用值域中的freq标记。如图2是一个插入的过程。 二叉查找树的时间复杂度要看这棵树的形态，如果比较接近一一棵完全二叉树，那么时间复杂度在O(logn)左右，如果遇到如图3这样的二叉树的话，那么时间复杂度就会恢复到线性的O(n)了。 平衡二叉树会很好的解决如图3这种情况。 核心代码如下：123456789101112131415161718192021private boolean insert(SearchNode&lt;T&gt; curr, SearchNode&lt;T&gt; insertNode, SearchNode&lt;T&gt; parent, boolean currIsLeft) &#123; if (curr == null) &#123; curr = insertNode; if (currIsLeft) &#123; parent.setLeft(curr); &#125; else &#123; parent.setRight(curr); &#125; &#125; else &#123; int result = curr.getValue().compareTo(insertNode.getValue()); // 如果当前值大于插入的值 if (result &gt; 0) &#123; return insert((SearchNode&lt;T&gt;)curr.getLeft(), insertNode, curr, true); &#125; else if (result &lt; 0) &#123; return insert((SearchNode&lt;T&gt;)curr.getRight(), insertNode, curr, false); &#125;else &#123; curr.freq++; &#125; &#125; return true;&#125; 查找在二叉查找树中查找x的过程如下： 若二叉树是空树，则查找失败。 若x等于根结点的数据，则查找成功，否则。 若x小于根结点的数据，则递归查找其左子树，否则。 递归查找其右子树。 核心代码如下：123456789101112protected TreeNode&lt;T&gt; find0(TreeNode&lt;T&gt; node, T value) &#123; if (node == null) &#123; return null; &#125; int result = node.getValue().compareTo(value); if (result &gt; 0) &#123; return find0(node.getLeft(), value); &#125; else if (result &lt; 0) &#123; return find0(node.getRight(), value); &#125; return node;&#125; 删除删除会麻烦一点，如果是叶子节点的话，直接删除就可以了。如果只有一个孩子的话，就让它的父亲指向它的儿子，然后删除这个节点。图4显示了一棵初始树和4节点被删除后的结果。先用一个临时指针指向4节点，再让4节点的地址指向它的孩子，这个时候2节点的右儿子就变成了3节点，最后删除临时节点指向的空间，也就是4节点。 删除有两个儿子的节点会比较复杂一些。一般的删除策略是用其右子树最小的数据代替该节点的数据并递归的删除掉右子树中最小数据的节点。因为右子树中数据最小的节点肯定没有左儿子，所以删除的时候容易一些。图5显示了一棵初始树和2节点被删除后的结果。首先在2节点的右子树中找到最小的节点3，然后把3的数据赋值给2节点，这个时候2节点的数据变为3，然后的工作就是删除右子树中的3节点了，采用递归删除。 我们发现对2节点右子树的查找进行了两遍，第一遍找到最小节点并赋值，第二遍删除这个最小的节点，这样的效率并不是很高。你能不能写出只查找一次就可以实现赋值和删除两个功能的函数呢？ 如果删除的次数不是很多的话，有一种删除的方法会比较快一点，名字叫懒惰删除法：当一个元素要被删除时，它仍留在树中，只是多了一个删除的标记。这种方法的优点是删除那一步的时间开销就可以避免了，如果重新插入删除的节点的话，插入时也避免了分配空间的时间开销。缺点是树的深度会增加，查找的时间复杂度会增加，插入的时间可能会增加。 核心代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071protected void deleteNode(TreeNode&lt;T&gt; deleteNodeParent, TreeNode&lt;T&gt; deleteNode) &#123; if (deleteNodeParent == null) &#123; // 左右子树都为空 if (deleteNode.getLeft() == null &amp;&amp; deleteNode.getRight() == null) &#123; root = null; &#125; else if (deleteNode.getLeft() == null || deleteNode.getRight() == null) &#123; // 存在左子树或右子树 if (deleteNode.getLeft() != null) &#123; root = deleteNode.getLeft(); &#125; else &#123; root = deleteNode.getRight(); &#125; &#125; else &#123; // 左右子树都不为空 TreeNode&lt;T&gt; temp = deleteNode; TreeNode&lt;T&gt; rightLeft = deleteNode.getRight(); while (rightLeft.getLeft() != null) &#123; temp = rightLeft; rightLeft = rightLeft.getLeft(); &#125; if(temp == deleteNode) &#123; deleteNode.setRight(rightLeft.getRight()); &#125;else &#123; temp.setLeft(rightLeft.getRight()); &#125; deleteNode.setValue(rightLeft.getValue()); &#125; &#125; else &#123; // 左右子树都为空 if (deleteNode.getLeft() == null &amp;&amp; deleteNode.getRight() == null) &#123; // 根结点 if (deleteNodeParent.getLeft() == deleteNode) &#123; deleteNodeParent.setLeft(null); &#125; else &#123; deleteNodeParent.setRight(null); &#125; &#125; else if (deleteNode.getLeft() == null || deleteNode.getRight() == null) &#123; // 存在左子树或右子树 if (deleteNode.getLeft() != null) &#123; if (deleteNodeParent.getLeft() == deleteNode) &#123; // 如果待删除结点是左结点，且其存在左结点 deleteNodeParent.setLeft(deleteNode.getLeft()); &#125; else &#123; // 如果待删除结点是左结点，且其存在右结点 deleteNodeParent.setRight(deleteNode.getLeft()); &#125; &#125; else &#123; if (deleteNodeParent.getRight() == deleteNode) &#123; deleteNodeParent.setRight(deleteNode.getRight()); &#125; else &#123; deleteNodeParent.setLeft(deleteNode.getRight()); &#125; &#125; &#125; else &#123; // 左右子树都不为空 TreeNode&lt;T&gt; temp = deleteNode; TreeNode&lt;T&gt; rightLeft = deleteNode.getRight(); while (rightLeft.getLeft() != null) &#123; temp = rightLeft; rightLeft = rightLeft.getLeft(); &#125; if(temp == deleteNode) &#123; deleteNode.setRight(rightLeft.getRight()); &#125;else &#123; temp.setLeft(rightLeft.getRight()); &#125; deleteNode.setValue(rightLeft.getValue()); &#125; &#125;&#125; 具体实现代码请查看：https://github.com/mastery001/study/blob/master/study-datastruct/src/main/java/tree/search/BinarySearchTree.java 参考资料： 二叉查找树—查找、删除、插入（Java实现） 一步一步写二叉查找树]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
      <tags>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记面试题目（1）]]></title>
    <url>%2F2016%2F04%2F22%2F2016-04-22-interview_problem_1%2F</url>
    <content type="text"><![CDATA[一个晚上，一个突然的面试电话呼来，让我措手不及，面试的过程中也发挥的不好，犯了很多不应该犯的错误，一些能答出来的题目却没有答出来。其实真正让我意识到了，工作这段时间自己将很多基础知识都荒废了，而且新知识也并没有接触到多少。 下面就简单将面试的题目总结并解答。 Java反射的原理 Reflection(反射)机制是jdk1.5之后新增的功能，简单来说，反射机制就是在程序运行时能够获取自身的信息；仅知道一个类的全限定类名即可获取该类中的方法，构造器，字段，类型，修饰符等所以信息。 在Java中，创建对象的方式有以下几种 使用new关键字 使用Class.forName(className).newInstance() 使用cglib动态生成类 无论创建对象的方式是怎样的，其原理都是通过类加载器将该类对应的class文件加载到JVM内存区域(类加载机制)；并在内存中生成一个代表这个类的java.lang.Class对象，做为这个类的各种数据的访问入口(对于Hotspot虚拟机来讲，Class对象存放在方法区)。 HashMap原理 HashMap是Java语言中一种基于散列表(数组+链表)实现的数据结构，其典型的运用了空间换时间的策略，采用hash算法来定位数组下标方式使得查询的时间复杂度最优情况下为O(1). HashMap需要注意的几点 初始化的容量大小一般为2的n次幂，如果不是，则内部会自动取值为改值为2的n次幂的接近值，例如，当值为15时，自动取值为16. HashMap是线程不安全的 合理的设置loadFactor参数，能够尽可能的减少hash冲突,可参考hash冲突解决方法 参考资料： HashMap实现原理分析 深入理解HashMap(及hash函数的真正巧妙之处) HashMap原理 线程安全的HashMap实现 HashMap在Java官方文档中是被列为线程不安全的，那么如何自己实现线程安全的HashMap呢？ 有以下几种方式： 使用synchronized或java.util.concurrent.locks.Lock对象给get和put操作加锁 使用java.util.concurrent.locks.ReadWriteLock的读写锁，使读写锁分离提高效率 附上方法2的代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import java.util.HashMap;import java.util.Map;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReadWriteLock;import java.util.concurrent.locks.ReentrantReadWriteLock;/** * 线程安全的HashMap * * @author mastery * * 2016年4月22日 下午3:49:56 */public class SafeThreadHashMap&lt;K, V&gt; extends HashMap&lt;K, V&gt; implements Map&lt;K, V&gt; &#123; /** * 2016年4月22日 下午3:52:04 */ private static final long serialVersionUID = 4530811996637218105L; private final ReadWriteLock lock = new ReentrantReadWriteLock(); private final Lock readLock = lock.readLock(); private final Lock writeLock = lock.writeLock(); public SafeThreadHashMap() &#123; super(); &#125; public SafeThreadHashMap(int initialCapacity) &#123; super(initialCapacity); &#125; @Override public V get(Object key) &#123; readLock.lock(); try &#123; return super.get(key); &#125; finally &#123; readLock.unlock(); &#125; &#125; @Override public V put(K key, V value) &#123; writeLock.lock(); try &#123; return super.put(key, value); &#125; finally &#123; writeLock.unlock(); &#125; &#125;&#125; 字符逆序问题 abc,def转换成def,abc，要求空间只有O(1) 这个是一个典型的字符串逆转的题目，面试的时候想到用栈来实现只允许O(1)空间的要求，但是会用到一份临时空间（不确定栈到底需不需要占一份空间）；下面附上解题思路： 将被逗号分隔的每个单词先倒序，变成cba,fed 再将倒序后的字符串整体倒序，变成def,abc 后来仔细想了下，还是可以不用临时空间的，可以复用字符数组。 代码如下：12345678910111213141516171819202122232425262728293031323334353637import java.util.Stack;public class StringInverse &#123; private static Stack&lt;Character&gt; stack = new Stack&lt;Character&gt;(); public static String inverse(String str) &#123; char[] strArray = str.toCharArray(); int index =0; for(char s : strArray) &#123; if(s == ',') &#123; while(!stack.empty()) &#123; strArray[index++] = stack.pop(); &#125; strArray[index++] = s; &#125;else &#123; stack.push(s); &#125; &#125; while(!stack.empty()) &#123; strArray[index++] = stack.pop(); &#125; for(int i = 0 ; i &lt; index ; i ++) &#123; stack.push(strArray[i]); &#125; index = 0; while(!stack.empty()) &#123; strArray[index++] = stack.pop(); &#125; return new String(strArray); &#125; public static void main(String[] args) &#123; String str = inverse("abc,efg,hij"); System.out.println(str); &#125;&#125; 以上的步骤属于个人见解。 volatile的作用，缺陷 可见性：对一个volatile变量的读，总是能看到(任意线程)对这个volatile变量的写入。意思为工作线程总能第一时间得到主线程的该变量的值。 原子性：对任意单个volatile变量的读/写操作具有原子性，但类似于自加这种复合操作不具有原子性。 有关volatile的解释下面有几篇个人觉得比较好的解释： 深入理解Java内存模型（四）——volatile java中volatile关键字的含义 关于volatile关键字 volatile的缺陷： volatile的原子性和可见性只是作用于被定义为volatile的变量和简单的对volatile变量的原子操作（例如简单的set和get操作），当对该变量进行运算时则可能会无法保证其特性。 MySQL索引的实现附上几篇认为写的蛮好的mysql索引原理的文章： MySQL索引背后的数据结构及算法原理 —详细的介绍了mysql索引的实现，并解释为何使用B树结构 mysql索引结构原理、性能分析与优化 Mysql索引的设计、使用和优化]]></content>
      <categories>
        <category>Experience</category>
      </categories>
      <tags>
        <tag>collections thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java GC机制]]></title>
    <url>%2F2016%2F04%2F21%2F2016-04-21-java_gc_mechanism%2F</url>
    <content type="text"><![CDATA[Java GC机制是JVM中一个最重要的部分，了解GC机制也对了解JVM中内存分布，日志查看等原理有所了解 存储划分垃圾回收算法 1、引用计数(Reference Counting) 2、标记清除(Mark-Sweep) 3、复制(Copying) 4、标记-整理(Mark-Compact) 5、增量收集(Incremental Collecting) 6、分代(Generational Collecting)：基于对对象生命周期分析后得出的垃圾回收算法。把对象分为年轻代、年老代、持久代，对不同生命周期的对象使用不同的算法（上述方式中的一个）进行回收。jdk从J2SE1.2开始都是使用分代算法。下面我们都具体介绍分代算法。 分代算法对象存储划分1.Young(年轻代)年轻代分为三个区：一个Eden区，两个Survivor区。 对象一般是在&#39;Eden&#39;区生成（大对象直接进入老年代）。当`Eden`区满时，还存活的对象将被复制到`Survivor`区（两个中被标记为活Survivor区的一个），当活动Survivor区满时，该区的存活对象将会被复制到非活动Survivor区，此时两个区的标志得到改变（非活动Survivor区变成活动Survivor区，&#39;活动Survivor`区变成`非活动Survivor`区），当新的活动Survivor区也满时，当对象被复制的次数超过`-XX:MaxTenuringThreshold`参数设定的值时还存在则会被复制到老年代。需要注意，Survivor的两个区是对称的，没先后关 系，所以同一个区中可能同时存在从Eden复制过来对象，和从前一个Survivor复制过来的对象，而复制到年老区的只有从第一个Survivor去过来的对象。而且，Survivor区总有一个是空的。 `-XX:MaxTenuringThreshold`：设置对象在新生代中最大的存活次数，默认为16 大对象的定义：需要大量连续内存空间的Java对象 例如：byte []bytes = new byte[4 * 1024 * 1024]; 1.年轻代已经无法存放 -XX:PretenureSizeThreshold参数：大于这个值的对象直接进入老年代 意义：避免在Eden区和两个Survivor区之间发生大量的内存拷贝。 2.Tenured(老年代) 老年代存放从年轻代存活的对象。一般来说老年代存放的都是生命期较长的对象。 3.Perm(持久代) 用于存放静态文件，Java类，方法的字节码。持久代大小通过-XX:MaxPermSize=进行设置。 内存申请和对象衰老过程内存申请过程 1.JVM会试图为相关Java对象在Eden中初始化一块内存区域； 2.当Eden空间足够时，内存申请结束。否则到下一步； 3.JVM试图释放在Eden中所有不活跃的对象（minor collection），释放后若Eden空间仍然不足以放入新对象，则试图将部分Eden中活跃对象放入Survivor区； 4.Survivor区被用来作为Eden及old的中间交换区域，当OLD区空间足够时，Survivor区的对象会被移到Old区，否则会被保留在Survivor区； 5.当old区空间不够时，JVM会在old区进行major collection； 6.完全垃圾收集后，若Survivor及old区仍然无法存放从Eden复制过来的部分对象，导致JVM无法在Eden区为新对象创建内存区域，则出现”Out of memory错误”； 对象衰老过程 对象的衰老的过程就是GC的过程。可通过jstat –gcutil gc log查看GC日志。GC有两种类型：Minor GC 和Full GC GC类型 触发条件 产生动作 注意 Minor GC 大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够的空间进行分配时，虚拟机将会发起一次Minor GC。 (1)GC后仍然存活的对象复制到活动Survivor区，假设又经过GC则将Eden区和活动Survivor区都移动到非活动Survivor区，并标记为活动Survivor区。(2)清空Eden+活动survivor中所有no reference的对象占用的内存将eden+活动survivor中所有存活的对象copy到非活动survivor中一些对象将晋升到old中:非活动survivor放不下的存活次数超过turningthreshold中的重新计算tenuring threshold(serial parallel GC会触发此项)重新调整Eden 和from的大小(parallel GC会触发此项) 全过程暂停应用;是否为多线程处理由具体的GC决定 Full GC 触发条件请参考以下四点 1.清空heap中no reference的对象2.Perm区中已经被卸载的classloader中加载的class信息3.如配置了CollectGenOFirst,则先触发YGC(针对serial GC)4.如配置了ScavengeBeforeFullGC,则先触发YGC(针对serial GC) 全过程暂停应用;是否为多线程处理由具体的GC决定;是否压缩需要看配置的具体GC Full GC触发条件： 1).老年代空间不足：老年代空间只有在新生代对象复制进来以及直接创建的是大对象才会出现不足的现象，此时执行Full GC，当空间仍不足时，则抛出java.lang.OutOfMemoryError: Java heap space 2).Perm 空间满：Perm区存放的为一些class的信息等，当系统中要加载的类、反射的类和调用的方法较多时，Perm区可能被占满，在没有配置为采用CMS GC的情况下将会执行Full GC。当空间仍不足时，则抛出java.lang.OutOfMemoryError: PermGen space 3).CMS GC时出现promotion failed和concurrent mode failure:对于采用CMS进行旧生代GC的程序而言，尤其要注意GC日志中是否有promotion failed和concurrent mode failure两种状况，当这两种状况出现时可能会触发Full GC。promotion failed是在进行Minor GC时，survivor space放不下、对象只能放入旧生代，而此时旧生代也放不下造成的；concurrent mode failure是在执行CMS GC的过程中同时有对象要放入旧生代，而此时旧生代空间不足造成的。应对措施为：增大survivorspace、旧生代空间或调低触发并发GC的比率，但在JDK 5.0+、6.0+的版本中有可能会由于JDK的bug29导致CMS在remark完毕后很久才触发sweeping动作。对于这种状况，可通过设置-XX:CMSMaxAbortablePrecleanTime=5（单位为ms）来避免。 4).统计得到的Minor GC晋升到旧生代的平均大小大于旧生代的剩余空间这是一个较为复杂的触发情况，Hotspot为了避免由于新生代对象晋升到旧生代导致旧生代空间不足的现象，在进行Minor GC时，做了一个判断，如果之前统计所得到的Minor GC晋升到旧生代的平均大小大于旧生代的剩余空间，那么就直接触发Full GC。例如程序第一次触发MinorGC后，有6MB的对象晋升到旧生代，那么当下一次Minor GC发生时，首先检查旧生代的剩余空间是否大于6MB，如果小于6MB，则执行Full GC。当新生代采用PSGC时，方式稍有不同，PS GC是在Minor GC后也会检查，例如上面的例子中第一次Minor GC后，PS GC会检查此时旧生代的剩余空间是否大于6MB，如小于，则触发对旧生代的回收。 除了以上4种状况外，对于使用RMI来进行RPC或管理的Sun JDK应用而言，默认情况下会一小时执行一次Full GC。可通过在启动时通过- java-Dsun.rmi.dgc.client.gcInterval=3600000来设置Full GC执行的间隔时间或通过-XX:+ DisableExplicitGC来禁止RMI调用System.gc。 对象分配规则 1.对象优先分配在Eden区，如果Eden区没有足够的空间时，虚拟机执行一次Minor GC。 2.大对象直接进入老年代（大对象是指需要大量连续内存空间的对象）。这样做的目的是避免在Eden区和两个Survivor区之间发生大量的内存拷贝（新生代采用复制算法收集内存）。 3.长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄计数器，如果对象经过了1次Minor GC那么对象会进入Survivor区，之后每经过一次Minor GC那么对象的年龄加1，知道达到阀值对象进入老年区。 4.动态判断对象的年龄。如果Survivor区中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代。 5.空间分配担保。每次进行Minor GC时，JVM会计算Survivor区移至老年区的对象的平均大小，如果这个值大于老年区的剩余值大小则进行一次Full GC，如果小于检查HandlePromotionFailure设置，如果true则只进行Monitor GC,如果false则进行Full GC。 以下附上JVM经典配置： 吞吐量优先的并行收集器如上文所述，并行收集器主要以到达一定的吞吐量为目标，适用于科学技术和后台处理等。典型配置： § java -Xmx3800m -Xms3800m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20-XX:+UseParallelGC：选择垃圾收集器为并行收集器。此配置仅对年轻代有效。即上述配置下，年轻代使用并发收集，而年老代仍旧使用串行收集。-XX:ParallelGCThreads=20：配置并行收集器的线程数，即：同时多少个线程一起进行垃圾回收。此值最好配置与处理器数目 相等。 § java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20 -XX:+UseParallelOldGC-XX:+UseParallelOldGC：配置年老代垃圾收集方式为并行收集。JDK6.0支持对年老代并行收集。 § java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:MaxGCPauseMillis=100-XX:MaxGCPauseMillis=100:设置每次年轻代垃圾回收的最长时间，如果无法满足此时间，JVM会自动调整年轻代大小，以满足此值。 § java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:MaxGCPauseMillis=100 -XX:+UseAdaptiveSizePolicy-XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动选择年轻代区大小和相应的Survivor区比例，以达到目标系统规定的最低相应时间或者收集频率等，此值建议使用并行收集器时，一直打开。 响应时间优先的并发收集器如上文所述，并发收集器主要是保证系统的响应时间，减少垃圾收集时的停顿时间。适用于应用服务器、电信领域等。典型配置： § java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:ParallelGCThreads=20 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC-XX:+UseConcMarkSweepGC：设置年老代为并发收集。测试中配置这个以后，-XX:NewRatio=4的配置失效了，原因不明。所以，此时年轻代大小最好用-Xmn设置。-XX:+UseParNewGC:设置年轻代为并行收集。可与CMS收集同时使用。JDK5.0以上，JVM会根据系统配置自行设置，所以无需再设置此值。 § java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseConcMarkSweepGC -XX:CMSFullGCsBeforeCompaction=5 -XX:+UseCMSCompactAtFullCollection-XX:CMSFullGCsBeforeCompaction：由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此值设置运行多少次GC以后对内存空间进行压缩、整理。-XX:+UseCMSCompactAtFullCollection：打开对年老代的压缩。可能会影响性能，但是可以消除碎片]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>gc</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[压测工具siege的简单使用]]></title>
    <url>%2F2016%2F04%2F20%2F2016-04-20-siege%2F</url>
    <content type="text"><![CDATA[Siege是一个压力测试和评测工具，设计用于WEB开发这评估应用在压力下的承受能力：可以根据配置对一个WEB站点进行多用户的并发访问，记录每个用户所有请求过程的相应时间，并在一定数量的并发访问下重复进行。 Siege IntroductionUbuntu Install1234#检验siege是否存在apt中 sudo apt-cache search siege#安装sudo apt-get install siege Parameter Description Parameter Description -C或-config 在屏幕上打印显示出当前的配置,配置是包括在他的配置文$HOME/.siegerc中,可以编辑里面的参数,这样每次siege 都会按照它运行. -v 输出版本信息和siege一些命令 -c n或-concurrent=n 模拟n个用户同时并发访问；n不要设置的太大，因为越大，siege消耗的资源就更多 -i 或 -internet 随机访问urls.txt中的url列表项，以此模拟真实的访问情况（随机性） -d n 或 -delay=n hit每个url之间访问的延迟，在0-n之间;一般该参数不会&gt;10 -r n 或 -reps=n 重复运行测试n次，不能与-t同时设置 -t n 或 -time=n 持续运行siege n秒，如10S(秒)，10M(分钟),10H(小时) -l 运行结束，将统计数据保存到日志文件中siege.log，可通过siege -C查看siege.log的具体位置，也可在配置文件中自定义 -R SIEGERC 或 -rc=SIEGERC 指定用特定的siege配置文件来运行，默认为$HOME/.siegerc -f file 或 -file=FILE 指定urls文件来运行 -u URL 或 -url=URL 测试指定的url Use Parameter siege -C siege -v siege -d 10 -c 50 -t 10 https://www.baidu.com/ (模拟50个用户并发访问百度首页10秒) siege -d 10 -i -c 100 -t 10S -f urls.txt (模拟100个用户并发随机访问urls中的网址) Result Parameter Description123456789101112Transactions:165 hits //总请求次数165次Availability:100.00 % //成功率100%Elapsed time:9.55 secs //使用时间Data transferred:0.03 MB //总数据传输Response time:0.05 secs // 响应时间，显示网络连接的速度Transaction rate:17.28 trans/sec //平均每秒完成17.28次处理Throughput:0.00 MB/sec//平均每秒传输数据Concurrency:0.84 //实际最高并发连接数Successful transactions:165 //成功请求次数Failed transactions:0//失败请求次数Longest transaction:0.12 //每次传输花费最长时间Shortest transaction:0.02//每次传输花费最短时间]]></content>
      <categories>
        <category>Knowledge</category>
      </categories>
      <tags>
        <tag>siege</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用maven在eclipse中上传code到私服]]></title>
    <url>%2F2015%2F11%2F13%2F2015-11-13-mavenUploadToNexus%2F</url>
    <content type="text"><![CDATA[最近在公司新开发了一个公用模块，然后需要上传到公司自己的maven私服nexus上，中途遇到蛮多的困难的。所以想在这里总结一下自己的错误，以防以后再犯。 上传至私服的准备工作—配置1.模块pom文件的配置123456789101112131415161718192021222324252627282930313233&lt;build&gt; &lt;plugins&gt; &lt;!-- 要将源码放上去，需要加入这个插件 --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.1.2&lt;/version&gt; &lt;configuration&gt; &lt;attach&gt;true&lt;/attach&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;name&gt;releases&lt;/name&gt; &lt;url&gt;maven私服releases的url&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;name&gt;snapshots&lt;/name&gt; &lt;url&gt;maven私服snapshots快照的url&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 2.settings.xml的配置 需要在这个区域内加入配置,这个username和password分别是对应releases和snapshots的用户名和密码12345678910&lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;username&lt;/username&gt; &lt;password&gt;password&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;username&lt;/username&gt; &lt;password&gt;password&lt;/password&gt;&lt;/server&gt; 3.部署到maven私服 完成上述两步那么准备工作就完成了，之后就可以进行上传部署操作了。这里有两种方式来进行部署。 （1）maven命令行部署 mvn deploy -X （2）eclipse下的部署方式 部署如下图： 问题总结一般执行deploy -X会打印出出错信息至控制台，报错一般都会返回Return Code,【400、401、402、403、404、405、500、502、503】 1. 405错误问题分析： 405错误的含义是“用来访问本页面的HTTP方法不被允许”，所以这个问题一般是配置上的repository的地址写错了，或者是端口写错了。 解决方案： 检查repository的地址是否写错并改正 2. 401或403错误问题分析： 403错误的含义是“禁止访问”，那么当然是在maven私服上设置了不让该用户访问。 解决方案： 将Releases仓库默认的Deployment Policy修改为“Allow Redeploy” 如果是使用deployment账号登录的朋友请参考《maven报错：mvn deploy ）》 3. 400错误问题分析： 400错误的含义是“错误的请求”，在这里的原因是往往是没有部署到nexus的仓库中。nexus的repository分三种类型：Hosted、 Proxy和Virtual，另外还有一个repository group(仓库组)用于对多个仓库进行组合。部署的时候只能部署到Hosted类型的仓库中，如果是其他类型就会出现这个400错误。 maven的部署是有针对性的，假设只是配置了releases库，但是模块的artifactId中存在SNAPSHOT字样也会报400错误，同理相反也是这种情况。 解决方案： 将releases库和snapshots库两个配置都配置上，或者一一对应的配置。 4. 500错误错误原因：服务器满了 5. 402错误错误原因：你使用的是nexus的Professional版本，但是你的license已经过期了，需要重新注册。 这里只列举出这几种错误，至于其他的错误请参考《Maven deploy部署失败原因及解决）》]]></content>
      <categories>
        <category>Experience</category>
      </categories>
      <tags>
        <tag>maven</tag>
        <tag>eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC的Controller的方法约束]]></title>
    <url>%2F2015%2F08%2F16%2F2015-08-16-SpringMVC_Controller_MethodConstraint%2F</url>
    <content type="text"><![CDATA[介绍SpringMVC的Controller的方法约束 Controller内的方法限制传递的参数类型(1)`HttpServlet`的内置对象。{`ServletRequest`,`HttpServletRequest`,`ServletResponse`,`HttpServletResponse`} (2)`HttpSession`对象：当传递这种参数类型时必须保证会话是存在的，不能为null。 Notes：Session访问可能不是线程安全的，特别是在一个Servlet环境。如果多个请求被允许同时访问一个会话,考虑设置 RequestMappingHandlerAdapter的“synchronizeOnSession”标志为“true”， (3)`org.springframework.web.context.request.WebRequest`或者`org.springframework.web.context.request.NativeWebRequest`。 (4)`java.util.Locale`：当前请求的语言环境，LocaleResolver在Servlet环境中配置 (5)`java.io.InputStream`/`java.io.Reader`：request中的内容 (6)`java.io.OutputStream` / `java.io.Writer`：用于生成response对象中的内容 (7)`java.security.Principal`:包含当前身份验证的用户 (8)`HttpEntity&lt;?&gt;`:用于接收Http的请求头中的内容（headers和contents）.通过HttpMessageConverter来将请求数据转换成实体。 (9)`java.util.Map` / `org.springframework.ui.Model` / `org.springframework.ui.ModelMap`:将后台需要的数据传递到视图层 (10)`org.springframework.web.servlet.mvc.support.RedirectAttributes`:当完成一个数据库对应的操作时，需要给前台做出 一个响应，为了防止刷新重复提交表单，不能采用request的方式，所以采用这种方式将信息临时存储在服务器端，使其可用于 重定向之后的请求. (11)`org.springframework.validation.Errors` / `org.springframework.validation.BindingResult`:这个参数必须紧接在配置 了@Valid注解的参数之后，有多少个配置了@Valid注解的参数就有多少个该对象。用于验证一个表单对象或者结果前的命令 (12)`org.springframework.web.bind.support.SessionStatus`：可以通过该类型 status 对象显式结束表单的处理， 这相当于触发 session 清除其中的通过 @SessionAttributes 定义的属性 (13)`org.springframework.web.util.UriComponentsBuilder`:针对当前请求的主机，端口，scaheme,context path和servlet 对应的映射路径来构造uri 下面是一些可传递的注解类型的参数： (1)`@PathVariable`:注解用于访问uri上的模板参数，可接收一些基本类型，如int，long，String，Date等基本类型 (2)`@MatrixVariable`:注解用于访问uri上的若干组键值对参数；该注解传递参数的uri有如下格式： 1.&quot;/cars;color=red;year=2012&quot; 2. &quot;color=red,green,blue&quot; 3.&quot;color=red;color=green;color=blue&quot; (3)`@RequestParam`:注解用于访问request请求中参数，并会将值自动转换成注解对应的类型。 (4)`@RequestHeader`:注解用于访问request HTTP请求头信息，并且值也会做相应的转换。 (5)`@RequestBody`:注解用于访问HTTP请求体中的内容。值通过HttpMessageConverter来转换。 (6)`@RequestPart`:注解用于访问当表单的enctype设置为&#39;multipart/form-data&#39;时的请求内容。 Controller内的方法支持的返回类型(1)`ModelAndView Object`:返回对应的视图层和Model层的数据. (2)`Model Object`:返回对应设定的Model层的数据,主要包含Spring封装好的model和modelMap，以及java.util.Map 当没有视图返回时视图名称由RequestToViewNameTranslator决定，一般是配置了RequestMapping的映射路径。 (3)`Map Object`:与Model类型相似。 (4)`View Object`:返回对应的视图对象，并且可以通过`render(Map&lt;String ,?&gt; , request , response)`参数向前台传递 属性。 (5)`String Object`:返回一个视图名称，Spring会通过相应的ViewResolver来解析 (6)`void`:返回空值。 (7)`other type`:其他任何被认为不是属于上述的类型的返回类型，使用@ModelAttribute在方法级别上指定名称。 以上内容仅属于个人观点，有错误的地方希望各位同仁能加以指正！谢谢！]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eclipse下构建SpringMVC的Maven项目]]></title>
    <url>%2F2015%2F08%2F15%2F2015-08-15-buildMavenSpringMVCProject_InEclipse%2F</url>
    <content type="text"><![CDATA[最近在学习Spring、SpringMVC、Maven，想在Eclipse下构建maven项目，不过由于是第一次操作且Eclipse默认支持的创建org-archetype-webapp需要修改许多的配置，所以望在此分享给大家， 1.引言最近在学习Spring、SpringMVC、Maven，想在Eclipse下构建maven项目，不过由于是第一次操作且Eclipse默认支持的创建org-archetype-webapp需要修改许多的配置，所以望在此分享给大家，可以让以后在构建的过程中不用花太多时间。 2.eclipse下使用maven构建SpringMVC在eclipse官网下载的Eclipse IDE for Java EE Developers的版本默认是已经安装了m2e插件的，所以大家不需要去重新安装； 我使用的是maven3.3.3和Eclipse4.4.2 1)maven下载链接：maven3.3.3 2)eclipse下载链接：eclipse 3)如果eclipse下没有m2e的插件，可参考eclipse下安装m2e Notes:最后将eclipse中的maven版本配置成本地的maven，配置方式如下图： 在eclipse中点击Window-&gt;Maven-&gt;Installations-&gt;Add;之后添加自己本地的maven路径就行； 软件已经准备好了！那么我们开始构建maven项目吧！ maven构建web项目点击Eclipse上方的File-&gt;New-&gt;Other-&gt;Maven Project-&gt;Next-&gt;Next（这里默认就好）-&gt;选择maven-archetype-webapp-&gt;Next-&gt;填写project信息-&gt;Finish Notes：由于Eclipse创建的Maven项目默认的web版本是2.3的，且没有加入servlet的依赖包，而且可能 会没有`src/main/java`和`src/main/test`这两个目录，所以下面我们就来解决这个问题。 1)更改项目的默认jdk右键项目根路径-&gt;点击Properties 如果上一步的Server Runtime没有选项，则可以如下图配置tomcat为服务器 2)更改配置文件切换视图为Navigator,[由于不方便截图，文字描述]选择Window-&gt;Show View-&gt;Other-&gt;Navigator] 1.将org.eclipse.wst.common.component配置文件中的project-version的1.5.0修改为1.6.0 2.将org.eclipse.wst.common.project.facet.core.xml的jst.web对应的version修改为2.5 这样，我们的maven项目准备工作就做好了，下面我们开始编写SpringMVC的程序吧！ 2.构建SpringMVC工程2.1 添加Spring依赖包用Maven POM Editor打开pom.xml，然后选择Dependencies视图，在Dependencies下点击Add Spring的依赖包： Group Id: org.springframework Artifact Id: spring-web Version: 3.2.5.RELEASE 添加完毕后需要将远程包加载到本地来，右键项目根路径Run As-&gt;Maven install 如果报错-Dmaven.multiModuleProjectDirectory system propery is not set. Check $M2_HOME environment variable and mvn script match. 可以设一个环境变量M2_HOME指向你的maven安装目录 然后在Window-&gt;Preference-&gt;Java-&gt;Installed JREs-&gt;Edit 在Default VM arguments中设置 -Dmaven.multiModuleProjectDirectory=$M2_HOME 2.2 编写web.xml和spring-mvc的xmlweb.xml 如下：123456789101112131415&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;web-app xmlns="http://java.sun.com/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd" version="3.0"&gt; &lt;servlet&gt; &lt;servlet-name&gt;spring-mvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;spring-mvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;/web-app&gt; spring-mvc-servlet.xml 如下：123456789101112131415161718&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.2.xsd"&gt; &lt;context:component-scan base-package="cn.springmvc.controller" /&gt; &lt;bean id="viewResolver" class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="/WEB-INF/views/" /&gt; &lt;property name="suffix" value=".jsp" /&gt; &lt;/bean&gt; &lt;/beans&gt; 需要在WEB-INF目录下创建views目录并创建HelloWorld.jsp文件12345678910111213&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt; &lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt; &lt;html&gt; &lt;head&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt; &lt;title&gt;Insert title here&lt;/title&gt;title&gt; &lt;/meta&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;message:₵&#123;message&#125;&lt;/h1&gt;h1&gt; &lt;/body&gt;&lt;/html&gt; 并且需要在src/main/java下建立一个控制层，包名cn.springmvc.controller，类名为Hello.java 123456789101112131415package cn.springmvc.controller;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;@Controllerpublic class Hello &#123; @RequestMapping("/hello") public String HelloWorld(Model model) &#123; model.addAttribute("message" , "Hello World!!"); return "HelloWorld"; &#125;&#125; 运行程序右键项目根路径Run As-&gt;Run On Server-&gt;Next-&gt;Finish 运行结果如下：]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>maven</tag>
        <tag>eclipse</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随手记]]></title>
    <url>%2F2015%2F08%2F04%2F2015-08-04-notes_001%2F</url>
    <content type="text"><![CDATA[有时候，心里有事，很想找个人倾诉，于是打电话，”没人接啊，睡觉吧。”第二天，电话拨回来，可是你已经没了想说的情绪。有时候，看到一件衣服很喜欢，没适合码，过几天有了，你却已经失去了拥有的欲望。曾经你喜欢上一个人，喜欢的死去活来，难以救药，喜欢到你以为再也不会这样去喜欢一个人了。几年后，却模糊的想不起来。 原来，所有那些激情、冲动、放不开、舍不得的当下，都会随着时间，在岁月里悄悄流逝，慢慢消散。所以，何必念念不忘，何必苦苦执着。 你是你自己的作者，又何必写那么难演的剧本。后来的后来，你会知道。那些所有你以为过不去的过去，都会过去。握不住的沙，不如扬了它。 有时候，是我们自己想得太多，才让自己如此难受。你的脾气赶走了很多人，但留下了最真的人。上天给了你这种活，因为它知道你强大到可以活下去。别在喜悦时许诺，别在忧伤时回答，别在愤怒时做决定。生命中总有那么一段时光，充满不安，可是，除了勇敢面对，我们别无选择。 一个懂你泪水的朋友，胜过一群只懂你笑容的朋友。 有的人对你好，是因为你对他好，有的人对你好，是因为懂得你。 幸福，从来都没有捷径，也没有完美无瑕， 只有经营，只靠真心。]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>mood</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合类--List篇]]></title>
    <url>%2F2015%2F07%2F27%2F2015-07-27-Java_collection_List%2F</url>
    <content type="text"><![CDATA[1. 集合类概述 2. Collection接口 2.1 List篇 2.1.1 List遍历方式 2.1.2 ArrayList的实现原理 2.1.3 LinkedList的实现原理 2.1.4 ArrayList和LinkedList比较 2.2 List的使用场景 1. 集合类概述 Java中以一种比数组存储更复杂的方式来存储对象的一组对象—-“容器类”，其基本类型有List、Set和Map，它们被组织在以Collection及Map接口为根的层次结构中，称之为集合框架。 2. Collection接口Collection的层次结构图 2.1 List篇List的特征是其元素以线性的方式的存储，集合中允许放重复元素。 List接口主要的实现类有： - ArrayList() : 代表长度可以动态改变的数组；可以对元素进行随机的访问，向ArrayList 中插入与删除元素的速度慢。 - LinkedList() : 采用链表结构实现。其插入和删除的速度快。 2.1.1 List遍历方式：(1)for循环遍历 /** * List中的get(int index)方法支持取指定下标的元素 */ 1)使用一般for循环 for(int i = 0 ; i &lt; list.size ; i ++) { E obj = list.get(i); } 2)使用增强型for循环 for(E obj : list) { } (2)使用迭代器（Iterator） /** * Collection集合接口继承了Iterator接口，允许集合被迭代 * 迭代方式如下： */ Iterator&lt;E&gt; it = list.iterator(); while(it.hasNext()) { E obj = it.next(); } 2.1.2 ArrayList实现原理ArrayList部分源码分析：1234567891011121314151617/*** Default initial capacity.*/private static final int DEFAULT_CAPACITY = 10;// 存放元素的数组private transient Object[] elementData;/* 构造方法，* @param initialCapacity : 设置数组的初始容量*/public ArrayList(int initialCapacity) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); this.elementData = new Object[initialCapacity];&#125; 从源码可以分析出：ArrayList采用一个Object型对象数组来存放元素，默认初始容量为10。123456789101112131415public boolean add(E e) &#123; // 当添加元素时判断数组容量超出可容纳范围时会自动扩容 // 具体会调用grow(int minCapacity)方法 ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; // 该方法会自动给数组扩大一倍,数组可被扩大的最大容量为Integer.MAX_VALUE private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); ....省略部分代码 &#125; 2.1.3 LinkedList实现原理LinkedList采用双向链表结构来实现的,使用结点来存储数据域和连接前一个和后一个结点.123456789101112131415161718192021222324252627282930313233343536static class Node&lt;E&gt; &#123; E item; // 用来保存数据域 Node&lt;E&gt; next; // 下一个结点 Node&lt;E&gt; prev; // 上一个结点 ....此处省略&#125;// 代指第一个结点transient Node&lt;E&gt; first; transient Node&lt;E&gt; last;// 这里只做add方法的讲解public boolean add(E e) &#123; // 链接下一个结点 linkLast(e); return true;&#125;void linkLast(E e) &#123; // 保存当前的最后一个结点 final Node&lt;E&gt; l = last; //创建一个新添加元素的结点，前一个结点为该链表的最后一个结点 final Node&lt;E&gt; newNode = new Node&lt;E&gt;(l , e , null); // 将新结点作为最后一个结点 last = newNode; // 如果未添加前最后一个结点为空，则说明此时链表没有元素，将新结点作为首结点 // 否则将新结点链接在末尾 if(l != null) first = newNode; else l.next = newNode; size++; modCount++;&#125; LinkedList数据结构是链式结构，具体查阅数据结构相关的书籍。 2.1.4 ArrayList和LinkedList比较ArrayList和LinkedList的区别： 1.ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。 2.对于随机访问get和set，ArrayList觉得优于LinkedList，因为LinkedList要移动指针。 3.对于新增和删除操作add和remove，LinedList比较占优势，因为ArrayList要移动数据。 2.2 List使用场景如果涉及到“栈”、“队列”、“链表”等操作，应该考虑用List，具体的选择哪个List，根据下面的标准来取舍。 01) 对于需要快速插入，删除元素，应该使用LinkedList。 02) 对于需要快速随机访问元素，应该使用ArrayList。 03) 对于“单线程环境” 或者 “多线程环境，但List仅仅只会被单个线程操作”，此时应该使用非同步的类(如ArrayList)。 对于“多线程环境，且List可能同时被多个线程操作”，此时，应该使用同步的类(如Vector)。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>collections</tag>
        <tag>list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM启动工作原理]]></title>
    <url>%2F2015%2F07%2F22%2F2015-07-22-JVM_start_principle%2F</url>
    <content type="text"><![CDATA[JVM的一些基本知识 1. JVM体系结构(1)类加载器（`ClassLoader`）（用来装载`.class`文件） (2)执行引擎（执行字节码，或者执行本地方法） (3)运行时数据区（方法区、堆、java栈、本地方法栈、PC寄存器） 2. JVM的生命周期1.JVM实例对应了一个独立运行的java程序，它是进程级别; a）启动。启动一个java程序时，一个JVM实例就产生了，任何一个拥有`public static void main(String [] args)`方法的class都可以作为JVM实例运行的起点 b）运行。`main()`作为该程序的初始线程的起点，任何其他线程均由该线程启动。JVM内部有两种线程：守护线 程和非守护线程，`main()`属于非守护线程守护线程通常由JVM自己使用，java程序也可以标明自己创建的线程 是守护线程 c）消亡。当程序中的所有非守护线程都终止时，JVM才退出；若安全管理器允许，程序也可以使用`Runtime`类 或者`System.exit()`来退出 2.JVM执行引擎实例则对应了属于用户运行程序的线程，它是线程级别的; 3. JVM启动JVM工作原理和特点主要是指操作系统装入JVM，是通过JDK中java.exe来完成，通过下面4步来完成JVM环境. 1.创建`JVM`装载环境和配置 2.装载`JVM.dll` 3.初始化`JVM.dll`并挂界到`JNIENV`(`JNI`调用接口)实例 4.调用`JNIENV`实例装载并处理`class`类 3.1 启动代码分析main函数在openjdk\jdk\src\share\bin\main.c文件中。简单流程分析如下： 1.SelectVersion-&gt;LocateJRE,定位jre的位置。因为在不同的操作系统上定位jre的方式不同，相关代码就 和平台相关了，windows相关的一些代码在`openjdk\jdk\src\windows`下，而`linux`和`solaris`相关 代码在`openjdk\jdk\src\solaris`下。这部分调用到的代码都在`{os}\bin\java_md.c`中。简单的说， windows中，LocateJRE要通过查找注册表来定位jre的位置，而Linux下可能需要读取环境变量等。 2.CreateExecutionEnvironment-&gt;GetJVMPath得到jvm的路径，其实就是找到相应的动态链接 库，具体到linux上，就是`libjvm.so`。 3.LoadJavaVM，linux上是加载&#39;libjvm.so&#39;,windows上是加载`jvm.dll`，导 出`JNI_CreateJavaVM`、`JNI_GetDefaultJavaVMInitArgs`等函数。 4.获取classpath。 5.ContinueInNewThread(&amp;ifn,argc,argv,jarfile,classname,ret);在一个新线程中启动jvm。 ContinueInNewThread的功能是：调用`GetDefaultJavaJVMInitArgs`(其实就 调用`JNI_GetDefaultJavaVMInitArgs`)获取虚拟机初始化参数，设定线程栈的大小， 创建java程序需要的各项参数。然后调用下面函数 `ContinueInNewThread0(JavaMain, threadStackSize, (void*)&amp;args);` ;因为不同操作系统创建线程的方式不同，所以又进 入os相关的代码，`{os}\bin\java_md.c`中的`int ContinueInNewThread0(JNICALL *continuation)(void *), jlong stack_size,void *args)`函数。 阅读其源码，可知linux上是通过`pthread_create`创建线程，Solaris通过`thr_create`创建线程， 而`windows`则通过`_beginthreadex`创建线程。 在新线程中调用JavaMain函数，即openjdk\jdk\src\share\bin\main.c中的 `int JNICALL JavaMain(void *_args)`,它负责加载要运行的的java class，并调用class的main方法。 处理步骤是： 1).InitalizeJVM-&gt;CreateJavaVM(即通过JNI调用JNI_CreateJavaVM),创建jvm。 2).LoadMainClass加载main class，并确保main方法的签名是正确的。 3).mainID = (*env)-&gt;GetStaticMethodID(env, mainClass, &quot;main&quot;,&quot;([Ljava/lang/String;)V&quot;);通过JNI得到main方法的method信息。 4).mainArgs = NewPlatformStringArray(env, argv, argc);为main方法准备参数数组。 5).(*env)-&gt;CallStaticVoidMethod(env, mainClass, mainID, mainArgs);通过JNI调用main方法。 6).ret = (*env)-&gt;ExceptionOccurred(env) == NULL ? 0 : 1;处理异常 7).DetachCurrentThread 8).DestroyJavaVM 销毁JVM 下面以windows的实现进行分析： 首先查找jre路径，Java通过GetApplicationHome api来获得当前的java.exe绝对路径（例如我电脑上的e:\program \java\jdk\bin\java.exe），那么它会截取到绝对路径e:\program\java\jdk，判断e:\program\java\jdk\jvm.dll文件是否存在，如果存在就把e:\program\java\jdk，作为jre路径；如果不存在则判断e:\program\java\jdk\jre\jvm.dll是否存在，如果存在则将e:\program\java\jdk\jre作为jre路径。如果不存在调用GetPublicJREHome查HKEY_LOCAL _MACHINE\Software\JavaSoft\Java Runtime Environment\&quot;当前JRE版本号&quot;\JavaHome的路径作为jre路径。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaClass文件结构]]></title>
    <url>%2F2015%2F07%2F21%2F2015-07-21-Java_class_structs%2F</url>
    <content type="text"><![CDATA[1.Class文件概述 1.1 无符号数和表 1.2 魔数和Class文件的版本 1.3 常量池 1.3.1 javap的使用 1.3.2 常量池中的14种常量项的结构总表 1.4 访问标志 1.5 类索引、父类索引和接口索引集合 1.6 字段表集合 1.7 方法表集合 1.8 属性表集合 1.Class文件概述Class文件是一组以8位字节为基础的二进制流，各个数据项目严格按照顺序紧凑地排列在Class文件之中，中间没有添加任何分隔符，这使得整个Class文件中存储的内容几乎全部是程序的必要数据，没有空隙存在。当遇到需要占用8位字节的以上的空间的数据项时，则会按照高位在前的方式分隔成若干个8位字节进行存储。 根据Java虚拟机规范的规定，Class文件格式采用一种类似于C语言结构体的伪结构来存储数据，这种伪结构中只有两种数据类型：无符号和表，后面的解析都要以这两种数据类型为基础，所以这里要先介绍这两个概念。 1.1 无符号数和表 无符号数属于基本的数据类型，以u1、u2、u4、u8来分别代表1个字节、2个字节、4个字节和8个字节的无符号数，无符号数可以用来描述数字、索引引用、数量值或者按照UTF-8编码构成字符串值。 表是由多个无符号数或者其他表作为数据项构成的复合数据类型，所有表都习惯性的以”_info”结尾。表用于描述有层次关系的复合结构的数据，整个Class文件本质上就是一张表。它由表6-1所示的数据项构成。 1.2 魔数与Class文件的版本 每个Class文件的头4个字节称为魔数（Magic Number），它的唯一作用是确定这个文件是否为一个能被虚拟机接受的Class文件。 Class文件的魔数很有“浪漫气息”，值为0xCAFEBABY(咖啡宝贝). 紧接着魔数的4个字节值是Class文件的版本号：第5和第6个字节是次版本号（Minor Version）,第7和第8个字节是主版本号（Major Version）。Java的版本号是从45开始的。 为了方便讲解，现准备了一段最简单的Java代码（见代码清单1-1） 代码清单1-1 package org.fenixsoft.clazz; public class TestClass { private int m; public int inc() { return m+1; } } 下面是部分class文件中的内容： 1.3 常量池 紧接着主次版本号之后的是常量池入口，常量池可以理解为Class文件之中的资源仓库，它是Class文件结构中与其他项目关联最多的数据类型，也是占用Class文件空间最大的数据项目之一，同时它还是在Class文件中第一个出现的表类型数据项目。 由于常量池常量的数量是不固定的，所以在常量池的入口需要放置一项u2类型的数据，代表常量池容量计数值(constant_pool_count)。 如下图所示 常量池容量（偏移地址：0x00000008）为十六进制数0x0016，即二进制的22.这就代表常量池中有21项常量，索引值范围为1-21.在Class文件格式规范制定之时，设计者将第0项常量空出来在于满足后面某些指向常量池的索引值的数据在特定情况下需要表达“不引用任何一个常量池项目”的含义。 常量池主要存放两大类常量：字面量（Literal）和符号引用（Symbolic References）。字面量比较接近 Java语言层面的常量概念，如文本字符串,声明为final的常量值等。而符号引用则属于编译原理方面的概念， 包括了下面三类常量： 1.类和接口的全限定名（Full Qualified Name） 2.字段的名称和描述符（Descriptor） 3.方法的名称和描述符 常量池的每一项常量都是一个表，在JDK1.7之前共有11种结构各不相同的表结构数据，在JDK1.7中为了更好支持动态语言调用，又额外增加了3种（CONSTANT_MethodHandle_info、CONSTANT_MethodType_info、CONSTANT_InvokeDynamic_info）。 这14种表都有一个共同特点，就是表开始的第一位是一个u1类型的标志位(tag，取值见下表中标志列)，代表当前常量属于哪种常量类型. 1.3.1 javap的使用 在JDK的bin目录中，有一个专门用于分析Class文件字节码的工具：javap，下图列出了使用javap工具的-verbose参数输出的TestClass.class文件字节码内容。 1.3.2 常量池中的14种常量项的结构总表 1.4 访问标志 在常量池结束之后，紧接着的两个字节代表访问标志（access_flags），这个标志用于识别一些类或者接口层次的访问信息，包括：这个Class是类还是接口；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否被声明为final等。 具体的标志位以及标志的含义见下图 access_flags中一共有16个标志位可以使用，上图只定义了其中8个，未使用的标志位要求一律为0. 1.5 类索引、父类索引和接口索引集合 类索引(this_class)和父类索引(super_class)都是一个u2类型的数据，而接口索引集合(interfaces)是一组u2类型的数据的集合，Class文件中由这三项数据来确定这个类的继承关系。 1.6 字段表集合 字段表(field_info)用于描述接口或者类中声明的变量。字段(field)包括类级变量和实例级变量，但不包括在方法内部声明的局部变量。 字段表结构： 字段修饰符放在access_flags项目中，它与类中的access_flags项目非常类似的，都是一个u2的数据类型，其中可以设置的标志位的含义见下表： 跟随access_flags标志的两项索引值：name_index和descriptor_index。它们都是对常量池的引用，分别代表着字段的简单名称以及字段和方法的描述符。 名词解释 全限定名：将类名中的&#39;.&#39;替换成&#39;/&#39;，例如：java.lang.Object的全限定名为java/lang/Object 简单名称：指没有类型和参数修饰的方法和字段名称，例如Object的equals()方法的简单名称为equals 描述符：用来描述字段的数据类型，方法的参数列表(包括数量、类型以及顺序)和返回值。根据描述符规则 基本数据类型以及代表无返回值的void类型都用一个大写字符来表示，而对象类型则用字符L加对象的全限 定名来表示。描述数组类型时，每一维度将使用一个前置的&#39;[&#39;字符来描述。 描述符标识字符含义 用描述符来描述方法时，按照先参数列表，后返回值的顺序描述，参数列表按照参数的严格顺序放在一组小括号`()` 之内。如方法void inc()的描述符为`()V`,方法java.lang.String()的描述符为`()Ljava/lang/String;`, 方法int indexOf(char[] source, int sourceOffset, int sourceCount, char[] target,int targetOffset,int targetCount, int frmoIndex)的描述符为`([CII[CIII)I` 1.7 方法表集合 方法表的结构与字段表的结构相同，依次包括了访问标志(access_flags)、名称索引(name_index)、描述符索引(descriptor_index)、属性表集合(arrtibutes)几项。 方法访问标志 注意事项： 如果父类方法在子类中没有被重写(Override)，方法表集合中就不会出现来自父类的方法信息。但同样的 ，有可能会出现编译器自动添加的方法，最典型的便是类构造器`&lt;clinit&gt;`方法和实例构造器`&lt;init&gt;`方法。 1.8 属性表集合 在Class文件、字段表、方法表都可以携带自己的属性表集合，以用于描述某些场景专有的信息。 具体属性表在此不作讲解，请参考《深入理解Java虚拟机：JVM高级特性与最佳实践（最新第二版）》的第六章 文件提取码是2f30 声明本文摘自《深入理解Java虚拟机：JVM高级特性与最佳实践（最新第二版）》一书，为了更好的理解java的字节码文件。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何阅读GC日志]]></title>
    <url>%2F2015%2F07%2F20%2F2015-07-20-c_on_Java%2F</url>
    <content type="text"><![CDATA[一名合格的java程序员应具备基本的使用JVM能力，了解其启动时的配置参数，并且能够理解GC的输出日志。 阅读GC日志是处理Java虚拟机内存问题的基础技能，它只是一些认为确定的规则，没有太多技术含量。每一种收集器的日志形式都是由它们自身的实现而决定的，换而言之，每个收集器的日志格式都可以不一样。下面为两段典型的GC日志： 最前面的数字33.125;和100.667;代表了GC发生的时间，这个数字的含义是从Java虚拟机启动以来经过的秒数。 GC日志开头的[GC和[Full GC说明了这次垃圾收集的停顿类型，而不是用来区分新生代GC还是老年代GC的。如果有Full，说明这次GC是发生在Stop-the-world的，例如下面这段新生代收集器ParNew的日志也会出现[Full GC（这一般是因为出现了分配担保失败之类的问题，所以才会导致STW）。如果是调用System.gc()方法所触发的收集，那么在这里将显示[Full GC(System)。 [Full GC 283.776: [ParNew: 261559K-&gt;261559K(261559K),0.0000288 secs] 接下来的[DefNew，[Tenured，[Perm表示GC发生的区域，这里显示的区域名称与使用的GC收集器是密切相关的，例如上面样例所使用的Serial收集器的新生代名为Default New Generation，所以显示的是[DefNew。如果是ParNew收集器，新生代名称就会变成[ParNew，意为Parallel New Generation。如果采用的是Parallel Scavenge收集器，那它配套的新生代称为PSYoungGen，老年代和永久代同理，名称也是由收集器决定。 后面方括号内部的3324K-&gt;152K(3712K)含义是GC前该内存区域已使用容量-&gt;GC后该内存区域已使用容量（该内存区域总容量）。而在方括号之外的3324K-&gt;152K(11904K)表示GC前Java堆已使用容量-&gt;GC后Java堆已使用容量（Java堆总容量）。再往后，0.0025925 secs表示该内存区域GC所占用的时间，单位是秒。有的收集器会给出更具体的时间数据，如[Times: user=0.01 sys=0.02,real=0.02 secs]，这里面的user,sys和real与Linux的time命令所输出的时间含义一致，分别代表用户态消耗的CPU时间，内核消耗的CPU时间和操作从开始到结束的墙钟时间（Wall Clock Time）。CPU时间与墙钟时间的区别是，墙钟时间包括各种非运算的等待耗时，例如等待磁盘I/O，等待线程阻塞，而CPU时间不包括这些耗时，但当系统有多CPU或者多核的话，多线程操作会叠加这些CPU时间，所以读者看到user或sys时间超过real时间是完全正常的。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>gc</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java数据结构（四）：线性表之双向链表]]></title>
    <url>%2F2015%2F07%2F01%2F2015-07-01-java_datastruct_list_4%2F</url>
    <content type="text"><![CDATA[java实现简单的双向链表，双向链表也叫双链表，是链表的一种，它的每个数据结点中都有两个指针，分别指向直接后继和直接前驱。 所以，从双向链表中的任意一个结点开始，都可以很方便地访问它的前驱结点和后继结点。 一般我们都构造双向循环链表。 具体代码可参见：BidirectionalLinkedList.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120package datastructure.linear.linked;import datastructure.exception.StructureException;import datastructure.linear.AbstractList;/** * @Description 双向链表实现 * @author mastery * @Date 2015年6月30日下午9:11:51 */public class BidirectionalLinkedList&lt;T&gt; extends AbstractList&lt;T&gt; &#123; class Node&lt;T1&gt; &#123; /** * 数据域 */ T1 element; /** * 指向前驱结点的指针 */ Node&lt;T1&gt; prior; /** * 指向后继结点的指针 */ Node&lt;T1&gt; next; public Node(Node&lt;T1&gt; next) &#123; super(); this.prior = next; this.next = next; &#125; public Node(T1 element, Node&lt;T1&gt; prior, Node&lt;T1&gt; next) &#123; super(); this.element = element; this.prior = prior; this.next = next; &#125; &#125; private Node&lt;T&gt; head; private Node&lt;T&gt; currentNode; public BidirectionalLinkedList() &#123; head = currentNode = new Node&lt;T&gt;(null); size = 0; &#125; /** * 得到当前下标对应的结点 * * @param index * @throws StructureException */ public void indexNodeToCurrent(int index) throws StructureException &#123; currentNode = head; if (index &lt; -1 || index &gt; size - 1) &#123; throw new StructureException("index参数异常！"); &#125; if (index == -1) &#123; return; &#125; currentNode = head.next; int j = 0; while (currentNode != null &amp;&amp; j &lt; index) &#123; currentNode = currentNode.next; j++; &#125; &#125; @Override public void insert(int index, T t) throws StructureException &#123; if (index &lt; 0 || index &gt; size) &#123; throw new StructureException("index参数异常！"); &#125; // 得到当前下标的上一个结点 indexNodeToCurrent(index - 1); Node&lt;T&gt; insertNode = new Node&lt;T&gt;(t, currentNode, currentNode.next); if (currentNode.next != null) &#123; // 将新元素生成结点插入到当前结点下 currentNode.next.prior = insertNode; &#125; currentNode.next = insertNode; size++; &#125; @Override public void delete(int index) throws StructureException &#123; if (isEmpty()) &#123; throw new StructureException("链表为空"); &#125; if (index &lt; 0 || index &gt; size) &#123; throw new StructureException("index参数异常"); &#125; indexNodeToCurrent(index - 1); Node&lt;T&gt; twoNextNode = currentNode.next.next; if (twoNextNode != null) &#123; twoNextNode.prior = currentNode; &#125; currentNode.next = twoNextNode; size--; &#125; @Override public T get(int index) throws StructureException &#123; if (isEmpty()) &#123; throw new StructureException("链表为空"); &#125; if (index &lt; 0 || index &gt; size) &#123; throw new StructureException("index参数异常！"); &#125; indexNodeToCurrent(index); return currentNode.element; &#125;&#125;]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
      <tags>
        <tag>list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java数据结构（三）：线性表之单链表]]></title>
    <url>%2F2015%2F07%2F01%2F2015-07-01-java_datastruct_list_3%2F</url>
    <content type="text"><![CDATA[链式存储结构存储线性表的方法是把存放数据元素的结点用指针域构造成链。指针是指向下一个节点的引用，由数据元素域和一个或若干个指针域组成的一个类称之为结点。链式存储结构的特点是数据元素间的逻辑关系表现在节点的链接关系上。 本例中实现的链表结构都是带头结点的。具体代码如下：【详见：SingleLinkedList.java】123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package datastructure.linear.linked;import datastructure.exception.StructureException;import datastructure.linear.AbstractList;/** * @Description 单链表 * @author mastery * @Date 2015年6月30日下午6:11:17 * @param &lt;T&gt; */public class SingleLinkedList&lt;T&gt; extends AbstractList&lt;T&gt; &#123; class Node&lt;T1&gt; &#123; T1 element; Node&lt;T1&gt; next; public Node(Node&lt;T1&gt; next) &#123; super(); this.next = next; &#125; public Node(T1 element, Node&lt;T1&gt; next) &#123; super(); this.element = element; this.next = next; &#125; @Override public String toString() &#123; return element.toString(); &#125; &#125; /** * 头结点,该结点没有参数值，只是指向第一个元素的结点 */ private Node&lt;T&gt; head; /** * 当前的结点 */ private Node&lt;T&gt; currentNode; public SingleLinkedList() &#123; head = currentNode = new Node&lt;T&gt;(null); size = 0; &#125; /** * 得到当前下标对应的结点 * @param index * @throws StructureException */ public void indexNodeToCurrent(int index) throws StructureException &#123; currentNode = head; if(index &lt; -1 || index &gt; size -1) &#123; throw new StructureException("index参数异常！"); &#125; if(index == -1) &#123; return ; &#125; currentNode = head.next; int j = 0 ; while(currentNode != null &amp;&amp; j &lt; index) &#123; currentNode = currentNode.next; j++; &#125; &#125; @Override public void insert(int index, T t) throws StructureException &#123; if(index &lt; 0 || index &gt; size) &#123; throw new StructureException("index参数异常！"); &#125; // 得到当前下标的上一个结点 indexNodeToCurrent(index-1); // 将新元素生成结点插入到当前结点下 currentNode.next = new Node&lt;T&gt;(t , currentNode.next); size++; &#125; @Override public void delete(int index) throws StructureException &#123; if(isEmpty()) &#123; throw new StructureException("链表为空"); &#125; if(index &lt; 0 || index &gt; size) &#123; throw new StructureException("index参数异常"); &#125; indexNodeToCurrent(index-1); Node&lt;T&gt; twoNextNode = currentNode.next.next; currentNode.next = twoNextNode; size--; &#125; @Override public T get(int index) throws StructureException &#123; if(isEmpty()) &#123; throw new StructureException("链表为空"); &#125; if(index &lt; 0 || index &gt; size) &#123; throw new StructureException("index参数异常！"); &#125; indexNodeToCurrent(index); return currentNode.element; &#125;&#125; 单链表的插入和删除操作的时间效率分析方法和顺序表的插入和删除操作的时间效率分析方法类同，差别是单链表的插入和删除操作不需移动数据元素，只需比较数据元素。要说明的是，虽然单链表插入和删除操作的时间复杂度与顺序表插入和删除操作的时间复杂度相同，但是，顺序表插入和删除操作的时间复杂度指的是移动数据元素的时间复杂度，当数据元素占据的内存空间比较大时，这要比单链表插入和删除操作比较数据元素花费的时间大一个常数倍。另外，单链表求数据元素个数操作的时间复杂度为O(n). 与顺序表相比，单链表的主要有点是不需要预先确定数据元素的最大个数，插入和删除时不需要移动元素；主要缺点三每个结点中要有一个指针域，因此，空间单元利用效率不高。此外，单链表操作的算法比较复杂。 测试类如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445package datastructure.linear.linked;import static org.junit.Assert.*;import org.junit.Test;import datastructure.exception.StructureException;import datastructure.linear.List;import datastructure.linear.linked.SingleLinkedList;public class SingleLinkedListTest &#123; @Test public void testSingleLinkedList() throws StructureException &#123; List&lt;Integer&gt; list = new SingleLinkedList&lt;Integer&gt;(); for(int i = 0 ; i &lt; 10 ; i ++) &#123; list.insert(i, i+1); &#125; list.delete(0); for(int i = 0 ; i &lt; list.size() ; i++) &#123; System.out.print(list.get(i) + " "); &#125; &#125; @Test public void testIndexNodeToCurrent() &#123; fail("尚未实现"); &#125; @Test public void testInsert() &#123; fail("尚未实现"); &#125; @Test public void testDelete() &#123; fail("尚未实现"); &#125; @Test public void testGet() &#123; fail("尚未实现"); &#125;&#125;]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
      <tags>
        <tag>list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java数据结构（二）：线性表之顺序表]]></title>
    <url>%2F2015%2F07%2F01%2F2015-07-01-java_datastruct_list_2%2F</url>
    <content type="text"><![CDATA[顺序表采用数组实现，并且通过继承AbstractList类，下图为顺序表的存储结构图： 具体代码如下：【详见SequenceList.java】12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package datastructure.linear.sequence;import datastructure.exception.StructureException;import datastructure.linear.AbstractList;public class SequenceList&lt;T&gt; extends AbstractList&lt;T&gt;&#123; /** * 该顺序表的默认容量为10 */ private final static int defaultCapacity = 10; /** * 实现顺序表的数组 */ private Object[] arrs; /** * 实例化顺序表，使用默认的容量大小，为10 */ public SequenceList() &#123; this(defaultCapacity); &#125; /** * 实例化顺序表， 指定顺序表的容量 * @param capacity */ public SequenceList(int capacity) &#123; arrs = new Object[capacity]; size = 0; &#125; @Override public void insert(int index, T t) throws StructureException &#123; if(arrs.length &lt;= size) &#123; throw new StructureException("顺序表的容量已满"); &#125; if(index &lt; 0 || index &gt; size) &#123; throw new StructureException("参数异常！不能小于0或者大于当前长度"); &#125; // 插入前先后移之后的元素 for(int i = size ; i &gt; index ; i--) &#123; arrs[i] = arrs[i-1]; &#125; arrs[index] = t; size ++; &#125; @Override public void delete(int index) throws StructureException&#123; if(isEmpty()) &#123; throw new StructureException("该顺序表为空！不存在任何元素"); &#125; if(index &lt; 0 || index &gt;size - 1) &#123; throw new StructureException("参数异常！不能小于0或者大于顺序表的容量"); &#125; for(int i = index+1 ; i &lt; size ; i++ ) &#123; arrs[i-1] = arrs[i]; &#125; arrs[size -1] = null; size--; &#125; @SuppressWarnings("unchecked") @Override public T get(int index) throws StructureException &#123; if(isEmpty()) &#123; throw new StructureException("该顺序表为空！不存在任何元素"); &#125; if(index &lt; 0 || index &gt;arrs.length) &#123; throw new StructureException("参数异常！不能小于0或者大于顺序表的容量"); &#125; return (T) arrs[index]; &#125; &#125; 顺序表上的插入和删除是顺序表中时间复杂度最高的成员函数。在顺序表中插入一个数据元素时，算法中时间复杂度最高的部分是循环移动数据元素。循环移动数据元素的效率与插入数据元素的位置pos有关，最坏情况是pos=0,需移动size个数据元素；最好情况是pos=size，需移动0个元素。 顺序表的时间复杂度：顺序表中的其余操作都是与数据元素个数n无关，因此，在顺序表中插入和删除一个数据元素的时间复杂度为O(n)，其余操作的时间复杂度为O(1). 顺序表的主要优点：算法简单，空间单元利用效率高；主要缺点是需要预先确定数据元素的最大个数，并且插入和删除操作时需要移动较多的数据元素。 测试类：代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package datastructure.linear.sequence;import static org.junit.Assert.fail;import org.junit.Test;import datastructure.exception.StructureException;import datastructure.linear.List;public class SequenceListTest &#123; @Test public void testSequenceList() throws StructureException &#123; List&lt;String&gt; list = new SequenceList&lt;String&gt;(); list.insert(0, "asdas"); list.insert(1, "sdf"); list.insert(1, "a"); int index = 0; while(index &lt; 10) &#123; if(list.get(index) != null) &#123; System.out.println(index + "----" + list.get(index)); &#125; index ++; &#125; System.out.println(list.size()); &#125; @Test public void testSequenceListInt() throws StructureException &#123; List&lt;Integer&gt; list = new SequenceList&lt;Integer&gt;(); for(int i = 0 ; i &lt; 10 ; i ++) &#123; list.insert(i, i+1); &#125; list.delete(4); for(int i = 0 ; i &lt; list.size() ; i++) &#123; System.out.print(list.get(i) + " "); &#125; &#125; @Test public void testInsert() &#123; fail("尚未实现"); &#125; @Test public void testDelete() &#123; fail("尚未实现"); &#125; @Test public void testGet() &#123; fail("尚未实现"); &#125; @Test public void testSize() &#123; fail("尚未实现"); &#125; @Test public void testIsEmpty() &#123; fail("尚未实现"); &#125;&#125;]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
      <tags>
        <tag>list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java数据结构（一）：线性表之开篇]]></title>
    <url>%2F2015%2F07%2F01%2F2015-07-01-java_datastruct_list_1%2F</url>
    <content type="text"><![CDATA[Java数据结构（一）：线性表之开篇 1、线性表 线性结构的特点是除第一个和最后一个数据元素外的每个数据元素只有一个前驱数据元素和一个后继数据元素。线性表是一个最简单的线性结构。线性表的操作特点主要是可以在任意位置插入和删除一个数据元素。线性表可以用顺序存储结构和链式存储结构存储。用顺序存储结构实现的线性表称作顺序表，用链式存储结构实现的线性表称作链表。链表主要有单链表，循环单链表和双向循环链表三种。顺序表和单链表各有优缺点，并且优缺点刚好相反。 1.1 线性表的定义 线性表是一种可以在任意位置进行插入和删除数据元素操作的由n（n&gt;=0）个相同类型数据元素a0,a1,a2,….,an-1组成的线性结构。 1.2 线性表的接口定义 该接口名为List 并且使用了泛型来指定对应类型。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041package datastructure.linear;import datastructure.exception.StructureException;/** * @Description 实现线性表结构的接口 * @author mastery * @Date 2015年6月29日下午8:14:33 * @param &lt;T&gt; */public interface List&lt;T&gt; &#123; /** * 在线性表指定位置插入元素t * @param index * @param t */ void insert(int index , T t) throws StructureException; /** * 删除该线性表中指定位置的元素 * @param index */ void delete(int index) throws StructureException; /** * 获得该线性表中指定位置的元素 * @param index * @return */ T get(int index) throws StructureException; /** * 得到该线性表当前的存在多少个元素 * @return */ int size() throws StructureException; /** * 判断该线性表是否为空 * @return */ boolean isEmpty() throws StructureException;&#125; 并且在实现中自定义了一种异常StructureException.代码如下： 12345678910111213141516171819202122232425262728293031package datastructure.exception;public class StructureException extends Exception&#123; /** * */ private static final long serialVersionUID = 4187679158109073384L; public StructureException() &#123; super(); &#125; public StructureException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace) &#123; super(message, cause, enableSuppression, writableStackTrace); &#125; public StructureException(String message, Throwable cause) &#123; super(message, cause); &#125; public StructureException(String message) &#123; super(message); &#125; public StructureException(Throwable cause) &#123; super(cause); &#125;&#125; 1.3 线性表的抽象实现 该抽象类名为AbstractList：对线性表进行抽象实现，实现了上述接口的两个方法，具体代码如下： 123456789101112131415161718192021package datastructure.linear;import datastructure.exception.StructureException;public abstract class AbstractList&lt;T&gt; implements List&lt;T&gt;&#123; /** * 结构的长度 */ protected int size; @Override public int size() throws StructureException &#123; return size; &#125; @Override public boolean isEmpty() throws StructureException &#123; return size == 0; &#125;&#125; 好了！我们已经定义好接口和抽象类了，下面我们来具体来谈谈实现了吧！]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
      <tags>
        <tag>list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java_collections_Map]]></title>
    <url>%2F2012%2F07%2F29%2F2012-07-29-Java_collections_Map%2F</url>
    <content type="text"></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>collections</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java_collections_Set]]></title>
    <url>%2F2012%2F07%2F29%2F2012-07-29-Java_collections_Set%2F</url>
    <content type="text"></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>collections</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[判断两个单向链表是否相交]]></title>
    <url>%2F2012%2F07%2F20%2F2012-07-20-c_on_DataStructure%2F</url>
    <content type="text"><![CDATA[判断两个单向链表是否相交 1.链表定义链式存储结构存储线性表的方法是把存放数据元素的结点用指针域构造成链。指针是指向下一个节点的引用，由数据元素域和一个或若干个指针域组成的一个类称之为结点。链式存储结构的特点是数据元素间的逻辑关系表现在节点的链接关系上。 2.相交的理解若两个链表相交则其相交后的结点必定是最后一个结点或者是第k个结点能使其后面的结点连结成一条直线的结点 则可以得出若两个单向链表相交，则两个链表组成的形状一定为V型或Y型。 3.解决方法可以考虑用三种方式来实现： (1)使用双重循环来遍历结点判断是否相同 (2)判断两个链表的最后一个结点是否相同 (3)将两个链表构成环 设定链表结点的数据结构为 Node{ DataType data; //数据元素域 Node next; //指针域 } 第一个结点的引用为root； 3.1 双重循环判断对两个单向链表进行循环，临界值：当第一个链表的结点与第二个链表的结点相同时返回true，反之返回false。 /** * 使用while进行循环迭代，条件为next不为null */ while root1 != null begin while root2 != null begin if root1 == root2 then do return true end if end end return false 时间复杂度分析： 最好的情况下是root1==root2,此时只循环一次，时间复杂度为O(1),最坏情况下是两个链表的最后一个结点相等时，循环需要n*n次，即时间复杂度为O(n2). 3.2 判断最后一个结点是否相同得到两个单向链表的最后一个结点，临界值：两个结点相同时返回true，反之返回false /** * 用root1指代第一个链表的头结点，root2指代第二个链表的头结点 */ while root1 != null begin root1 = root1.next end while root2 != null begin root2 = root2.next end if root1 == root2 then return true else return false end if 时间复杂度分析： 该算法的时间复杂度在最好和最坏的情况下都是O(n)，都必须循环n次到尾结点。 3.3 链表成环假设两个链表尾部相连能够构成环，则一个链表的首结点遍历到末尾结点必然是另一个链表的首结点。 /** * 该程序的先觉条件是必须在两个链表已经构成环的基础上可执行 */ while root1 != null begin root1 = root1.next end if root1 == root2 then return true return false 时间复杂度分析： 该算法的时间复杂度在最好和最坏的情况下都是O(n)，都必须循环n次到尾结点。]]></content>
      <categories>
        <category>DataStructure</category>
      </categories>
      <tags>
        <tag>list</tag>
      </tags>
  </entry>
</search>
